{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Library imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pyforest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from turtle import forward\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparams and loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1698, 1397, 168848)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'steam'\n",
    "\n",
    "train_edges = np.load(dataset + '/train_edges.npy')\n",
    "user_list = train_edges[:, 0]\n",
    "item_list = train_edges[:, 1]\n",
    "rating_list = train_edges[:, 2].astype('float32')\n",
    "\n",
    "n_users = user_list.max() + 1 \n",
    "n_items = item_list.max() + 1\n",
    "n_samples = len(rating_list)\n",
    "\n",
    "n_users, n_items, n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining collaborative filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(CollaborativeFiltering, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        dot = (u * i).sum(1)\n",
    "        return torch.sigmoid(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "        self.fc1 = nn.Linear(n_factors * 2, n_factors)\n",
    "        self.fc2 = nn.Linear(n_factors, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        tanh = nn.Tanh()\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        swish = nn.SiLU()\n",
    "\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        x = torch.concat([u, i], dim = 1)\n",
    "        x = swish(x)\n",
    "        x = self.fc1(x)\n",
    "        x = swish(x)\n",
    "        x = self.fc2(x)\n",
    "        x = sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_hat, y):\n",
    "    y = y.clone().int()\n",
    "    y_hat = (y_hat.clone() > 0.5).int()\n",
    "    accuracy = (y == y_hat).sum() / len(y)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Code for surrogate meta-attack (surrogate-CF-SGD, evaluation-CF-Adam, same-init)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> T:  250\n",
      "-> Delta: 200 (0.12%)\n",
      "-> Embedding size:  64\n",
      "-> Device:  cuda:6\n",
      "\n",
      "-> Surrogate:  NCF-3\n",
      "-> Target:  NCF-3\n",
      "-> Surrogate optimizer:  adam\n",
      "-> Target optimizer:  adam\n",
      "-> Surrogate learning rate:  0.01\n",
      "-> Target learning rate:  0.01\n",
      "\n",
      "-> Retain graph:  True\n",
      "-> Create graph:  False\n",
      "-> Save results:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-> Perturbations: 100%|██████████| 200/200 [05:23<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Execution time: 00h 05m 24s\n"
     ]
    }
   ],
   "source": [
    "# model settings\n",
    "surrogate = 'NCF-3'\n",
    "target = 'NCF-3'\n",
    "opt_surrogate = 'adam'\n",
    "opt_target = 'adam'\n",
    "lr_surrogate = 0.01\n",
    "lr_target = 0.01\n",
    "\n",
    "# start execution\n",
    "start_time = time.time()\n",
    "\n",
    "# GPU settings (set use_gpu = -1 if you want to use CPU)\n",
    "use_gpu = 6\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# some hyperparams\n",
    "T = 250\n",
    "Delta = 200 # 5% ~ 10K perturbations\n",
    "n_factors = 64\n",
    "save_results = True\n",
    "retain_graph = True \n",
    "create_graph = False\n",
    "\n",
    "# list of perturbations\n",
    "perturbations = dict()\n",
    "perturbations['edges'] = []\n",
    "perturbations['metagrad'] = []\n",
    "perturbations['accuracy_before'] = []\n",
    "perturbations['accuracy_after'] = []\n",
    "perturbations['loss_before'] = []\n",
    "perturbations['loss_after'] = []\n",
    "\n",
    "perturbations['accuracy_before_eval'] = []\n",
    "perturbations['accuracy_after_eval'] = []\n",
    "perturbations['loss_before_eval'] = []\n",
    "perturbations['loss_after_eval'] = []\n",
    "\n",
    "# print hyperparam config\n",
    "print('-> T: ', T)\n",
    "print('-> Delta: {} ({}%)'.format(Delta, round(Delta * 100 / n_samples, 2)))\n",
    "print('-> Embedding size: ', n_factors)\n",
    "print('-> Device: ', device)\n",
    "print()\n",
    "print('-> Surrogate: ', surrogate)\n",
    "print('-> Target: ', target)\n",
    "print('-> Surrogate optimizer: ', opt_surrogate)\n",
    "print('-> Target optimizer: ', opt_target)\n",
    "print('-> Surrogate learning rate: ', lr_surrogate)\n",
    "print('-> Target learning rate: ', lr_target)\n",
    "print()\n",
    "print('-> Retain graph: ', retain_graph)\n",
    "print('-> Create graph: ', create_graph)\n",
    "print('-> Save results: ', save_results)\n",
    "\n",
    "# load users, items and ratings as tensors\n",
    "users = torch.tensor(user_list, device = device)\n",
    "items = torch.tensor(item_list, device = device)\n",
    "ratings = torch.tensor(rating_list, device = device, requires_grad = True)\n",
    "if 'NCF' in surrogate:\n",
    "    ratings = ratings.reshape((n_samples, 1))\n",
    "perturbs = torch.ones_like(ratings).bool()\n",
    "\n",
    "# for each perturbation do the following\n",
    "for delta in tqdm(range(Delta), desc='-> Perturbations'):\n",
    "\n",
    "    # define model and it's parameters\n",
    "    if 'NCF' in surrogate:\n",
    "        model = NCF(n_users, n_items, n_factors)\n",
    "    else:\n",
    "        model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "    model.to(device)\n",
    "\n",
    "    # reset model paramters \n",
    "    torch.manual_seed(50)\n",
    "    for layer in model.children():\n",
    "        layer.reset_parameters()\n",
    "\n",
    "    # define optimizer\n",
    "    if 'adam' in opt_surrogate:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = lr_surrogate)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = lr_surrogate)\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    # inner loop training process\n",
    "    model.train()\n",
    "    for i in range(T):\n",
    "        y_hat = model(users, items)\n",
    "        loss = loss_fn(y_hat, ratings)\n",
    "\n",
    "        # use torch.optim optimizer to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=retain_graph, create_graph=create_graph)\n",
    "        optimizer.step()\n",
    "\n",
    "    # compute and store accuracy of model after T training steps\n",
    "    with torch.no_grad():\n",
    "        # compute training accuracy and loss including perturbed edges\n",
    "        y_hat = model(users, items)\n",
    "        perturbations['accuracy_before'].append(get_accuracy(y_hat, ratings))\n",
    "        perturbations['loss_before'].append(loss_fn(y_hat, ratings).item())\n",
    "\n",
    "        # compute training accuracy and loss excluding perturbed edges\n",
    "        y_hat_masked = torch.masked_select(y_hat, perturbs)\n",
    "        ratings_masked = torch.masked_select(ratings, perturbs)\n",
    "        perturbations['accuracy_after'].append(get_accuracy(y_hat_masked, ratings_masked))\n",
    "        perturbations['loss_after'].append(loss_fn(y_hat_masked, ratings_masked).item())\n",
    "    \n",
    "    # compute meta gradient\n",
    "    meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "\n",
    "    # define evaluation model\n",
    "    if 'NCF' in target:\n",
    "        eval_model = NCF(n_users, n_items, n_factors)\n",
    "    else:\n",
    "        eval_model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "    eval_model.to(device)\n",
    "\n",
    "    # reset eval model parameters\n",
    "    torch.manual_seed(50)\n",
    "    for layer in eval_model.children():\n",
    "        layer.reset_parameters()\n",
    "\n",
    "    # define optimizer_eval\n",
    "    if 'adam' in opt_target:\n",
    "        optimizer_eval = torch.optim.Adam(eval_model.parameters(), lr = lr_target)\n",
    "    else:\n",
    "        optimizer_eval = torch.optim.SGD(eval_model.parameters(), lr = lr_target)\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn_eval = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    # detach ratings and perturbs for eval model\n",
    "    ratings_eval = ratings.detach().clone()\n",
    "    perturbs_eval = perturbs.detach().clone()\n",
    "\n",
    "    # reshape ratings and perturbs if necessary\n",
    "    if surrogate != target:\n",
    "        if 'NCF' in target:\n",
    "            ratings_eval = ratings_eval.reshape((n_samples, 1))\n",
    "            perturbs_eval = perturbs_eval.reshape((n_samples, 1))\n",
    "        else:\n",
    "            ratings_eval = ratings_eval.reshape((n_samples))\n",
    "            perturbs_eval = perturbs_eval.reshape((n_samples))\n",
    "\n",
    "    # inner train  evaluation model\n",
    "    eval_model.train()\n",
    "    for i in range(T):\n",
    "        y_hat = eval_model(users, items)\n",
    "        loss_eval = loss_fn_eval(y_hat, ratings_eval)\n",
    "\n",
    "        # use torch.optim optimizer to compute gradients\n",
    "        optimizer_eval.zero_grad()\n",
    "        loss_eval.backward(retain_graph=retain_graph, create_graph=create_graph)\n",
    "        optimizer_eval.step()\n",
    "    \n",
    "    eval_model.eval()\n",
    "\n",
    "    # compute and store accuracy of eval model after T training steps\n",
    "    with torch.no_grad():\n",
    "        # compute training accuracy and loss including perturbed edges\n",
    "        y_hat = eval_model(users, items)\n",
    "        perturbations['accuracy_before_eval'].append(get_accuracy(y_hat, ratings_eval))\n",
    "        perturbations['loss_before_eval'].append(loss_fn_eval(y_hat, ratings_eval).item())\n",
    "\n",
    "        # compute training accuracy and loss excluding perturbed edges\n",
    "        y_hat_masked = torch.masked_select(y_hat, perturbs_eval)\n",
    "        ratings_masked = torch.masked_select(ratings_eval, perturbs_eval)\n",
    "        perturbations['accuracy_after_eval'].append(get_accuracy(y_hat_masked, ratings_masked))\n",
    "        perturbations['loss_after_eval'].append(loss_fn_eval(y_hat_masked, ratings_masked).item())\n",
    "\n",
    "    # select best edge and perform perturbation\n",
    "    with torch.no_grad():\n",
    "        mask = ratings.detach().int()\n",
    "        meta_grad[mask == 1] = 0\n",
    "        best_edge = meta_grad.argmax().item()\n",
    "        ratings[best_edge] = 1\n",
    "        perturbs[best_edge] = False\n",
    "\n",
    "        # keep track of perturbations and accuracy\n",
    "        perturbations['edges'].append(best_edge)\n",
    "        perturbations['metagrad'].append(meta_grad[best_edge].item())\n",
    "\n",
    "sleep(1)\n",
    "# compute execution time\n",
    "exec_time = int(time.time() - start_time)\n",
    "exec_time = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(exec_time))\n",
    "print('-> Execution time: {}'.format(exec_time))\n",
    "\n",
    "# convert results to dataframes for visualisation\n",
    "perturbations = pd.DataFrame(perturbations)\n",
    "filename = 'surrogate{}-target{}-meta-Delta={}-T={}-sameinit'.format(surrogate, target, Delta, T)\n",
    "\n",
    "# save results in CSV format\n",
    "if save_results:\n",
    "    perturbations.to_csv('results/' + dataset + '/' + filename + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edges</th>\n",
       "      <th>metagrad</th>\n",
       "      <th>accuracy_before</th>\n",
       "      <th>accuracy_after</th>\n",
       "      <th>loss_before</th>\n",
       "      <th>loss_after</th>\n",
       "      <th>accuracy_before_eval</th>\n",
       "      <th>accuracy_after_eval</th>\n",
       "      <th>loss_before_eval</th>\n",
       "      <th>loss_after_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3641</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.944992</td>\n",
       "      <td>0.944992</td>\n",
       "      <td>0.155346</td>\n",
       "      <td>0.155346</td>\n",
       "      <td>0.944992</td>\n",
       "      <td>0.944992</td>\n",
       "      <td>0.155346</td>\n",
       "      <td>0.155346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49526</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.943890</td>\n",
       "      <td>0.943890</td>\n",
       "      <td>0.160610</td>\n",
       "      <td>0.160611</td>\n",
       "      <td>0.943890</td>\n",
       "      <td>0.943890</td>\n",
       "      <td>0.160610</td>\n",
       "      <td>0.160611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101502</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.941456</td>\n",
       "      <td>0.941456</td>\n",
       "      <td>0.165901</td>\n",
       "      <td>0.165899</td>\n",
       "      <td>0.941456</td>\n",
       "      <td>0.941456</td>\n",
       "      <td>0.165901</td>\n",
       "      <td>0.165899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123489</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.946141</td>\n",
       "      <td>0.946140</td>\n",
       "      <td>0.154077</td>\n",
       "      <td>0.154077</td>\n",
       "      <td>0.946141</td>\n",
       "      <td>0.946140</td>\n",
       "      <td>0.154077</td>\n",
       "      <td>0.154077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12088</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.945999</td>\n",
       "      <td>0.945997</td>\n",
       "      <td>0.155850</td>\n",
       "      <td>0.155849</td>\n",
       "      <td>0.945999</td>\n",
       "      <td>0.945997</td>\n",
       "      <td>0.155850</td>\n",
       "      <td>0.155849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>143874</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.942659</td>\n",
       "      <td>0.942764</td>\n",
       "      <td>0.162967</td>\n",
       "      <td>0.162599</td>\n",
       "      <td>0.942659</td>\n",
       "      <td>0.942764</td>\n",
       "      <td>0.162967</td>\n",
       "      <td>0.162599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>27633</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.942664</td>\n",
       "      <td>0.942770</td>\n",
       "      <td>0.162740</td>\n",
       "      <td>0.162366</td>\n",
       "      <td>0.942664</td>\n",
       "      <td>0.942770</td>\n",
       "      <td>0.162740</td>\n",
       "      <td>0.162366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>97860</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.941356</td>\n",
       "      <td>0.941524</td>\n",
       "      <td>0.167381</td>\n",
       "      <td>0.166826</td>\n",
       "      <td>0.941356</td>\n",
       "      <td>0.941524</td>\n",
       "      <td>0.167381</td>\n",
       "      <td>0.166826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>55460</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.933034</td>\n",
       "      <td>0.933495</td>\n",
       "      <td>0.174160</td>\n",
       "      <td>0.173050</td>\n",
       "      <td>0.933034</td>\n",
       "      <td>0.933495</td>\n",
       "      <td>0.174160</td>\n",
       "      <td>0.173050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>72922</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.942576</td>\n",
       "      <td>0.942680</td>\n",
       "      <td>0.163229</td>\n",
       "      <td>0.162822</td>\n",
       "      <td>0.942576</td>\n",
       "      <td>0.942680</td>\n",
       "      <td>0.163229</td>\n",
       "      <td>0.162822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      edges  metagrad  accuracy_before  accuracy_after  loss_before  \\\n",
       "0      3641  0.000304         0.944992        0.944992     0.155346   \n",
       "1     49526  0.000283         0.943890        0.943890     0.160610   \n",
       "2    101502  0.000258         0.941456        0.941456     0.165901   \n",
       "3    123489  0.000270         0.946141        0.946140     0.154077   \n",
       "4     12088  0.000247         0.945999        0.945997     0.155850   \n",
       "..      ...       ...              ...             ...          ...   \n",
       "195  143874  0.000138         0.942659        0.942764     0.162967   \n",
       "196   27633  0.000138         0.942664        0.942770     0.162740   \n",
       "197   97860  0.000131         0.941356        0.941524     0.167381   \n",
       "198   55460  0.000129         0.933034        0.933495     0.174160   \n",
       "199   72922  0.000138         0.942576        0.942680     0.163229   \n",
       "\n",
       "     loss_after  accuracy_before_eval  accuracy_after_eval  loss_before_eval  \\\n",
       "0      0.155346              0.944992             0.944992          0.155346   \n",
       "1      0.160611              0.943890             0.943890          0.160610   \n",
       "2      0.165899              0.941456             0.941456          0.165901   \n",
       "3      0.154077              0.946141             0.946140          0.154077   \n",
       "4      0.155849              0.945999             0.945997          0.155850   \n",
       "..          ...                   ...                  ...               ...   \n",
       "195    0.162599              0.942659             0.942764          0.162967   \n",
       "196    0.162366              0.942664             0.942770          0.162740   \n",
       "197    0.166826              0.941356             0.941524          0.167381   \n",
       "198    0.173050              0.933034             0.933495          0.174160   \n",
       "199    0.162822              0.942576             0.942680          0.163229   \n",
       "\n",
       "     loss_after_eval  \n",
       "0           0.155346  \n",
       "1           0.160611  \n",
       "2           0.165899  \n",
       "3           0.154077  \n",
       "4           0.155849  \n",
       "..               ...  \n",
       "195         0.162599  \n",
       "196         0.162366  \n",
       "197         0.166826  \n",
       "198         0.173050  \n",
       "199         0.162822  \n",
       "\n",
       "[200 rows x 10 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00030420697294175625"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbations.metagrad.max()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f38835821387ecea7238337192aa99e87ed1a9c9c1fa6562e207de7e0c31193"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('PyG': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
