{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pyforest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from turtle import forward\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682, 159619, 40381)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'movielens'\n",
    "\n",
    "train_edges = np.load('data/' + dataset + '/train_edges.npy')\n",
    "test_edges = np.load('data/' + dataset + '/test_edges.npy')\n",
    "\n",
    "user_list_train = train_edges[:, 0]\n",
    "user_list_test = test_edges[:, 0]\n",
    "item_list_train = train_edges[:, 1]\n",
    "item_list_test = test_edges[:, 1]\n",
    "rating_list_train = train_edges[:, 2].astype('float32')\n",
    "rating_list_test = test_edges[:, 2].astype('float32')\n",
    "\n",
    "n_users = max(user_list_train.max(), user_list_test.max()) + 1 \n",
    "n_items = max(item_list_train.max(), item_list_test.max()) + 1\n",
    "n_samples_train = len(rating_list_train)\n",
    "n_samples_test = len(rating_list_test)\n",
    "\n",
    "n_users, n_items, n_samples_train, n_samples_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining collaborative filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(CollaborativeFiltering, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        drop_u = nn.Dropout(p = 0.3)\n",
    "        drop_i = nn.Dropout(p = 0.3)\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        u = drop_u(u)\n",
    "        i = drop_i(i)\n",
    "        dot = (u * i).sum(1)\n",
    "        return torch.sigmoid(dot)\n",
    "\n",
    "class CF1(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(CF1, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "        self.fc = nn.Linear(n_factors * 3, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        drop_u = nn.Dropout(p = 0.3)\n",
    "        drop_i = nn.Dropout(p = 0.3)\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        u = drop_u(u)\n",
    "        i = drop_i(i)\n",
    "        dot = (u * i)\n",
    "        x = torch.concat([u, i, dot], dim = 1)\n",
    "        # x = torch.sigmoid(x)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class CF2(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(CF2, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        drop_u = nn.Dropout(p = 0.3)\n",
    "        drop_i = nn.Dropout(p = 0.3)\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        u = drop_u(u)\n",
    "        i = drop_i(i)\n",
    "        dot = (u * i)\n",
    "        sum = u + i + dot\n",
    "        return torch.sigmoid(sum.sum(dim = 1))\n",
    "\n",
    "class CF4(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(CF4, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        drop_u = nn.Dropout(p = 0.2)\n",
    "        drop_i = nn.Dropout(p = 0.2)\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        u = drop_u(u)\n",
    "        i = drop_i(i)\n",
    "        dot = (u * i)\n",
    "        bias_u = self.user_bias(user).squeeze()\n",
    "        bias_i = self.item_bias(item).squeeze()\n",
    "        total = dot.sum(dim = 1) + bias_u + bias_i\n",
    "        # total = dot.sum(dim = 1)\n",
    "        return torch.sigmoid(total)\n",
    "\n",
    "class NCF(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "        self.fc1 = nn.Linear(n_factors * 2, n_factors)\n",
    "        self.fc2 = nn.Linear(n_factors, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        tanh = nn.Tanh()\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        swish = nn.SiLU()\n",
    "\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        x = torch.concat([u, i], dim = 1)\n",
    "        x = swish(x)\n",
    "        x = self.fc1(x)\n",
    "        x = swish(x)\n",
    "        x = self.fc2(x)\n",
    "        x = sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def get_accuracy(y_hat, y):\n",
    "    y = y.clone().int()\n",
    "    y_hat = (y_hat.clone() > 0.5).int()\n",
    "    accuracy = (y == y_hat).sum() / len(y)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test collaborative filtering on unseen data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = 6\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "users = torch.tensor(user_list_train, device = device)\n",
    "items = torch.tensor(item_list_train, device = device)\n",
    "ratings = torch.tensor(rating_list_train, device = device, requires_grad = True)\n",
    "\n",
    "users_test = torch.tensor(user_list_test, device = device)\n",
    "items_test = torch.tensor(item_list_test, device = device)\n",
    "ratings_test = torch.tensor(rating_list_test, device = device)\n",
    "\n",
    "# ratings = ratings.reshape((n_samples_train, 1))\n",
    "# ratings_test = ratings_test.reshape((n_samples_test, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:  1000\n",
      "Training accuracy:  0.9091523885726929\n",
      "Training AUC:  0.972411446309926\n",
      "\n",
      "max meta grad:  tensor(0.0003, device='cuda:6')\n",
      "min meta grad:  tensor(-inf, device='cuda:6')\n",
      "tensor([ 3.1337e-06, -1.8015e-05, -4.1825e-05,  ..., -2.6589e-06,\n",
      "         5.7238e-05,  1.4330e-06], device='cuda:6')\n",
      "\n",
      "Testing accuracy:  0.7618434429168701\n",
      "Training AUC:  0.8365536504587607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lr = 1 and T = 1000, Adam giving 77% accuracy on test data\n",
    "# lr = 1.1 and T = 500, Adam giving 77.2% accuracy on test data\n",
    "# lr = 1.4 and T = 250, Adam giving 76.8% accuracy on test data\n",
    "# lr = 1.8 and T = 250, Adam giving 77.2% accuracy on test data\n",
    "# lr = 1.6 and T = 250, Adam giving 77% accuracy on test data but making meta-gradients infinite\n",
    "# lr = 500, T = 300, SGD, Dropout = 0.3 gives 75.58% accuracy on test data\n",
    "# lr = 1000, T = 300, SGD, Dropout = 0.3, seed = 0 gives 76% accuracy on test data\n",
    "# lr = 1000, T = 300, SGD, Dropout = 0.3, seed = 50 gives 76.18% accuracy on test data\n",
    "\n",
    "n_factors = 64\n",
    "T = 300\n",
    "seed = 50\n",
    "\n",
    "# for lr in list(range(1, 500, 5)):\n",
    "for lr in [1000]:\n",
    "\n",
    "    # model = CF4(n_users, n_items, n_factors)\n",
    "    model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "    model.to(device)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    for layer in model.children():\n",
    "        layer.reset_parameters()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "    loss_fn = torch.nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(T):\n",
    "        y_hat = model(users, items)\n",
    "        loss = loss_fn(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        # loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    y_hat = model(users, items)\n",
    "    print('lr: ', lr)\n",
    "    print('Training accuracy: ', get_accuracy(y_hat, ratings))\n",
    "    y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "    y = ratings.detach().clone().to('cpu').numpy()\n",
    "    print('Training AUC: ', roc_auc_score(y, y_pred))\n",
    "    print()\n",
    "    loss = loss_fn(y_hat, ratings)\n",
    "    meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "    print('max meta grad: ', meta_grad.max())\n",
    "    print('min meta grad: ', meta_grad.min())\n",
    "    print(meta_grad)\n",
    "    print()\n",
    "    y_hat = model(users_test, items_test)\n",
    "    print('Testing accuracy: ', get_accuracy(y_hat, ratings_test))\n",
    "    y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "    y = ratings_test.detach().clone().to('cpu').numpy()\n",
    "    print('Testing AUC: ', roc_auc_score(y, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:  1000\n",
      "Training accuracy:  0.9477066993713379\n",
      "Training AUC:  0.9897268408765496\n",
      "\n",
      "max meta grad:  tensor(0.0004, device='cuda:6')\n",
      "min meta grad:  tensor(-inf, device='cuda:6')\n",
      "tensor([ 1.5794e-05, -3.6377e-05, -5.2107e-05,  ..., -1.5311e-07,\n",
      "         4.8483e-05,  7.0595e-05], device='cuda:6')\n",
      "\n",
      "Testing accuracy:  0.7502290606498718\n",
      "Testing AUC:  0.8177104337373045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Double factors \n",
    "\n",
    "# lr = 1 and T = 1000, Adam giving 77% accuracy on test data\n",
    "# lr = 1.1 and T = 500, Adam giving 77.2% accuracy on test data\n",
    "# lr = 1.4 and T = 250, Adam giving 76.8% accuracy on test data\n",
    "# lr = 1.8 and T = 250, Adam giving 77.2% accuracy on test data\n",
    "# lr = 1.6 and T = 250, Adam giving 77% accuracy on test data but making meta-gradients infinite\n",
    "# lr = 500, T = 300, SGD, Dropout = 0.3 gives 75.58% accuracy on test data\n",
    "# lr = 1000, T = 300, SGD, Dropout = 0.3, seed = 0 gives 76% accuracy on test data\n",
    "# lr = 1000, T = 300, SGD, Dropout = 0.3, seed = 50 gives 76.18% accuracy on test data\n",
    "\n",
    "n_factors = 128\n",
    "T = 300\n",
    "seed = 0\n",
    "\n",
    "# for lr in list(range(1, 500, 5)):\n",
    "for lr in [1000]:\n",
    "\n",
    "    # model = CF4(n_users, n_items, n_factors)\n",
    "    model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "    model.to(device)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    for layer in model.children():\n",
    "        layer.reset_parameters()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "    loss_fn = torch.nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(T):\n",
    "        y_hat = model(users, items)\n",
    "        loss = loss_fn(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        # loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    y_hat = model(users, items)\n",
    "    print('lr: ', lr)\n",
    "    print('Training accuracy: ', get_accuracy(y_hat, ratings))\n",
    "    y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "    y = ratings.detach().clone().to('cpu').numpy()\n",
    "    print('Training AUC: ', roc_auc_score(y, y_pred))\n",
    "    print()\n",
    "    loss = loss_fn(y_hat, ratings)\n",
    "    meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "    print('max meta grad: ', meta_grad.max())\n",
    "    print('min meta grad: ', meta_grad.min())\n",
    "    print(meta_grad)\n",
    "    print()\n",
    "    y_hat = model(users_test, items_test)\n",
    "    print('Testing accuracy: ', get_accuracy(y_hat, ratings_test))\n",
    "    y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "    y = ratings_test.detach().clone().to('cpu').numpy()\n",
    "    print('Testing AUC: ', roc_auc_score(y, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "n_factors = 64\n",
    "print(n_factors)\n",
    "print(n_factors // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:  1000\n",
      "Training accuracy:  0.8688000440597534\n",
      "Training AUC:  0.9457508614463884\n",
      "\n",
      "max meta grad:  tensor(0.0002, device='cuda:6')\n",
      "min meta grad:  tensor(-inf, device='cuda:6')\n",
      "tensor([-5.5593e-07, -1.4034e-05, -4.0434e-05,  ..., -3.7553e-06,\n",
      "         1.1104e-05,  1.5798e-05], device='cuda:6')\n",
      "\n",
      "Testing accuracy:  0.7748198509216309\n",
      "Testing AUC:  0.8519723553800107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Half factors \n",
    "\n",
    "# lr = 1 and T = 1000, Adam giving 77% accuracy on test data\n",
    "# lr = 1.1 and T = 500, Adam giving 77.2% accuracy on test data\n",
    "# lr = 1.4 and T = 250, Adam giving 76.8% accuracy on test data\n",
    "# lr = 1.8 and T = 250, Adam giving 77.2% accuracy on test data\n",
    "# lr = 1.6 and T = 250, Adam giving 77% accuracy on test data but making meta-gradients infinite\n",
    "# lr = 500, T = 300, SGD, Dropout = 0.3 gives 75.58% accuracy on test data\n",
    "# lr = 1000, T = 300, SGD, Dropout = 0.3, seed = 0 gives 76% accuracy on test data\n",
    "# lr = 1000, T = 300, SGD, Dropout = 0.3, seed = 50 gives 76.18% accuracy on test data\n",
    "\n",
    "n_factors = 32\n",
    "T = 300\n",
    "seed = 0\n",
    "\n",
    "# for lr in list(range(1, 500, 5)):\n",
    "for lr in [1000]:\n",
    "\n",
    "    # model = CF4(n_users, n_items, n_factors)\n",
    "    model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "    model.to(device)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    for layer in model.children():\n",
    "        layer.reset_parameters()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "    loss_fn = torch.nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(T):\n",
    "        y_hat = model(users, items)\n",
    "        loss = loss_fn(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        # loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    y_hat = model(users, items)\n",
    "    print('lr: ', lr)\n",
    "    print('Training accuracy: ', get_accuracy(y_hat, ratings))\n",
    "    y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "    y = ratings.detach().clone().to('cpu').numpy()\n",
    "    print('Training AUC: ', roc_auc_score(y, y_pred))\n",
    "    print()\n",
    "    loss = loss_fn(y_hat, ratings)\n",
    "    meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "    print('max meta grad: ', meta_grad.max())\n",
    "    print('min meta grad: ', meta_grad.min())\n",
    "    print(meta_grad)\n",
    "    print()\n",
    "    y_hat = model(users_test, items_test)\n",
    "    print('Testing accuracy: ', get_accuracy(y_hat, ratings_test))\n",
    "    y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "    y = ratings_test.detach().clone().to('cpu').numpy()\n",
    "    print('Testing AUC: ', roc_auc_score(y, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollaborativeFiltering(\n",
       "  (user_emb): Embedding(943, 128)\n",
       "  (item_emb): Embedding(1682, 128)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exploring AUC metric for evaluating CF model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far, using AUROC instead of accuracy seems promising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AUC for CF2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0000000e+00 6.9827717e-03 2.9729408e-06 ... 5.6153840e-01 9.9999976e-01\n",
      " 9.9999976e-01]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8441157242039155"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model(users_test, items_test)\n",
    "y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "y = ratings_test.detach().clone().to('cpu').numpy()\n",
    "print(y_pred)\n",
    "print(y)\n",
    "roc_auc_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1213294e-06 9.9984813e-01 1.0000000e+00 ... 1.9173597e-01 3.6412087e-09\n",
      " 8.8454279e-12]\n",
      "[0. 1. 1. ... 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9632291366539395"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model(users, items)\n",
    "y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "y = ratings.detach().clone().to('cpu').numpy()\n",
    "print(y_pred)\n",
    "print(y)\n",
    "roc_auc_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AUC for original CF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90360653 0.7383278  0.7618074  ... 0.9854893  0.85157144 0.9285307 ]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8366609955350572"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model(users_test, items_test)\n",
    "y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "y = ratings_test.detach().clone().to('cpu').numpy()\n",
    "print(y_pred)\n",
    "print(y)\n",
    "roc_auc_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9852560e-01 9.3463689e-01 9.8911220e-01 ... 8.1000441e-01 4.7213207e-06\n",
      " 5.9991311e-02]\n",
      "[0. 1. 1. ... 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9726644336621912"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model(users, items)\n",
    "y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "y = ratings.detach().clone().to('cpu').numpy()\n",
    "print(y_pred)\n",
    "print(y)\n",
    "roc_auc_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AUC for CF4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98724735 0.96072394 0.62975526 ... 0.90863013 0.9980566  0.99219877]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8266789252244737"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model(users_test, items_test)\n",
    "y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "y = ratings_test.detach().clone().to('cpu').numpy()\n",
    "print(y_pred)\n",
    "print(y)\n",
    "roc_auc_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58443964 0.94244504 0.65263903 ... 0.98390967 0.01553843 0.00244195]\n",
      "[0. 1. 1. ... 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9771650815917055"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model(users, items)\n",
    "y_pred = y_hat.detach().clone().to('cpu').numpy()\n",
    "y = ratings.detach().clone().to('cpu').numpy()\n",
    "print(y_pred)\n",
    "print(y)\n",
    "roc_auc_score(y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f38835821387ecea7238337192aa99e87ed1a9c9c1fa6562e207de7e0c31193"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('PyG': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
