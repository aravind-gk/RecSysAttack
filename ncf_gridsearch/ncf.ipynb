{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pyforest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from turtle import forward\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparams and data-loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682, 200000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_edges = np.load('movielens/train_edges.npy')\n",
    "user_list = train_edges[:, 0]\n",
    "item_list = train_edges[:, 1]\n",
    "rating_list = train_edges[:, 2].astype('float32')\n",
    "\n",
    "n_users = user_list.max() + 1 \n",
    "n_items = item_list.max() + 1\n",
    "n_samples = len(rating_list)\n",
    "\n",
    "n_users, n_items, n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Neural collaborative filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "        self.fc = nn.Linear(n_factors * 2, 1)\n",
    "        # self.user_emb.weight.data.uniform_(0, 0.5)\n",
    "        # self.item_emb.weight.data.uniform_(0, 0.5)\n",
    "        # self.fc.weight.data.uniform_(0, 0.5)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        features = torch.concat([u, i], dim = 1)\n",
    "        x = self.fc(features)\n",
    "        out = torch.sigmoid(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "        self.fc1 = nn.Linear(n_factors * 2, n_factors)\n",
    "        # self.fc2 = nn.Linear(n_factors, n_factors // 2)\n",
    "        self.fc2 = nn.Linear(n_factors, 1)\n",
    "        # self.fc3 = nn.Linear(n_factors // 2, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        tanh = nn.Tanh()\n",
    "        sigmoid = nn.Sigmoid()\n",
    "\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        features = torch.concat([u, i], dim = 1)\n",
    "        x = self.fc1(features)\n",
    "        x = tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = tanh(x)\n",
    "        # x = self.fc3(x)\n",
    "        x = sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(CollaborativeFiltering, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        dot = (u * i).sum(1)\n",
    "        return torch.sigmoid(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_hat, y):\n",
    "    y = y.clone().int()\n",
    "    y_hat = (y_hat.clone() > 0.5).int()\n",
    "    accuracy = (y == y_hat).sum() / len(y)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test NCF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:  60  T =  50  loss:  18.998760223388672  acc:  0.729640007019043\n",
      "lr:  60  T:  50  loss:  18.998760223388672  acc:  0.729640007019043\n",
      "lr:  60  T =  100  loss:  19.90110969543457  acc:  0.7626699805259705\n",
      "lr:  60  T:  100  loss:  19.90110969543457  acc:  0.7626699805259705\n",
      "lr:  60  T =  150  loss:  11.323873519897461  acc:  0.8144049644470215\n",
      "lr:  60  T:  150  loss:  11.323873519897461  acc:  0.8144049644470215\n",
      "lr:  60  T =  200  loss:  14.165251731872559  acc:  0.7846999764442444\n",
      "lr:  60  T:  200  loss:  14.165251731872559  acc:  0.7846999764442444\n",
      "lr:  60  T =  250  loss:  6.241816520690918  acc:  0.8203099966049194\n",
      "lr:  60  T:  250  loss:  6.241816520690918  acc:  0.8203099966049194\n",
      "lr:  60  T =  300  loss:  13.87374496459961  acc:  0.7644099593162537\n",
      "lr:  60  T:  300  loss:  13.87374496459961  acc:  0.7644099593162537\n",
      "lr:  60  T =  350  loss:  3.9853928089141846  acc:  0.7546049952507019\n",
      "lr:  60  T:  350  loss:  3.9853928089141846  acc:  0.7546049952507019\n",
      "lr:  60  T =  400  loss:  4.229449272155762  acc:  0.8406599760055542\n",
      "lr:  60  T:  400  loss:  4.229449272155762  acc:  0.8406599760055542\n",
      "lr:  60  T =  450  loss:  1.395048975944519  acc:  0.8378399610519409\n",
      "lr:  60  T:  450  loss:  1.395048975944519  acc:  0.8378399610519409\n",
      "lr:  60  T =  500  loss:  1.6557347774505615  acc:  0.8527799844741821\n",
      "lr:  60  T:  500  loss:  1.6557347774505615  acc:  0.8527799844741821\n",
      "lr:  60  T =  550  loss:  2.1843152046203613  acc:  0.888384997844696\n",
      "lr:  60  T:  550  loss:  2.1843152046203613  acc:  0.888384997844696\n",
      "\n",
      "lr:  61  T =  50  loss:  25.453832626342773  acc:  0.7146250009536743\n",
      "lr:  61  T:  50  loss:  25.453832626342773  acc:  0.7146250009536743\n",
      "lr:  61  T =  100  loss:  10.357095718383789  acc:  0.7492199540138245\n",
      "lr:  61  T:  100  loss:  10.357095718383789  acc:  0.7492199540138245\n",
      "lr:  61  T =  150  loss:  16.618541717529297  acc:  0.5723949670791626\n",
      "lr:  61  T:  150  loss:  16.618541717529297  acc:  0.5723949670791626\n",
      "lr:  61  T =  200  loss:  3.413670539855957  acc:  0.6212300062179565\n",
      "lr:  61  T:  200  loss:  3.413670539855957  acc:  0.6212300062179565\n",
      "lr:  61  T =  250  loss:  4.933384895324707  acc:  0.7216249704360962\n",
      "lr:  61  T:  250  loss:  4.933384895324707  acc:  0.7216249704360962\n",
      "lr:  61  T =  300  loss:  3.3467514514923096  acc:  0.7920199632644653\n",
      "lr:  61  T:  300  loss:  3.3467514514923096  acc:  0.7920199632644653\n",
      "lr:  61  T =  350  loss:  2.3379595279693604  acc:  0.8187249898910522\n",
      "lr:  61  T:  350  loss:  2.3379595279693604  acc:  0.8187249898910522\n",
      "lr:  61  T =  400  loss:  1.9545120000839233  acc:  0.8174999952316284\n",
      "lr:  61  T:  400  loss:  1.9545120000839233  acc:  0.8174999952316284\n",
      "lr:  61  T =  450  loss:  1.482234239578247  acc:  0.8200699687004089\n",
      "lr:  61  T:  450  loss:  1.482234239578247  acc:  0.8200699687004089\n",
      "lr:  61  T =  500  loss:  1.6532821655273438  acc:  0.8458549976348877\n",
      "lr:  61  T:  500  loss:  1.6532821655273438  acc:  0.8458549976348877\n",
      "lr:  61  T =  550  loss:  2.1490442752838135  acc:  0.8470249772071838\n",
      "lr:  61  T:  550  loss:  2.1490442752838135  acc:  0.8470249772071838\n",
      "\n",
      "lr:  62  T =  50  loss:  20.69415855407715  acc:  0.7271549701690674\n",
      "lr:  62  T:  50  loss:  20.69415855407715  acc:  0.7271549701690674\n",
      "lr:  62  T =  100  loss:  12.260459899902344  acc:  0.7389900088310242\n",
      "lr:  62  T:  100  loss:  12.260459899902344  acc:  0.7389900088310242\n",
      "lr:  62  T =  150  loss:  13.942952156066895  acc:  0.8209750056266785\n",
      "lr:  62  T:  150  loss:  13.942952156066895  acc:  0.8209750056266785\n",
      "lr:  62  T =  200  loss:  10.416788101196289  acc:  0.8594499826431274\n",
      "lr:  62  T:  200  loss:  10.416788101196289  acc:  0.8594499826431274\n",
      "lr:  62  T =  250  loss:  7.749234199523926  acc:  0.9059450030326843\n",
      "lr:  62  T:  250  loss:  7.749234199523926  acc:  0.9059450030326843\n",
      "lr:  62  T =  300  loss:  7.450450897216797  acc:  0.9214049577713013\n",
      "lr:  62  T:  300  loss:  7.450450897216797  acc:  0.9214049577713013\n",
      "lr:  62  T =  350  loss:  7.427155494689941  acc:  0.921904981136322\n",
      "lr:  62  T:  350  loss:  7.427155494689941  acc:  0.921904981136322\n",
      "lr:  62  T =  400  loss:  7.412624359130859  acc:  0.9222899675369263\n",
      "lr:  62  T:  400  loss:  7.412624359130859  acc:  0.9222899675369263\n",
      "lr:  62  T =  450  loss:  7.394530296325684  acc:  0.922569990158081\n",
      "lr:  62  T:  450  loss:  7.394530296325684  acc:  0.922569990158081\n",
      "lr:  62  T =  500  loss:  7.373831272125244  acc:  0.9228149652481079\n",
      "lr:  62  T:  500  loss:  7.373831272125244  acc:  0.9228149652481079\n",
      "lr:  62  T =  550  loss:  7.35581111907959  acc:  0.9230349659919739\n",
      "lr:  62  T:  550  loss:  7.35581111907959  acc:  0.9230349659919739\n",
      "\n",
      "lr:  63  T =  50  loss:  22.56668472290039  acc:  0.7395449876785278\n",
      "lr:  63  T:  50  loss:  22.56668472290039  acc:  0.7395449876785278\n",
      "lr:  63  T =  100  loss:  16.696361541748047  acc:  0.7411850094795227\n",
      "lr:  63  T:  100  loss:  16.696361541748047  acc:  0.7411850094795227\n",
      "lr:  63  T =  150  loss:  12.313032150268555  acc:  0.7445849776268005\n",
      "lr:  63  T:  150  loss:  12.313032150268555  acc:  0.7445849776268005\n",
      "lr:  63  T =  200  loss:  7.066432476043701  acc:  0.7909849882125854\n",
      "lr:  63  T:  200  loss:  7.066432476043701  acc:  0.7909849882125854\n",
      "lr:  63  T =  250  loss:  3.617191791534424  acc:  0.7940499782562256\n",
      "lr:  63  T:  250  loss:  3.617191791534424  acc:  0.7940499782562256\n",
      "lr:  63  T =  300  loss:  3.337925910949707  acc:  0.8167849779129028\n",
      "lr:  63  T:  300  loss:  3.337925910949707  acc:  0.8167849779129028\n",
      "lr:  63  T =  350  loss:  2.8858389854431152  acc:  0.7671299576759338\n",
      "lr:  63  T:  350  loss:  2.8858389854431152  acc:  0.7671299576759338\n",
      "lr:  63  T =  400  loss:  1.4439876079559326  acc:  0.8392350077629089\n",
      "lr:  63  T:  400  loss:  1.4439876079559326  acc:  0.8392350077629089\n",
      "lr:  63  T =  450  loss:  3.2376277446746826  acc:  0.8019700050354004\n",
      "lr:  63  T:  450  loss:  3.2376277446746826  acc:  0.8019700050354004\n",
      "lr:  63  T =  500  loss:  2.1345956325531006  acc:  0.8317300081253052\n",
      "lr:  63  T:  500  loss:  2.1345956325531006  acc:  0.8317300081253052\n",
      "lr:  63  T =  550  loss:  1.911356806755066  acc:  0.8564049601554871\n",
      "lr:  63  T:  550  loss:  1.911356806755066  acc:  0.8564049601554871\n",
      "\n",
      "lr:  64  T =  50  loss:  26.576356887817383  acc:  0.7257800102233887\n",
      "lr:  64  T:  50  loss:  26.576356887817383  acc:  0.7257800102233887\n",
      "lr:  64  T =  100  loss:  23.299776077270508  acc:  0.7513249516487122\n",
      "lr:  64  T:  100  loss:  23.299776077270508  acc:  0.7513249516487122\n",
      "lr:  64  T =  150  loss:  15.89895248413086  acc:  0.7689799666404724\n",
      "lr:  64  T:  150  loss:  15.89895248413086  acc:  0.7689799666404724\n",
      "lr:  64  T =  200  loss:  4.46252965927124  acc:  0.7637099623680115\n",
      "lr:  64  T:  200  loss:  4.46252965927124  acc:  0.7637099623680115\n",
      "lr:  64  T =  250  loss:  12.881518363952637  acc:  0.8303849697113037\n",
      "lr:  64  T:  250  loss:  12.881518363952637  acc:  0.8303849697113037\n",
      "lr:  64  T =  300  loss:  8.857754707336426  acc:  0.8656299710273743\n",
      "lr:  64  T:  300  loss:  8.857754707336426  acc:  0.8656299710273743\n",
      "lr:  64  T =  350  loss:  6.437074184417725  acc:  0.8933899998664856\n",
      "lr:  64  T:  350  loss:  6.437074184417725  acc:  0.8933899998664856\n",
      "lr:  64  T =  400  loss:  4.041851043701172  acc:  0.8438999652862549\n",
      "lr:  64  T:  400  loss:  4.041851043701172  acc:  0.8438999652862549\n",
      "lr:  64  T =  450  loss:  3.8673675060272217  acc:  0.904229998588562\n",
      "lr:  64  T:  450  loss:  3.8673675060272217  acc:  0.904229998588562\n",
      "lr:  64  T =  500  loss:  4.316995620727539  acc:  0.9204999804496765\n",
      "lr:  64  T:  500  loss:  4.316995620727539  acc:  0.9204999804496765\n",
      "lr:  64  T =  550  loss:  2.773019552230835  acc:  0.9416149854660034\n",
      "lr:  64  T:  550  loss:  2.773019552230835  acc:  0.9416149854660034\n",
      "\n",
      "lr:  65  T =  50  loss:  18.12516975402832  acc:  0.6429849863052368\n",
      "lr:  65  T:  50  loss:  18.12516975402832  acc:  0.6429849863052368\n",
      "lr:  65  T =  100  loss:  15.75430965423584  acc:  0.7861649990081787\n",
      "lr:  65  T:  100  loss:  15.75430965423584  acc:  0.7861649990081787\n",
      "lr:  65  T =  150  loss:  7.255085468292236  acc:  0.8393449783325195\n",
      "lr:  65  T:  150  loss:  7.255085468292236  acc:  0.8393449783325195\n",
      "lr:  65  T =  200  loss:  14.98234748840332  acc:  0.7644699811935425\n",
      "lr:  65  T:  200  loss:  14.98234748840332  acc:  0.7644699811935425\n",
      "lr:  65  T =  250  loss:  9.448012351989746  acc:  0.7420600056648254\n",
      "lr:  65  T:  250  loss:  9.448012351989746  acc:  0.7420600056648254\n",
      "lr:  65  T =  300  loss:  4.873420715332031  acc:  0.817234992980957\n",
      "lr:  65  T:  300  loss:  4.873420715332031  acc:  0.817234992980957\n",
      "lr:  65  T =  350  loss:  6.251082897186279  acc:  0.7943549752235413\n",
      "lr:  65  T:  350  loss:  6.251082897186279  acc:  0.7943549752235413\n",
      "lr:  65  T =  400  loss:  1.4071547985076904  acc:  0.8918449878692627\n",
      "lr:  65  T:  400  loss:  1.4071547985076904  acc:  0.8918449878692627\n",
      "lr:  65  T =  450  loss:  1.249720573425293  acc:  0.9073099493980408\n",
      "lr:  65  T:  450  loss:  1.249720573425293  acc:  0.9073099493980408\n",
      "lr:  65  T =  500  loss:  1.1988111734390259  acc:  0.8934999704360962\n",
      "lr:  65  T:  500  loss:  1.1988111734390259  acc:  0.8934999704360962\n",
      "lr:  65  T =  550  loss:  1.144842267036438  acc:  0.9364199638366699\n",
      "lr:  65  T:  550  loss:  1.144842267036438  acc:  0.9364199638366699\n",
      "\n",
      "lr:  66  T =  50  loss:  26.76390266418457  acc:  0.7246099710464478\n",
      "lr:  66  T:  50  loss:  26.76390266418457  acc:  0.7246099710464478\n",
      "lr:  66  T =  100  loss:  13.95723819732666  acc:  0.7553349733352661\n",
      "lr:  66  T:  100  loss:  13.95723819732666  acc:  0.7553349733352661\n",
      "lr:  66  T =  150  loss:  14.903542518615723  acc:  0.7949699759483337\n",
      "lr:  66  T:  150  loss:  14.903542518615723  acc:  0.7949699759483337\n",
      "lr:  66  T =  200  loss:  16.604040145874023  acc:  0.7898949980735779\n",
      "lr:  66  T:  200  loss:  16.604040145874023  acc:  0.7898949980735779\n",
      "lr:  66  T =  250  loss:  6.552257537841797  acc:  0.7376049757003784\n",
      "lr:  66  T:  250  loss:  6.552257537841797  acc:  0.7376049757003784\n",
      "lr:  66  T =  300  loss:  5.695700645446777  acc:  0.793554961681366\n",
      "lr:  66  T:  300  loss:  5.695700645446777  acc:  0.793554961681366\n",
      "lr:  66  T =  350  loss:  2.906728744506836  acc:  0.8266449570655823\n",
      "lr:  66  T:  350  loss:  2.906728744506836  acc:  0.8266449570655823\n",
      "lr:  66  T =  400  loss:  2.761103391647339  acc:  0.776229977607727\n",
      "lr:  66  T:  400  loss:  2.761103391647339  acc:  0.776229977607727\n",
      "lr:  66  T =  450  loss:  2.248608112335205  acc:  0.7748450040817261\n",
      "lr:  66  T:  450  loss:  2.248608112335205  acc:  0.7748450040817261\n",
      "lr:  66  T =  500  loss:  2.1203665733337402  acc:  0.8292049765586853\n",
      "lr:  66  T:  500  loss:  2.1203665733337402  acc:  0.8292049765586853\n",
      "lr:  66  T =  550  loss:  1.2266489267349243  acc:  0.8742150068283081\n",
      "lr:  66  T:  550  loss:  1.2266489267349243  acc:  0.8742150068283081\n",
      "\n",
      "lr:  67  T =  50  loss:  26.740644454956055  acc:  0.7266950011253357\n",
      "lr:  67  T:  50  loss:  26.740644454956055  acc:  0.7266950011253357\n",
      "lr:  67  T =  100  loss:  24.04037857055664  acc:  0.7392950057983398\n",
      "lr:  67  T:  100  loss:  24.04037857055664  acc:  0.7392950057983398\n",
      "lr:  67  T =  150  loss:  15.729772567749023  acc:  0.7623199820518494\n",
      "lr:  67  T:  150  loss:  15.729772567749023  acc:  0.7623199820518494\n",
      "lr:  67  T =  200  loss:  10.659810066223145  acc:  0.8018199801445007\n",
      "lr:  67  T:  200  loss:  10.659810066223145  acc:  0.8018199801445007\n",
      "lr:  67  T =  250  loss:  8.250069618225098  acc:  0.7871949672698975\n",
      "lr:  67  T:  250  loss:  8.250069618225098  acc:  0.7871949672698975\n",
      "lr:  67  T =  300  loss:  5.267323017120361  acc:  0.8235999941825867\n",
      "lr:  67  T:  300  loss:  5.267323017120361  acc:  0.8235999941825867\n",
      "lr:  67  T =  350  loss:  4.4960198402404785  acc:  0.8027399778366089\n",
      "lr:  67  T:  350  loss:  4.4960198402404785  acc:  0.8027399778366089\n",
      "lr:  67  T =  400  loss:  4.287238121032715  acc:  0.7651949524879456\n",
      "lr:  67  T:  400  loss:  4.287238121032715  acc:  0.7651949524879456\n",
      "lr:  67  T =  450  loss:  2.5292611122131348  acc:  0.8062599897384644\n",
      "lr:  67  T:  450  loss:  2.5292611122131348  acc:  0.8062599897384644\n",
      "lr:  67  T =  500  loss:  1.702469825744629  acc:  0.8464549779891968\n",
      "lr:  67  T:  500  loss:  1.702469825744629  acc:  0.8464549779891968\n",
      "lr:  67  T =  550  loss:  1.4210197925567627  acc:  0.8801049590110779\n",
      "lr:  67  T:  550  loss:  1.4210197925567627  acc:  0.8801049590110779\n",
      "\n",
      "lr:  68  T =  50  loss:  26.87677001953125  acc:  0.727180004119873\n",
      "lr:  68  T:  50  loss:  26.87677001953125  acc:  0.727180004119873\n",
      "lr:  68  T =  100  loss:  24.557004928588867  acc:  0.7446099519729614\n",
      "lr:  68  T:  100  loss:  24.557004928588867  acc:  0.7446099519729614\n",
      "lr:  68  T =  150  loss:  20.226396560668945  acc:  0.7713249921798706\n",
      "lr:  68  T:  150  loss:  20.226396560668945  acc:  0.7713249921798706\n",
      "lr:  68  T =  200  loss:  18.345436096191406  acc:  0.7438349723815918\n",
      "lr:  68  T:  200  loss:  18.345436096191406  acc:  0.7438349723815918\n",
      "lr:  68  T =  250  loss:  11.455057144165039  acc:  0.7943199872970581\n",
      "lr:  68  T:  250  loss:  11.455057144165039  acc:  0.7943199872970581\n",
      "lr:  68  T =  300  loss:  8.836545944213867  acc:  0.812654972076416\n",
      "lr:  68  T:  300  loss:  8.836545944213867  acc:  0.812654972076416\n",
      "lr:  68  T =  350  loss:  3.720714569091797  acc:  0.8172149658203125\n",
      "lr:  68  T:  350  loss:  3.720714569091797  acc:  0.8172149658203125\n",
      "lr:  68  T =  400  loss:  3.369374990463257  acc:  0.8419950008392334\n",
      "lr:  68  T:  400  loss:  3.369374990463257  acc:  0.8419950008392334\n",
      "lr:  68  T =  450  loss:  2.2826151847839355  acc:  0.8258699774742126\n",
      "lr:  68  T:  450  loss:  2.2826151847839355  acc:  0.8258699774742126\n",
      "lr:  68  T =  500  loss:  2.178704023361206  acc:  0.8769499659538269\n",
      "lr:  68  T:  500  loss:  2.178704023361206  acc:  0.8769499659538269\n",
      "lr:  68  T =  550  loss:  1.287845492362976  acc:  0.8766549825668335\n",
      "lr:  68  T:  550  loss:  1.287845492362976  acc:  0.8766549825668335\n",
      "\n",
      "lr:  69  T =  50  loss:  18.54938507080078  acc:  0.7437949776649475\n",
      "lr:  69  T:  50  loss:  18.54938507080078  acc:  0.7437949776649475\n",
      "lr:  69  T =  100  loss:  18.681798934936523  acc:  0.7437899708747864\n",
      "lr:  69  T:  100  loss:  18.681798934936523  acc:  0.7437899708747864\n",
      "lr:  69  T =  150  loss:  10.040761947631836  acc:  0.7707299590110779\n",
      "lr:  69  T:  150  loss:  10.040761947631836  acc:  0.7707299590110779\n",
      "lr:  69  T =  200  loss:  9.199151992797852  acc:  0.7823950052261353\n",
      "lr:  69  T:  200  loss:  9.199151992797852  acc:  0.7823950052261353\n",
      "lr:  69  T =  250  loss:  6.539398670196533  acc:  0.7634849548339844\n",
      "lr:  69  T:  250  loss:  6.539398670196533  acc:  0.7634849548339844\n",
      "lr:  69  T =  300  loss:  3.4611668586730957  acc:  0.8238599896430969\n",
      "lr:  69  T:  300  loss:  3.4611668586730957  acc:  0.8238599896430969\n",
      "lr:  69  T =  350  loss:  2.580963611602783  acc:  0.8488049507141113\n",
      "lr:  69  T:  350  loss:  2.580963611602783  acc:  0.8488049507141113\n",
      "lr:  69  T =  400  loss:  1.9266955852508545  acc:  0.8326299786567688\n",
      "lr:  69  T:  400  loss:  1.9266955852508545  acc:  0.8326299786567688\n",
      "lr:  69  T =  450  loss:  1.8817914724349976  acc:  0.85139000415802\n",
      "lr:  69  T:  450  loss:  1.8817914724349976  acc:  0.85139000415802\n",
      "lr:  69  T =  500  loss:  2.5538296699523926  acc:  0.8515849709510803\n",
      "lr:  69  T:  500  loss:  2.5538296699523926  acc:  0.8515849709510803\n",
      "lr:  69  T =  550  loss:  1.2958592176437378  acc:  0.9062950015068054\n",
      "lr:  69  T:  550  loss:  1.2958592176437378  acc:  0.9062950015068054\n",
      "\n",
      "lr:  70  T =  50  loss:  26.777559280395508  acc:  0.7190699577331543\n",
      "lr:  70  T:  50  loss:  26.777559280395508  acc:  0.7190699577331543\n",
      "lr:  70  T =  100  loss:  19.096385955810547  acc:  0.742805004119873\n",
      "lr:  70  T:  100  loss:  19.096385955810547  acc:  0.742805004119873\n",
      "lr:  70  T =  150  loss:  7.99574089050293  acc:  0.6612150073051453\n",
      "lr:  70  T:  150  loss:  7.99574089050293  acc:  0.6612150073051453\n",
      "lr:  70  T =  200  loss:  3.082505464553833  acc:  0.7253899574279785\n",
      "lr:  70  T:  200  loss:  3.082505464553833  acc:  0.7253899574279785\n",
      "lr:  70  T =  250  loss:  10.372579574584961  acc:  0.8036499619483948\n",
      "lr:  70  T:  250  loss:  10.372579574584961  acc:  0.8036499619483948\n",
      "lr:  70  T =  300  loss:  4.768772602081299  acc:  0.7457799911499023\n",
      "lr:  70  T:  300  loss:  4.768772602081299  acc:  0.7457799911499023\n",
      "lr:  70  T =  350  loss:  3.309774875640869  acc:  0.7803449630737305\n",
      "lr:  70  T:  350  loss:  3.309774875640869  acc:  0.7803449630737305\n",
      "lr:  70  T =  400  loss:  2.443429946899414  acc:  0.8294199705123901\n",
      "lr:  70  T:  400  loss:  2.443429946899414  acc:  0.8294199705123901\n",
      "lr:  70  T =  450  loss:  3.2418370246887207  acc:  0.8297449946403503\n",
      "lr:  70  T:  450  loss:  3.2418370246887207  acc:  0.8297449946403503\n",
      "lr:  70  T =  500  loss:  1.0698118209838867  acc:  0.8854050040245056\n",
      "lr:  70  T:  500  loss:  1.0698118209838867  acc:  0.8854050040245056\n",
      "lr:  70  T =  550  loss:  1.0125133991241455  acc:  0.8854299783706665\n",
      "lr:  70  T:  550  loss:  1.0125133991241455  acc:  0.8854299783706665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use gpu \n",
    "use_gpu = 6\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load users, items and ratings as tensors\n",
    "users = torch.tensor(user_list, device = device)\n",
    "items = torch.tensor(item_list, device = device)\n",
    "ratings = torch.tensor(rating_list, device = device)\n",
    "ratings = ratings.reshape((len(ratings), 1))\n",
    "\n",
    "# define model and it's parameters\n",
    "n_factors = 64\n",
    "lr = 65\n",
    "T = 501\n",
    "\n",
    "# for lr in [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 1000]:\n",
    "for lr in range(60, 71):\n",
    "\n",
    "    for T in [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550]:\n",
    "\n",
    "        model = NCF(n_users, n_items, n_factors)\n",
    "        # model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "        model.to(device)\n",
    "        # optimizer = torch.optim.Adam(model.parameters())\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        for layer in model.children():\n",
    "            layer.reset_parameters()\n",
    "\n",
    "        loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "        model.train()\n",
    "\n",
    "        for i in range(T):\n",
    "            y_hat = model(users, items)\n",
    "            loss = loss_fn(y_hat, ratings)\n",
    "            if (i % (T - 1) == 0) and  (i > 0):\n",
    "                print('lr: ', lr, ' T = ', i + 1, ' loss: ', loss.item(), ' acc: ', get_accuracy(y_hat, ratings))\n",
    "\n",
    "            # use torch.optim optimizer to compute gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True, create_graph=False)\n",
    "            optimizer.step()\n",
    "        print('lr: ', lr, ' T: ', T, ' loss: ', loss.item(), ' acc: ', get_accuracy(y_hat, ratings))\n",
    "    print()\n",
    "\n",
    "# LR = 62 and T = 250 seems to do the job very well "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f38835821387ecea7238337192aa99e87ed1a9c9c1fa6562e207de7e0c31193"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('PyG': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
