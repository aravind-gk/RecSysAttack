{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Library imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pyforest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "from turtle import forward\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparams and loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edges = np.load('data/train_edges.npy')\n",
    "user_list = train_edges[:, 0]\n",
    "item_list = train_edges[:, 1]\n",
    "rating_list = train_edges[:, 2].astype('float32')\n",
    "\n",
    "n_users = 943 \n",
    "n_items = 1682\n",
    "n_samples = len(rating_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining collaborative filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(CollaborativeFiltering, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        dot = (u * i).sum(1)\n",
    "        return torch.sigmoid(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Code for meta attack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Learning rate:  1\n",
      "-> T:  10\n",
      "-> Delta:  10\n",
      "-> Embedding size:  64\n",
      "-> Execution time: 00h 00m 20s\n"
     ]
    }
   ],
   "source": [
    "# start execution\n",
    "start_time = time.time()\n",
    "\n",
    "# some hyperparams\n",
    "lr = 1\n",
    "T = 10\n",
    "Delta = 10\n",
    "n_factors = 64\n",
    "\n",
    "# store loss results in this list and later convert to dataframe \n",
    "results = []\n",
    "\n",
    "# list of perturbations\n",
    "perturbations = dict()\n",
    "perturbations['edges'] = []\n",
    "perturbations['metagrad'] = []\n",
    "\n",
    "# print hyperparam config\n",
    "print('-> Learning rate: ', lr)\n",
    "print('-> T: ', T)\n",
    "print('-> Delta: ', Delta)\n",
    "print('-> Embedding size: ', n_factors)\n",
    "\n",
    "for delta in range(Delta):\n",
    "    # reload the users, items and ratings tensors\n",
    "    users = torch.LongTensor(train_edges[:, 0])\n",
    "    items = torch.LongTensor(train_edges[:, 1])\n",
    "    ratings = torch.FloatTensor(train_edges[:, 2])\n",
    "\n",
    "    # add those perturbations to \"ratings\"\n",
    "    for index in perturbations['edges']:\n",
    "        ratings[index] = 1\n",
    "\n",
    "    # set requires_grad for ratings, to compute meta gradients\n",
    "    ratings.requires_grad_()\n",
    "\n",
    "    # makes code reproducible\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # define model and loss\n",
    "    model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "    p1, p2 = model.parameters()\n",
    "    loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "    model.train()\n",
    "\n",
    "    # inner loop training process\n",
    "    for i in range(T):\n",
    "        y_hat = model(users, items)\n",
    "        loss = loss_fn(y_hat, ratings)\n",
    "        results.append([delta, i, loss.item()])\n",
    "        \n",
    "        p1_grad = torch.autograd.grad(loss, p1, create_graph=True)\n",
    "        p2_grad = torch.autograd.grad(loss, p2, create_graph=True)\n",
    "\n",
    "        # compute inner parameter gradients\n",
    "        with torch.no_grad():\n",
    "            p1_new = p1 - lr * p1_grad[0]\n",
    "            p2_new = p2 - lr * p2_grad[0]\n",
    "            p1.copy_(p1_new)\n",
    "            p2.copy_(p2_new)\n",
    "    \n",
    "    # compute meta gradient\n",
    "    meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "\n",
    "    # select best edge to perturb\n",
    "    max_meta_grad = -math.inf\n",
    "    edge_to_add = -1\n",
    "    for i in range(n_samples):\n",
    "        if ratings[i] == 0: # search over only negative edges\n",
    "            if meta_grad[i] > max_meta_grad:\n",
    "                max_meta_grad = meta_grad[i]\n",
    "                edge_to_add = i \n",
    "\n",
    "    perturbations['edges'].append(edge_to_add)\n",
    "    perturbations['metagrad'].append(max_meta_grad.item())\n",
    "\n",
    "# compute execution time\n",
    "exec_time = int(time.time() - start_time)\n",
    "exec_time = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(exec_time))\n",
    "print('-> Execution time: {}'.format(exec_time))\n",
    "\n",
    "# store results in CSV files\n",
    "results = pd.DataFrame(results, columns = ['perturbs', 'iters', 'loss'])\n",
    "results.to_csv('results/losses_Delta={}_T={}_LR={}_Factors={}'.format(Delta, T, lr, n_factors))\n",
    "\n",
    "perturbations = pd.DataFrame(perturbations)\n",
    "perturbations.to_csv('results/perturbations_Delta={}_T={}_LR={}_Factors={}'.format(Delta, T, lr, n_factors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Checking stored results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edges</th>\n",
       "      <th>metagrad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141472</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152311</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173390</td>\n",
       "      <td>0.000174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5665</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112886</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    edges  metagrad\n",
       "0  141472  0.000193\n",
       "1  152311  0.000175\n",
       "2  173390  0.000174\n",
       "3    5665  0.000164\n",
       "4  112886  0.000164"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perturbs</th>\n",
       "      <th>iters</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.066596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.065507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.064427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.062918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.061419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   perturbs  iters      loss\n",
       "0         0      0  4.066596\n",
       "1         0      1  4.065507\n",
       "2         0      2  4.064427\n",
       "3         0      3  4.062918\n",
       "4         0      4  4.061419"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import torch'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Code for meta attack in GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Learning rate:  1\n",
      "-> T:  10\n",
      "-> Delta:  1\n",
      "-> Embedding size:  64\n",
      "-> Device:  cuda:0\n",
      "-> Execution time: 00h 00m 09s\n"
     ]
    }
   ],
   "source": [
    "# start execution\n",
    "start_time = time.time()\n",
    "\n",
    "# GPU settings (set use_gpu = -1 if you want to use CPU)\n",
    "use_gpu = 0\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# some hyperparams\n",
    "lr = 1\n",
    "T = 10\n",
    "Delta = 1\n",
    "n_factors = 64\n",
    "\n",
    "# store loss results in this list and later convert to dataframe \n",
    "results = []\n",
    "\n",
    "# list of perturbations\n",
    "perturbations = dict()\n",
    "perturbations['edges'] = []\n",
    "perturbations['metagrad'] = []\n",
    "\n",
    "# print hyperparam config\n",
    "print('-> Learning rate: ', lr)\n",
    "print('-> T: ', T)\n",
    "print('-> Delta: ', Delta)\n",
    "print('-> Embedding size: ', n_factors)\n",
    "print('-> Device: ', device)\n",
    "\n",
    "# Load users and items data as tensors\n",
    "users = torch.tensor(user_list, device = device)\n",
    "items = torch.tensor(item_list, device = device)\n",
    "ratings = torch.tensor(rating_list, device = device, requires_grad = True)\n",
    "\n",
    "# define model\n",
    "model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "model.to(device)\n",
    "p1, p2 = model.parameters()\n",
    "\n",
    "for delta in range(Delta):\n",
    "    # reload the ratings tensor\n",
    "    # ratings = torch.tensor(rating_list, device = device, requires_grad = True)\n",
    "\n",
    "    # add those perturbations to \"ratings\" (think of a more efficient way to do this)\n",
    "    # for index in perturbations['edges']:\n",
    "        # ratings[index] = 1\n",
    "\n",
    "    # makes code reproducible\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # reset model paramters \n",
    "    for layer in model.children():\n",
    "        layer.reset_parameters()\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # inner loop training process\n",
    "    for i in range(T):\n",
    "        y_hat = model(users, items)\n",
    "        loss = loss_fn(y_hat, ratings)\n",
    "        results.append([delta, i, loss.item()])\n",
    "        \n",
    "        p1_grad = torch.autograd.grad(loss, p1, create_graph=True)\n",
    "        p2_grad = torch.autograd.grad(loss, p2, create_graph=True)\n",
    "\n",
    "        # compute inner parameter gradients\n",
    "        with torch.no_grad():\n",
    "            p1_new = p1 - lr * p1_grad[0]\n",
    "            p2_new = p2 - lr * p2_grad[0]\n",
    "            p1.copy_(p1_new)\n",
    "            p2.copy_(p2_new)\n",
    "    \n",
    "    # compute meta gradient\n",
    "    meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "\n",
    "    # select best edge to perturb\n",
    "    max_meta_grad = -math.inf\n",
    "    edge_to_add = -1\n",
    "    for i in range(n_samples):\n",
    "        if ratings[i] == 0: # search over only negative edges\n",
    "            if meta_grad[i] > max_meta_grad:\n",
    "                max_meta_grad = meta_grad[i]\n",
    "                edge_to_add = i \n",
    "\n",
    "    perturbations['edges'].append(edge_to_add)\n",
    "    perturbations['metagrad'].append(max_meta_grad.item())\n",
    "\n",
    "    # perform one perturbation\n",
    "    with torch.no_grad():\n",
    "        ratings[edge_to_add] = 1\n",
    "\n",
    "# compute execution time\n",
    "exec_time = int(time.time() - start_time)\n",
    "exec_time = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(exec_time))\n",
    "print('-> Execution time: {}'.format(exec_time))\n",
    "\n",
    "# store results in CSV files\n",
    "results = pd.DataFrame(results, columns = ['perturbs', 'iters', 'loss'])\n",
    "# results.to_csv('results/losses_Delta={}_T={}_LR={}_Factors={}'.format(Delta, T, lr, n_factors))\n",
    "\n",
    "perturbations = pd.DataFrame(perturbations)\n",
    "# perturbations.to_csv('results/perturbations_Delta={}_T={}_LR={}_Factors={}'.format(Delta, T, lr, n_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#GPUs:  8\n"
     ]
    }
   ],
   "source": [
    "# check available devices\n",
    "print('#GPUs: ', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Some experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device:  cuda:0\n",
      "model params in gpu:  True\n",
      "users in gpu:  True\n",
      "items in gpu:  True\n",
      "ratings in gpu:  True\n",
      "p1:  Parameter containing:\n",
      "tensor([[-1.1258, -1.1524, -0.2506,  ..., -1.2341,  1.8197, -0.5515],\n",
      "        [-0.5692,  0.9200,  1.1108,  ...,  1.1648,  0.9234,  1.3873],\n",
      "        [-0.8834, -0.4189, -0.8048,  ..., -0.9944, -1.1894, -1.1959],\n",
      "        ...,\n",
      "        [-1.4320, -0.8343,  1.2806,  ...,  0.0558, -0.6904, -0.5621],\n",
      "        [-0.2673,  1.4002,  0.3839,  ..., -1.3639, -0.1925,  0.8499],\n",
      "        [ 0.7665,  0.2639,  1.3664,  ..., -0.2584, -0.1347, -1.5129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "tensor([1., 1., 0.,  ..., 1., 1., 0.], device='cuda:0')\n",
      "torch.float32\n",
      "torch.float32\n",
      "p1:  Parameter containing:\n",
      "tensor([[-1.1257, -1.1521, -0.2505,  ..., -1.2341,  1.8195, -0.5513],\n",
      "        [-0.5693,  0.9200,  1.1108,  ...,  1.1648,  0.9234,  1.3872],\n",
      "        [-0.8833, -0.4189, -0.8048,  ..., -0.9943, -1.1893, -1.1959],\n",
      "        ...,\n",
      "        [-1.4320, -0.8343,  1.2805,  ...,  0.0558, -0.6904, -0.5622],\n",
      "        [-0.2672,  1.4002,  0.3839,  ..., -1.3638, -0.1925,  0.8499],\n",
      "        [ 0.7665,  0.2638,  1.3663,  ..., -0.2583, -0.1346, -1.5128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p1:  Parameter containing:\n",
      "tensor([[-0.9247, -0.4253, -2.6438,  ...,  0.1210,  0.4730, -1.0823],\n",
      "        [-0.0334, -0.9734,  0.9559,  ..., -0.2128, -0.3315, -0.2023],\n",
      "        [-1.1451, -0.5715, -0.6510,  ...,  0.2144, -0.7369, -0.4516],\n",
      "        ...,\n",
      "        [ 0.6540, -0.5414, -1.3338,  ...,  0.8598,  1.8550,  1.4760],\n",
      "        [ 0.0534,  0.3653,  0.5379,  ...,  1.2643,  1.0971, -0.3906],\n",
      "        [-0.5607, -1.5466,  0.2973,  ..., -1.5469,  0.2791, -0.4792]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "use_gpu = 0\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "print('current device: ', device)\n",
    "\n",
    "model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "model.to(device)\n",
    "print('model params in gpu: ', next(model.parameters()).is_cuda)\n",
    "\n",
    "users = torch.tensor(user_list, device = device)\n",
    "items = torch.tensor(item_list, device = device)\n",
    "ratings = torch.tensor(rating_list, device = device)\n",
    "print('users in gpu: ', users.is_cuda)\n",
    "print('items in gpu: ', items.is_cuda)\n",
    "print('ratings in gpu: ', ratings.is_cuda)\n",
    "\n",
    "p1, p2 = model.parameters()\n",
    "print('p1: ', p1)\n",
    "# print(type(p1))\n",
    "# print('p1 in gpu: ', p1.is_cuda)\n",
    "# print(p2.is_cuda)\n",
    "\n",
    "model.train()\n",
    "y_hat = model(users, items)\n",
    "# print('y hat: ', y_hat)\n",
    "# print('y hat in gpu: ', y_hat.is_cuda)\n",
    "\n",
    "print(ratings)\n",
    "print(y_hat.dtype)\n",
    "print(ratings.dtype)\n",
    "\n",
    "loss = loss_fn(y_hat, ratings)\n",
    "# print('loss: ', loss)\n",
    "\n",
    "p1_grad = torch.autograd.grad(loss, p1, create_graph=True)\n",
    "# print('p1 grad: ', p1_grad)\n",
    "with torch.no_grad():\n",
    "    p1_new = p1 - lr * p1_grad[0]\n",
    "    p1.copy_(p1_new)\n",
    "\n",
    "print('p1: ', p1)\n",
    "\n",
    "for layer in model.children():\n",
    "    layer.reset_parameters()\n",
    "\n",
    "print('p1: ', p1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 0.,  ..., 1., 1., 0.], device='cuda:0', requires_grad=True)\n",
      "None\n",
      "tensor([-1.8709e-05,  4.7379e-05,  2.3085e-05,  ...,  9.8168e-05,\n",
      "         5.6827e-05,  5.9913e-08], device='cuda:0')\n",
      "None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3250222/2934864943.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmeta_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyG/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    234\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "ratings = torch.tensor(rating_list, device = device, requires_grad=True)\n",
    "y_hat = model(users, items)\n",
    "loss = loss_fn(y_hat, ratings)\n",
    "print(ratings)\n",
    "print(ratings.grad)\n",
    "meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "print(meta_grad)\n",
    "print(ratings.grad)\n",
    "meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "print(meta_grad)\n",
    "print(ratings.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 0.,  ..., 1., 1., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3250222/3927878834.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeta_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyG/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    234\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "print(meta_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollaborativeFiltering(\n",
       "  (user_emb): Embedding(943, 64)\n",
       "  (item_emb): Embedding(1682, 64)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.children():\n",
    "    layer.reset_parameters()\n",
    "    # if hasattr(layer, 'reset_parameters'):\n",
    "    #     layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollaborativeFiltering(\n",
       "  (user_emb): Embedding(943, 64)\n",
       "  (item_emb): Embedding(1682, 64)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1:  Parameter containing:\n",
      "tensor([[-0.9247, -0.4253, -2.6438,  ...,  0.1210,  0.4730, -1.0823],\n",
      "        [-0.0334, -0.9734,  0.9559,  ..., -0.2128, -0.3315, -0.2023],\n",
      "        [-1.1451, -0.5715, -0.6510,  ...,  0.2144, -0.7369, -0.4516],\n",
      "        ...,\n",
      "        [ 0.6540, -0.5414, -1.3338,  ...,  0.8598,  1.8550,  1.4760],\n",
      "        [ 0.0534,  0.3653,  0.5379,  ...,  1.2643,  1.0971, -0.3906],\n",
      "        [-0.5607, -1.5466,  0.2973,  ..., -1.5469,  0.2791, -0.4792]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p1:  Parameter containing:\n",
      "tensor([[-0.9247, -0.4253, -2.6438,  ...,  0.1210,  0.4730, -1.0823],\n",
      "        [-0.0334, -0.9734,  0.9559,  ..., -0.2128, -0.3315, -0.2023],\n",
      "        [-1.1451, -0.5715, -0.6510,  ...,  0.2144, -0.7369, -0.4516],\n",
      "        ...,\n",
      "        [ 0.6540, -0.5414, -1.3338,  ...,  0.8598,  1.8550,  1.4760],\n",
      "        [ 0.0534,  0.3653,  0.5379,  ...,  1.2643,  1.0971, -0.3906],\n",
      "        [-0.5607, -1.5466,  0.2973,  ..., -1.5469,  0.2791, -0.4792]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p1:  Parameter containing:\n",
      "tensor([[-0.9247, -0.4253, -2.6438,  ...,  0.1210,  0.4730, -1.0823],\n",
      "        [-0.0334, -0.9734,  0.9559,  ..., -0.2128, -0.3315, -0.2023],\n",
      "        [-1.1451, -0.5715, -0.6510,  ...,  0.2144, -0.7369, -0.4516],\n",
      "        ...,\n",
      "        [ 0.6540, -0.5414, -1.3338,  ...,  0.8598,  1.8550,  1.4760],\n",
      "        [ 0.0534,  0.3653,  0.5379,  ...,  1.2643,  1.0971, -0.3906],\n",
      "        [-0.5607, -1.5466,  0.2973,  ..., -1.5469,  0.2791, -0.4792]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p1:  Parameter containing:\n",
      "tensor([[-0.9247, -0.4253, -2.6438,  ...,  0.1210,  0.4730, -1.0823],\n",
      "        [-0.0334, -0.9734,  0.9559,  ..., -0.2128, -0.3315, -0.2023],\n",
      "        [-1.1451, -0.5715, -0.6510,  ...,  0.2144, -0.7369, -0.4516],\n",
      "        ...,\n",
      "        [ 0.6540, -0.5414, -1.3338,  ...,  0.8598,  1.8550,  1.4760],\n",
      "        [ 0.0534,  0.3653,  0.5379,  ...,  1.2643,  1.0971, -0.3906],\n",
      "        [-0.5607, -1.5466,  0.2973,  ..., -1.5469,  0.2791, -0.4792]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p1:  Parameter containing:\n",
      "tensor([[-0.9247, -0.4253, -2.6438,  ...,  0.1210,  0.4730, -1.0823],\n",
      "        [-0.0334, -0.9734,  0.9559,  ..., -0.2128, -0.3315, -0.2023],\n",
      "        [-1.1451, -0.5715, -0.6510,  ...,  0.2144, -0.7369, -0.4516],\n",
      "        ...,\n",
      "        [ 0.6540, -0.5414, -1.3338,  ...,  0.8598,  1.8550,  1.4760],\n",
      "        [ 0.0534,  0.3653,  0.5379,  ...,  1.2643,  1.0971, -0.3906],\n",
      "        [-0.5607, -1.5466,  0.2973,  ..., -1.5469,  0.2791, -0.4792]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "model.to(device)\n",
    "for _ in range(5):\n",
    "    torch.manual_seed(0)\n",
    "    for layer in model.children():\n",
    "        layer.reset_parameters()\n",
    "    p1, p2 = model.parameters()\n",
    "    print('p1: ', p1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Measure execution time for various operations in CPU and GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "-> Execution time: 00h 00m 07s\n"
     ]
    }
   ],
   "source": [
    "# choose device\n",
    "use_gpu = -1\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "\n",
    "# start execution\n",
    "start_time = time.time()\n",
    "\n",
    "# code\n",
    "\n",
    "for _ in range(300000):\n",
    "    ratings = torch.tensor(rating_list, device = device, requires_grad=True)\n",
    "    # ratings = torch.tensor(rating_list, device = device, requires_grad=False)\n",
    "    # ratings = torch.tensor(rating_list, device = device, requires_grad=False)\n",
    "\n",
    "# compute execution time\n",
    "exec_time = int(time.time() - start_time)\n",
    "exec_time = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(exec_time))\n",
    "print('-> Execution time: {}'.format(exec_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to initialise users, items and ratings 100,000 times,\n",
    "# cpu takes 8 seconds, and gpu takes 56 seconds\n",
    "\n",
    "# to initialise users, items and ratings 100,000 times with requires grad as True\n",
    "# cpu takes 7 seconds, and gpu takes 29 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda:0\n",
      "-> Execution time: 00h 00m 00s\n"
     ]
    }
   ],
   "source": [
    "# choose device\n",
    "use_gpu = 0\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "\n",
    "# start execution\n",
    "start_time = time.time()\n",
    "\n",
    "# code\n",
    "model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "model.to(device)\n",
    "\n",
    "for _ in range(10000):\n",
    "    torch.manual_seed(0)\n",
    "    # model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "    # model.to(device)\n",
    "    for layer in model.children():\n",
    "        layer.reset_parameters()\n",
    "    \n",
    "\n",
    "# compute execution time\n",
    "exec_time = int(time.time() - start_time)\n",
    "exec_time = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(exec_time))\n",
    "print('-> Execution time: {}'.format(exec_time))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f38835821387ecea7238337192aa99e87ed1a9c9c1fa6562e207de7e0c31193"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('PyG': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
