{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Library imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pyforest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "\n",
    "from turtle import forward\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparams and loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edges = np.load('train_edges.npy')\n",
    "adj = np.load('adj.npy')\n",
    "train_edges.shape\n",
    "users = torch.LongTensor(train_edges[:, 0])\n",
    "items = torch.LongTensor(train_edges[:, 1])\n",
    "ratings = torch.FloatTensor(train_edges[:, 2])\n",
    "\n",
    "# some hyperparams\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "n_users = 943 \n",
    "n_items = 1682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining collaborative filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(CollaborativeFiltering, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        dot = (u * i).sum(1)\n",
    "        return torch.sigmoid(dot)\n",
    "\n",
    "# This architecture is giving me the best performance yet, \n",
    "# using BCE loss, n_factors = 64 and epochs = 200 with a final loss of 0.088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_hat, y):\n",
    "    y = y.clone().int()\n",
    "    y_hat = (y_hat.clone() > 0.5).int()\n",
    "    accuracy = (y == y_hat).sum() / len(y)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing inner collaborative filtering model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:16<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BCE loss after 200 epochs: 0.094\n",
      "Training accuracy after 200 epochs: 0.984\n"
     ]
    }
   ],
   "source": [
    "model = CollaborativeFiltering(n_users, n_items, n_factors = 64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "losses = []\n",
    "model.train()\n",
    "loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "epochs = 200\n",
    "n_samples = len(ratings)\n",
    "\n",
    "for _ in tqdm(range(epochs)):\n",
    "    y_hat = model(users, items)\n",
    "    loss = loss_fn(y_hat, ratings)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "sleep(1)\n",
    "\n",
    "print('Training BCE loss after {} epochs: {}'.format(epochs, round(losses[-1], 3)))\n",
    "print('Training accuracy after {} epochs: {}'.format(epochs, round(get_accuracy(y_hat, ratings), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Storing model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(943, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.weights = []\n",
    "# user_emb = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Collaborative filtering with manual gradient updates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation of what `retain_graph = True` does:\n",
    "- https://stackoverflow.com/questions/46774641/what-does-the-parameter-retain-graph-mean-in-the-variables-backward-method\n",
    "- https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/ (Paperspace blog on Autograd)\n",
    "- https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/ (Official blog on computational graphs)\n",
    "\n",
    "An explanation of what `create_graph = True` does:\n",
    "- If `True`, graph of the derivative will be constructed, allowing to compute higher order derivative products. \n",
    "- Source - https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch-autograd-grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CollaborativeFiltering(n_users, n_items, n_factors = 64)\n",
    "losses = []\n",
    "model.train()\n",
    "loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "epochs = 200\n",
    "n_samples = len(ratings)\n",
    "\n",
    "for _ in tqdm(range(epochs)):\n",
    "    y_hat = model(users, items)\n",
    "    loss = loss_fn(y_hat, ratings)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Start executing from here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 0.,  ..., 1., 1., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "users = torch.LongTensor(train_edges[:, 0])\n",
    "items = torch.LongTensor(train_edges[:, 1])\n",
    "ratings = torch.FloatTensor(train_edges[:, 2])\n",
    "n_samples = len(ratings)\n",
    "ratings.requires_grad_() # set requires_grad = True for ratings\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CollaborativeFiltering(n_users, n_items, n_factors = 64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr) \n",
    "model.train()\n",
    "loss_fn = nn.BCELoss(reduction = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0821, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "4.082079887390137\n"
     ]
    }
   ],
   "source": [
    "y_hat = model(users, items)\n",
    "loss = loss_fn(y_hat, ratings)\n",
    "print(loss)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "# torch.autograd.grad(loss, ratings) # this line doesn't work with above code\n",
    "# # next time try updating weights manually without using optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.6400,  1.2311, -0.2783,  ..., -0.0792,  0.6679,  0.5190],\n",
       "        [-1.2116, -1.2453,  0.3350,  ..., -0.8086, -1.0625, -0.7433],\n",
       "        [-0.0861,  0.5862,  0.0486,  ..., -1.5140,  0.0466,  0.2254],\n",
       "        ...,\n",
       "        [-1.5609,  0.9318, -1.1871,  ...,  1.8340,  0.5131,  0.7619],\n",
       "        [ 0.5751, -0.0915, -1.4706,  ..., -0.6700,  0.9471,  1.3083],\n",
       "        [ 1.2015, -0.1723, -1.0820,  ..., -1.0306,  0.7489, -0.3371]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1, p2 = model.parameters()\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.1710e-04,  5.9523e-05,  3.1794e-06,  ..., -5.7551e-05,\n",
       "          -3.5378e-05,  1.0161e-04],\n",
       "         [ 1.8902e-05, -6.0580e-05,  3.5197e-05,  ..., -6.2680e-05,\n",
       "           2.3283e-05, -1.0836e-05],\n",
       "         [-1.0293e-05,  3.6901e-06, -1.9097e-05,  ...,  3.0192e-05,\n",
       "           1.4289e-05,  4.8925e-05],\n",
       "         ...,\n",
       "         [-3.7497e-06,  2.2213e-05, -2.4738e-05,  ..., -3.2561e-06,\n",
       "          -9.1096e-07, -6.7995e-06],\n",
       "         [ 1.6786e-05,  3.4801e-05, -1.2662e-04,  ..., -2.4084e-05,\n",
       "           3.1077e-05,  9.3164e-05],\n",
       "         [ 1.3467e-04, -2.3466e-05, -7.7969e-05,  ..., -5.8620e-05,\n",
       "           7.4789e-06,  6.4515e-06]], grad_fn=<EmbeddingDenseBackwardBackward0>),)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1_grad = torch.autograd.grad(loss, p1, create_graph=True)\n",
    "p1_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1710e-04,  5.9523e-05,  3.1794e-06,  ..., -5.7551e-05,\n",
       "         -3.5378e-05,  1.0161e-04],\n",
       "        [ 1.8902e-05, -6.0580e-05,  3.5197e-05,  ..., -6.2680e-05,\n",
       "          2.3283e-05, -1.0836e-05],\n",
       "        [-1.0293e-05,  3.6901e-06, -1.9097e-05,  ...,  3.0192e-05,\n",
       "          1.4289e-05,  4.8925e-05],\n",
       "        ...,\n",
       "        [-3.7497e-06,  2.2213e-05, -2.4738e-05,  ..., -3.2561e-06,\n",
       "         -9.1096e-07, -6.7995e-06],\n",
       "        [ 1.6786e-05,  3.4801e-05, -1.2662e-04,  ..., -2.4084e-05,\n",
       "          3.1077e-05,  9.3164e-05],\n",
       "        [ 1.3467e-04, -2.3466e-05, -7.7969e-05,  ..., -5.8620e-05,\n",
       "          7.4789e-06,  6.4515e-06]], grad_fn=<EmbeddingDenseBackwardBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1_grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6400,  1.2311, -0.2783,  ..., -0.0792,  0.6679,  0.5190],\n",
       "        [-1.2116, -1.2453,  0.3350,  ..., -0.8086, -1.0625, -0.7433],\n",
       "        [-0.0861,  0.5862,  0.0486,  ..., -1.5140,  0.0466,  0.2254],\n",
       "        ...,\n",
       "        [-1.5609,  0.9318, -1.1871,  ...,  1.8340,  0.5131,  0.7619],\n",
       "        [ 0.5751, -0.0915, -1.4706,  ..., -0.6700,  0.9471,  1.3083],\n",
       "        [ 1.2015, -0.1723, -1.0820,  ..., -1.0306,  0.7489, -0.3371]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = p1 - lr * p1_grad[0]\n",
    "p1\n",
    "# This way it's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0089e-05,        -inf, -9.0750e-06,  ...,  6.2042e-05,\n",
       "          1.3535e-06,  7.5071e-05]),)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradients w.r.t ratings works when create_graph = True is set for previous gradient computation\n",
    "torch.autograd.grad(loss, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0670, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner_grad = torch.autograd.grad(loss, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next try to put all these things together and get a workable code\n",
    "# it need not even be right, just should be workable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Putting it together: code for meta attack (one iteration)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Seems to work!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings:  tensor([1., 1., 0.,  ..., 1., 1., 0.], requires_grad=True)\n",
      "loss:  tensor(4.0275, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "first pass: loss.item():  4.027473449707031\n",
      "p1 before updation:  Parameter containing:\n",
      "tensor([[-0.2015,  0.0094,  0.9397,  ..., -1.3159,  0.2729, -1.6187],\n",
      "        [-1.2597,  1.5202, -0.2255,  ..., -1.3526,  0.3874, -2.7321],\n",
      "        [ 0.6932, -1.4194,  0.1402,  ...,  0.2244, -0.5958,  2.5095],\n",
      "        ...,\n",
      "        [ 0.4889, -2.0555, -0.5391,  ..., -1.6226, -0.1316,  0.0669],\n",
      "        [-0.4230,  0.9798, -0.4390,  ...,  0.7863,  1.2866, -0.1795],\n",
      "        [-0.7211, -0.4675,  1.3677,  ...,  0.9185,  0.1461, -0.7254]],\n",
      "       requires_grad=True)\n",
      "p1 after updation:  Parameter containing:\n",
      "tensor([[-0.2010,  0.0094,  0.9382,  ..., -1.3143,  0.2730, -1.6166],\n",
      "        [-1.2591,  1.5200, -0.2253,  ..., -1.3520,  0.3870, -2.7318],\n",
      "        [ 0.6939, -1.4189,  0.1402,  ...,  0.2243, -0.5958,  2.5092],\n",
      "        ...,\n",
      "        [ 0.4890, -2.0549, -0.5390,  ..., -1.6224, -0.1314,  0.0670],\n",
      "        [-0.4227,  0.9796, -0.4383,  ...,  0.7859,  1.2870, -0.1797],\n",
      "        [-0.7200, -0.4665,  1.3659,  ...,  0.9174,  0.1464, -0.7236]],\n",
      "       requires_grad=True)\n",
      "inner gradients updated for p1 and p2\n",
      "loss:  tensor(4.0154, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "second pass: loss.item():  4.015388011932373\n",
      "p1 before updation:  Parameter containing:\n",
      "tensor([[-0.2010,  0.0094,  0.9382,  ..., -1.3143,  0.2730, -1.6166],\n",
      "        [-1.2591,  1.5200, -0.2253,  ..., -1.3520,  0.3870, -2.7318],\n",
      "        [ 0.6939, -1.4189,  0.1402,  ...,  0.2243, -0.5958,  2.5092],\n",
      "        ...,\n",
      "        [ 0.4890, -2.0549, -0.5390,  ..., -1.6224, -0.1314,  0.0670],\n",
      "        [-0.4227,  0.9796, -0.4383,  ...,  0.7859,  1.2870, -0.1797],\n",
      "        [-0.7200, -0.4665,  1.3659,  ...,  0.9174,  0.1464, -0.7236]],\n",
      "       requires_grad=True)\n",
      "p1 after updation:  Parameter containing:\n",
      "tensor([[-0.2004,  0.0095,  0.9368,  ..., -1.3128,  0.2730, -1.6145],\n",
      "        [-1.2586,  1.5198, -0.2251,  ..., -1.3514,  0.3867, -2.7316],\n",
      "        [ 0.6946, -1.4185,  0.1403,  ...,  0.2242, -0.5958,  2.5090],\n",
      "        ...,\n",
      "        [ 0.4890, -2.0544, -0.5389,  ..., -1.6221, -0.1311,  0.0672],\n",
      "        [-0.4224,  0.9794, -0.4377,  ...,  0.7856,  1.2874, -0.1799],\n",
      "        [-0.7189, -0.4654,  1.3642,  ...,  0.9163,  0.1468, -0.7218]],\n",
      "       requires_grad=True)\n",
      "inner gradients updated for p1 and p2\n",
      "loss:  tensor(4.0071, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "third pass: loss.item():  4.007085800170898\n",
      "meta gradients:  (tensor([-1.4728e-05,  6.5787e-05, -3.5755e-06,  ..., -3.5894e-05,\n",
      "        -3.9617e-06,  8.5917e-06]),)\n"
     ]
    }
   ],
   "source": [
    "# basic data preparation\n",
    "users = torch.LongTensor(train_edges[:, 0])\n",
    "items = torch.LongTensor(train_edges[:, 1])\n",
    "ratings = torch.FloatTensor(train_edges[:, 2])\n",
    "n_samples = len(ratings)\n",
    "ratings.requires_grad_() # set requires_grad = True for ratings\n",
    "print('ratings: ', ratings)\n",
    "lr = 10\n",
    "\n",
    "# define model and loss function\n",
    "model = CollaborativeFiltering(n_users, n_items, n_factors = 64)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = lr) \n",
    "model.train()\n",
    "loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "# one iteration of forward pass \n",
    "y_hat = model(users, items)\n",
    "loss = loss_fn(y_hat, ratings)\n",
    "print('loss: ', loss)\n",
    "print('first pass: loss.item(): ', loss.item())\n",
    "\n",
    "# one iteration of inner gradient updation for two parameters\n",
    "p1, p2 = model.parameters()\n",
    "p1_grad = torch.autograd.grad(loss, p1, create_graph=True)\n",
    "p2_grad = torch.autograd.grad(loss, p2, create_graph=True)\n",
    "print('p1 before updation: ', p1)\n",
    "p1_new = p1 - lr * p1_grad[0]\n",
    "p2_new = p2 - lr * p2_grad[0]\n",
    "with torch.no_grad():\n",
    "    p1.copy_(p1_new)\n",
    "    p2.copy_(p2_new)\n",
    "print('p1 after updation: ', p1)\n",
    "print('inner gradients updated for p1 and p2')\n",
    "\n",
    "# second iteration of forward pass \n",
    "y_hat = model(users, items)\n",
    "loss = loss_fn(y_hat, ratings)\n",
    "print('loss: ', loss)\n",
    "print('second pass: loss.item(): ', loss.item())\n",
    "\n",
    "# second iteration of inner gradient updation for two parameters\n",
    "p1, p2 = model.parameters() # try to comment this out and see the change\n",
    "p1_grad = torch.autograd.grad(loss, p1, create_graph=True)\n",
    "p2_grad = torch.autograd.grad(loss, p2, create_graph=True)\n",
    "print('p1 before updation: ', p1)\n",
    "p1_new = p1 - lr * p1_grad[0]\n",
    "p2_new = p2 - lr * p2_grad[0]\n",
    "with torch.no_grad():\n",
    "    p1.copy_(p1_new)\n",
    "    p2.copy_(p2_new)\n",
    "print('p1 after updation: ', p1)\n",
    "print('inner gradients updated for p1 and p2')\n",
    "\n",
    "y_hat = model(users, items)\n",
    "loss = loss_fn(y_hat, ratings)\n",
    "print('loss: ', loss)\n",
    "print('third pass: loss.item(): ', loss.item())\n",
    "\n",
    "# it seems model weights are not updating when updating p1 and p2 manually\n",
    "# need to investigate\n",
    "\n",
    "# compute and print meta gradients w.r.t ratings\n",
    "meta_grad = torch.autograd.grad(loss, ratings)\n",
    "print('meta gradients: ', meta_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Putting it together: code for meta attack (inner loop)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings:  tensor([1., 1., 0.,  ..., 1., 1., 0.], requires_grad=True)\n",
      "inner loss at iter 0: 4.081691265106201\n",
      "inner loss at iter 1: 4.070385932922363\n",
      "inner loss at iter 2: 4.059072017669678\n",
      "inner loss at iter 3: 4.049466133117676\n",
      "inner loss at iter 4: 4.039066791534424\n",
      "inner loss at iter 5: 4.027796268463135\n",
      "inner loss at iter 6: 4.0173845291137695\n",
      "inner loss at iter 7: 4.006999969482422\n",
      "inner loss at iter 8: 3.9974775314331055\n",
      "inner loss at iter 9: 3.986273765563965\n",
      "meta gradients:  (tensor([-3.2998e-05, -4.8649e-05,        -inf,  ...,  5.9168e-06,\n",
      "         2.3956e-05,  2.2765e-05]),)\n"
     ]
    }
   ],
   "source": [
    "# basic data preparation\n",
    "users = torch.LongTensor(train_edges[:, 0])\n",
    "items = torch.LongTensor(train_edges[:, 1])\n",
    "ratings = torch.FloatTensor(train_edges[:, 2])\n",
    "n_samples = len(ratings)\n",
    "ratings.requires_grad_() # set requires_grad = True for ratings\n",
    "print('ratings: ', ratings)\n",
    "\n",
    "# setting hyperparams\n",
    "lr = 10\n",
    "T = 10\n",
    "\n",
    "# define model and loss function\n",
    "model = CollaborativeFiltering(n_users, n_items, n_factors = 64)\n",
    "p1, p2 = model.parameters()\n",
    "model.train()\n",
    "loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "# for i in tqdm(range(T)):\n",
    "for i in range(T):\n",
    "    y_hat = model(users, items)\n",
    "    loss = loss_fn(y_hat, ratings)\n",
    "    print('inner loss at iter {}: {}'.format(i, loss.item()))\n",
    "    \n",
    "    p1_grad = torch.autograd.grad(loss, p1, create_graph=True)\n",
    "    p2_grad = torch.autograd.grad(loss, p2, create_graph=True)\n",
    "\n",
    "    p1_new = p1 - lr * p1_grad[0]\n",
    "    p2_new = p2 - lr * p2_grad[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        p1.copy_(p1_new)\n",
    "        p2.copy_(p2_new)\n",
    "\n",
    "meta_grad = torch.autograd.grad(loss, ratings)\n",
    "print('meta gradients: ', meta_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inner loop experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings:  tensor([1., 1., 0.,  ..., 1., 1., 0.], requires_grad=True)\n",
      "inner loss at iter 0: 4.066965103149414\n",
      "inner loss at iter 1: 4.05279541015625\n",
      "inner loss at iter 2: 4.040768146514893\n",
      "inner loss at iter 3: 4.0320844650268555\n",
      "inner loss at iter 4: 4.020899295806885\n",
      "inner loss at iter 5: 4.009313583374023\n",
      "inner loss at iter 6: 3.9977314472198486\n",
      "inner loss at iter 7: 3.9878790378570557\n",
      "inner loss at iter 8: 3.976311206817627\n",
      "inner loss at iter 9: 3.9639394283294678\n",
      "inner loss at iter 10: 3.9541139602661133\n",
      "inner loss at iter 11: 3.9426400661468506\n",
      "inner loss at iter 12: 3.931095600128174\n",
      "inner loss at iter 13: 3.917526960372925\n",
      "inner loss at iter 14: 3.909027099609375\n",
      "inner loss at iter 15: 3.899671792984009\n",
      "inner loss at iter 16: 3.8882553577423096\n",
      "inner loss at iter 17: 3.8784937858581543\n",
      "inner loss at iter 18: 3.865010976791382\n",
      "inner loss at iter 19: 3.85528302192688\n",
      "meta gradients:  tensor([ 5.1595e-06,  2.0629e-05, -9.9811e-06,  ..., -1.5046e-05,\n",
      "        -2.3528e-05,  1.8430e-05])\n"
     ]
    }
   ],
   "source": [
    "# set seed to make results reproducible\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# basic data preparation\n",
    "users = torch.LongTensor(train_edges[:, 0])\n",
    "items = torch.LongTensor(train_edges[:, 1])\n",
    "ratings = torch.FloatTensor(train_edges[:, 2])\n",
    "n_samples = len(ratings)\n",
    "ratings.requires_grad_() # set requires_grad = True for ratings\n",
    "print('ratings: ', ratings)\n",
    "\n",
    "# setting hyperparams\n",
    "lr = 10\n",
    "T = 20\n",
    "\n",
    "# define model and loss function\n",
    "model = CollaborativeFiltering(n_users, n_items, n_factors = 64)\n",
    "p1, p2 = model.parameters()\n",
    "model.train()\n",
    "loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "# for i in tqdm(range(T)):\n",
    "for i in range(T):\n",
    "    y_hat = model(users, items)\n",
    "    loss = loss_fn(y_hat, ratings)\n",
    "    print('inner loss at iter {}: {}'.format(i, loss.item()))\n",
    "    \n",
    "    p1_grad = torch.autograd.grad(loss, p1, create_graph=True)\n",
    "    p2_grad = torch.autograd.grad(loss, p2, create_graph=True)\n",
    "\n",
    "    p1_new = p1 - lr * p1_grad[0]\n",
    "    p2_new = p2 - lr * p2_grad[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        p1.copy_(p1_new)\n",
    "        p2.copy_(p2_new)\n",
    "\n",
    "meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "print('meta gradients: ', meta_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0.,  ..., 1., 1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0002)\n",
      "152311\n",
      "tensor(13)\n",
      "tensor(1628)\n",
      "tensor(0., grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "max_meta_grad = -math.inf \n",
    "edge_to_add = -1\n",
    "for i in range(n_samples):\n",
    "    if ratings[i] == 0:\n",
    "        if meta_grad[i] > max_meta_grad:\n",
    "            max_meta_grad = meta_grad[i]\n",
    "            edge_to_add = i \n",
    "print(max_meta_grad)\n",
    "print(edge_to_add)\n",
    "print(users[i])\n",
    "print(items[i])\n",
    "print(ratings[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment with modified ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "inner loss at iter 0: 4.066965103149414\n",
      "inner loss at iter 1: 4.05279541015625\n",
      "inner loss at iter 2: 4.040768146514893\n",
      "inner loss at iter 3: 4.0320844650268555\n",
      "inner loss at iter 4: 4.020899295806885\n",
      "inner loss at iter 5: 4.009313583374023\n",
      "inner loss at iter 6: 3.9977314472198486\n",
      "inner loss at iter 7: 3.9878790378570557\n",
      "inner loss at iter 8: 3.976311206817627\n",
      "inner loss at iter 9: 3.9639394283294678\n",
      "inner loss at iter 10: 3.9541139602661133\n",
      "inner loss at iter 11: 3.9426400661468506\n",
      "inner loss at iter 12: 3.931095600128174\n",
      "inner loss at iter 13: 3.917526960372925\n",
      "inner loss at iter 14: 3.909027099609375\n",
      "inner loss at iter 15: 3.899671792984009\n",
      "inner loss at iter 16: 3.8882553577423096\n",
      "inner loss at iter 17: 3.8784937858581543\n",
      "inner loss at iter 18: 3.865010976791382\n",
      "inner loss at iter 19: 3.85528302192688\n",
      "inner loss at iter 20: 3.843494176864624\n",
      "inner loss at iter 21: 3.8321027755737305\n",
      "inner loss at iter 22: 3.822427272796631\n",
      "inner loss at iter 23: 3.8114821910858154\n",
      "inner loss at iter 24: 3.801396608352661\n",
      "inner loss at iter 25: 3.789243221282959\n",
      "inner loss at iter 26: 3.779627561569214\n",
      "inner loss at iter 27: 3.770043134689331\n",
      "inner loss at iter 28: 3.758329153060913\n",
      "inner loss at iter 29: 3.747511863708496\n",
      "inner loss at iter 30: 3.737102746963501\n",
      "inner loss at iter 31: 3.7254252433776855\n",
      "inner loss at iter 32: 3.7150344848632812\n",
      "inner loss at iter 33: 3.703806161880493\n",
      "inner loss at iter 34: 3.694298505783081\n",
      "inner loss at iter 35: 3.6839516162872314\n",
      "inner loss at iter 36: 3.671489715576172\n",
      "inner loss at iter 37: 3.6603243350982666\n",
      "inner loss at iter 38: 3.6504385471343994\n",
      "inner loss at iter 39: 3.6418116092681885\n",
      "inner loss at iter 40: 3.629868745803833\n",
      "inner loss at iter 41: 3.621260643005371\n",
      "inner loss at iter 42: 3.611849069595337\n",
      "inner loss at iter 43: 3.6003284454345703\n",
      "inner loss at iter 44: 3.5913631916046143\n",
      "inner loss at iter 45: 3.580716848373413\n",
      "inner loss at iter 46: 3.572598457336426\n",
      "inner loss at iter 47: 3.563248634338379\n",
      "inner loss at iter 48: 3.5555968284606934\n",
      "inner loss at iter 49: 3.544178009033203\n",
      "meta gradients:  tensor([ 3.7515e-06,  1.9252e-05, -8.1105e-06,  ..., -1.4346e-05,\n",
      "        -2.3698e-05,  1.8164e-05])\n"
     ]
    }
   ],
   "source": [
    "ratings_mod = train_edges[:, 2]\n",
    "print(ratings_mod[152311])\n",
    "ratings_mod[152311] = 1\n",
    "print(ratings_mod[152311])\n",
    "ratings_mod = torch.FloatTensor(ratings_mod)\n",
    "ratings_mod.requires_grad_()\n",
    "\n",
    "# set seed to make results reproducible\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# define model and loss function\n",
    "model = CollaborativeFiltering(n_users, n_items, n_factors = 64)\n",
    "p1, p2 = model.parameters()\n",
    "model.train()\n",
    "loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "# for i in tqdm(range(T)):\n",
    "T = 50\n",
    "for i in range(T):\n",
    "    y_hat = model(users, items)\n",
    "    loss = loss_fn(y_hat, ratings_mod)\n",
    "    print('inner loss at iter {}: {}'.format(i, loss.item()))\n",
    "    \n",
    "    p1_grad = torch.autograd.grad(loss, p1, create_graph=True)\n",
    "    p2_grad = torch.autograd.grad(loss, p2, create_graph=True)\n",
    "\n",
    "    p1_new = p1 - lr * p1_grad[0]\n",
    "    p2_new = p2 - lr * p2_grad[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        p1.copy_(p1_new)\n",
    "        p2.copy_(p2_new)\n",
    "\n",
    "meta_grad = torch.autograd.grad(loss, ratings_mod)[0]\n",
    "print('meta gradients: ', meta_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.6357e-06,  2.1075e-05, -1.0639e-05,  ..., -1.5299e-05,\n",
       "        -2.3461e-05,  1.8523e-05])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_grad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.7120, -0.0302, -1.2044,  ...,  0.9276,  0.6895, -0.4508],\n",
      "        [-1.1461, -0.6966, -0.2378,  ..., -0.2327,  0.6188,  0.9167],\n",
      "        [ 0.3843, -2.0148, -1.0853,  ...,  1.7147, -0.3024,  0.6696],\n",
      "        ...,\n",
      "        [ 1.2213,  0.2022, -0.0885,  ...,  1.5958,  1.3737, -1.0703],\n",
      "        [-0.9831,  0.7369, -1.5582,  ...,  1.3333,  0.0863,  0.3997],\n",
      "        [ 1.4446,  0.7950, -0.0190,  ...,  0.0838,  0.8825, -1.3919]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.4700e+00,  6.6813e-02,  3.5546e-01,  ..., -8.3447e-01,\n",
      "         -6.8905e-01, -5.9982e-01],\n",
      "        [-7.7575e-01, -3.0664e-02,  1.6253e+00,  ...,  7.7636e-01,\n",
      "         -1.0212e+00, -1.8676e+00],\n",
      "        [ 5.7605e-01,  7.0709e-01, -8.5691e-01,  ...,  8.8496e-01,\n",
      "         -2.7092e-02, -1.1606e+00],\n",
      "        ...,\n",
      "        [-7.8321e-02,  5.4087e-01, -2.2929e-02,  ...,  2.2237e-01,\n",
      "          7.0521e-01,  7.6299e-01],\n",
      "        [-1.4083e+00, -9.4570e-01, -1.2581e+00,  ...,  3.3569e-01,\n",
      "         -3.7511e-01, -1.7738e-03],\n",
      "        [ 1.2042e-01,  1.3620e+00,  4.3135e-01,  ...,  3.9287e-01,\n",
      "          4.9913e-01,  1.4052e-01]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "p1, p2 = model.parameters()\n",
    "print(p1)\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_emb.weight\n",
      "tensor([[-0.6898,  1.2371,  0.4595,  ..., -0.5926, -1.2892, -0.5708],\n",
      "        [-0.3267, -0.3558,  0.7712,  ..., -1.2938, -2.1441, -1.3277],\n",
      "        [ 0.6864, -0.2246, -0.6704,  ..., -0.4446,  0.0593,  0.5291],\n",
      "        ...,\n",
      "        [-0.7997, -0.0640, -0.7172,  ..., -0.4175, -0.5013, -0.1714],\n",
      "        [ 0.8728,  1.6324, -0.5116,  ..., -0.2887,  1.2637, -0.2122],\n",
      "        [ 1.2795, -1.6052,  0.0071,  ...,  0.5694,  1.2050, -0.8763]])\n",
      "item_emb.weight\n",
      "tensor([[-0.0990,  0.4868, -1.5878,  ..., -0.7078,  0.6567,  1.1695],\n",
      "        [-1.0863, -1.0977,  0.9486,  ...,  0.5633, -0.2189, -0.2361],\n",
      "        [-1.0151,  1.5876, -0.3008,  ..., -2.5757,  0.2672,  1.7589],\n",
      "        ...,\n",
      "        [ 0.2436, -0.7643,  0.5386,  ...,  0.5982, -0.2018,  2.3530],\n",
      "        [ 1.2479, -0.1559,  1.0687,  ...,  1.8135,  0.7614,  1.3881],\n",
      "        [-0.3744, -0.6366,  0.1969,  ..., -0.1726,  1.0843,  0.7719]])\n"
     ]
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "\n",
    "for name, param in state_dict.items():\n",
    "    print(name)\n",
    "    print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.6898,  1.2371,  0.4595,  ..., -0.5926, -1.2892, -0.5708],\n",
      "        [-0.3267, -0.3558,  0.7712,  ..., -1.2938, -2.1441, -1.3277],\n",
      "        [ 0.6864, -0.2246, -0.6704,  ..., -0.4446,  0.0593,  0.5291],\n",
      "        ...,\n",
      "        [-0.7997, -0.0640, -0.7172,  ..., -0.4175, -0.5013, -0.1714],\n",
      "        [ 0.8728,  1.6324, -0.5116,  ..., -0.2887,  1.2637, -0.2122],\n",
      "        [ 1.2795, -1.6052,  0.0071,  ...,  0.5694,  1.2050, -0.8763]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0990,  0.4868, -1.5878,  ..., -0.7078,  0.6567,  1.1695],\n",
      "        [-1.0863, -1.0977,  0.9486,  ...,  0.5633, -0.2189, -0.2361],\n",
      "        [-1.0151,  1.5876, -0.3008,  ..., -2.5757,  0.2672,  1.7589],\n",
      "        ...,\n",
      "        [ 0.2436, -0.7643,  0.5386,  ...,  0.5982, -0.2018,  2.3530],\n",
      "        [ 1.2479, -0.1559,  1.0687,  ...,  1.8135,  0.7614,  1.3881],\n",
      "        [-0.3744, -0.6366,  0.1969,  ..., -0.1726,  1.0843,  0.7719]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Explore nn.Embedding()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(943, 1682)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Embedding(n_users, n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward(self, user, item):\n",
    "#         u = self.user_emb(user)\n",
    "#         i = self.item_emb(item)\n",
    "#         dot = (u * i).sum(1)\n",
    "#         return torch.sigmoid(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 32\n",
    "user_emb = nn.Embedding(n_users, n_factors)\n",
    "item_emb = nn.Embedding(n_items, n_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4439, -1.9483, -0.9516,  ..., -1.4186, -1.3594,  0.2810],\n",
       "        [-1.1488, -0.0379, -0.6078,  ...,  0.9696,  0.4705,  0.2289],\n",
       "        [-0.4593,  1.6118, -1.6503,  ..., -1.1451,  0.0371, -1.4867],\n",
       "        ...,\n",
       "        [ 0.4872,  0.7919, -0.3043,  ...,  0.2604, -0.1841,  1.1218],\n",
       "        [-0.4748,  0.5623, -0.5283,  ..., -1.3008, -0.1314, -0.4233],\n",
       "        [-0.8145, -0.2831, -1.4781,  ..., -0.9306,  1.4456,  0.6241]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4081,  0.6106, -0.6611,  ..., -1.2111, -0.4488, -1.0079],\n",
       "        [ 0.5214,  1.6385,  2.1922,  ...,  0.8036,  0.1464, -1.1281],\n",
       "        [-0.2598, -1.0658, -1.0823,  ...,  2.8500,  0.2176, -1.6161],\n",
       "        ...,\n",
       "        [ 1.8614,  1.4431,  0.1386,  ..., -2.3231, -0.1649,  0.1128],\n",
       "        [-0.1436, -1.0374,  0.8331,  ..., -0.5254,  1.5097, -0.2469],\n",
       "        [-2.5575,  0.8269, -0.8540,  ..., -0.7730, -0.4882, -0.7975]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = user_emb(users)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings:  tensor([1., 1., 0.,  ..., 1., 1., 0.], requires_grad=True)\n",
      "loss:  tensor(2.4477, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "loss.item():  2.447725772857666\n",
      "Parameter containing:\n",
      "tensor([[ 0.5933, -1.1616, -1.1742,  ...,  0.5817, -0.7269,  0.8990],\n",
      "        [-0.1532,  1.6284,  0.4429,  ...,  0.6275,  1.0835,  0.0712],\n",
      "        [ 0.9543,  1.2078,  1.2772,  ..., -0.6062, -0.6404, -2.1918],\n",
      "        ...,\n",
      "        [ 0.2970, -1.1326, -0.1476,  ...,  2.5913,  1.8091, -0.8726],\n",
      "        [ 1.2338, -0.2983, -0.7845,  ...,  0.1493, -1.1972,  0.6469],\n",
      "        [ 1.3923, -0.2281,  0.8573,  ...,  0.4372, -0.9985, -0.3536]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.6581, -0.9044, -1.0907,  ...,  0.4492, -0.5738,  0.8677],\n",
      "        [-0.0961,  1.4929,  0.4090,  ...,  0.5991,  1.0047,  0.0726],\n",
      "        [ 0.8593,  1.1519,  1.2632,  ..., -0.5731, -0.6128, -2.0757],\n",
      "        ...,\n",
      "        [ 0.3062, -1.1411, -0.1266,  ...,  2.5627,  1.7878, -0.8593],\n",
      "        [ 1.2029, -0.3668, -0.7381,  ...,  0.0753, -1.1248,  0.5513],\n",
      "        [ 1.2333, -0.2736,  0.7004,  ...,  0.4303, -0.8845, -0.2743]],\n",
      "       requires_grad=True)\n",
      "(tensor([[-6.4721e-05, -2.5724e-04, -8.3501e-05,  ...,  1.3241e-04,\n",
      "         -1.5306e-04,  3.1248e-05],\n",
      "        [-5.7115e-05,  1.3549e-04,  3.3823e-05,  ...,  2.8380e-05,\n",
      "          7.8813e-05, -1.4333e-06],\n",
      "        [ 9.5074e-05,  5.5971e-05,  1.4009e-05,  ..., -3.3125e-05,\n",
      "         -2.7550e-05, -1.1606e-04],\n",
      "        ...,\n",
      "        [-9.1941e-06,  8.4719e-06, -2.0991e-05,  ...,  2.8644e-05,\n",
      "          2.1296e-05, -1.3263e-05],\n",
      "        [ 3.0890e-05,  6.8483e-05, -4.6389e-05,  ...,  7.3990e-05,\n",
      "         -7.2464e-05,  9.5601e-05],\n",
      "        [ 1.5906e-04,  4.5482e-05,  1.5683e-04,  ...,  6.9592e-06,\n",
      "         -1.1406e-04, -7.9272e-05]], grad_fn=<EmbeddingDenseBackwardBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "ratings = torch.FloatTensor(train_edges[:, 2])\n",
    "ratings.requires_grad_() # set requires_grad = True for ratings\n",
    "print('ratings: ', ratings)\n",
    "loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "n_factors = 32\n",
    "user_emb = nn.Embedding(n_users, n_factors)\n",
    "item_emb = nn.Embedding(n_items, n_factors)\n",
    "\n",
    "# first forward pass \n",
    "u = user_emb(users)\n",
    "i = item_emb(items)\n",
    "dot = (u * i).sum(1)\n",
    "# dot = torch.tensor(1)\n",
    "y_hat = torch.sigmoid(dot)\n",
    "# y_hat = 0\n",
    "\n",
    "# compute loss\n",
    "loss = loss_fn(y_hat, ratings)\n",
    "print('loss: ', loss)\n",
    "print('loss.item(): ', loss.item())\n",
    "\n",
    "print(user_emb.weight)\n",
    "user_grad = torch.autograd.grad(loss, user_emb.weight, create_graph=True)\n",
    "item_grad = torch.autograd.grad(loss, item_emb.weight, create_graph=True)\n",
    "user_emb.weight = nn.Parameter(user_emb.weight - 1000 * user_grad[0])\n",
    "# item_emb.weight = item_emb.weight - lr * item_grad[0]\n",
    "print(user_emb.weight)\n",
    "print(user_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4439, -1.9483, -0.9516,  ..., -1.4186, -1.3594,  0.2810],\n",
       "        [-1.1488, -0.0379, -0.6078,  ...,  0.9696,  0.4705,  0.2289],\n",
       "        [-0.4593,  1.6118, -1.6503,  ..., -1.1451,  0.0371, -1.4867],\n",
       "        ...,\n",
       "        [ 0.4872,  0.7919, -0.3043,  ...,  0.2604, -0.1841,  1.1218],\n",
       "        [-0.4748,  0.5623, -0.5283,  ..., -1.3008, -0.1314, -0.4233],\n",
       "        [-0.8145, -0.2831, -1.4781,  ..., -0.9306,  1.4456,  0.6241]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.5088, -1.1876, -0.2879,  ..., -0.7009,  2.4997,  0.3683],\n",
      "        [-0.2103, -0.2408, -2.2474,  ...,  0.4838, -0.4001, -0.2369],\n",
      "        [-1.6945,  0.5134, -0.2179,  ..., -0.8171, -0.9805,  1.2394],\n",
      "        ...,\n",
      "        [-0.5910,  1.0138,  0.1980,  ...,  0.5972,  0.1417, -0.3724],\n",
      "        [ 0.5018,  0.2675,  0.1218,  ..., -0.7895, -1.6456,  0.3717],\n",
      "        [ 0.7160,  0.0197,  0.8263,  ...,  0.0859, -0.5210,  0.2499]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 1.5088, -1.1876, -0.2879,  ..., -0.7009,  2.4997,  0.3683],\n",
      "        [-0.2103, -0.2408, -2.2474,  ...,  0.4838, -0.4001, -0.2369],\n",
      "        [-1.6945,  0.5134, -0.2179,  ..., -0.8171, -0.9805,  1.2394],\n",
      "        ...,\n",
      "        [-0.5910,  1.0138,  0.1980,  ...,  0.5972,  0.1417, -0.3724],\n",
      "        [ 0.5018,  0.2675,  0.1218,  ..., -0.7895, -1.6456,  0.3717],\n",
      "        [ 0.7160,  0.0197,  0.8263,  ...,  0.0859, -0.5210,  0.2499]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "p1, p2 = model.parameters()\n",
    "print(p1)\n",
    "p1 = p1 - lr * p1_grad[0]\n",
    "print(p1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f38835821387ecea7238337192aa99e87ed1a9c9c1fa6562e207de7e0c31193"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('PyG': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
