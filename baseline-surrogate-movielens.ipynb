{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Library imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pyforest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from turtle import forward\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparams and loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682, 200000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_edges = np.load('movielens/train_edges.npy')\n",
    "user_list = train_edges[:, 0]\n",
    "item_list = train_edges[:, 1]\n",
    "rating_list = train_edges[:, 2].astype('float32')\n",
    "\n",
    "n_users = 943 \n",
    "n_items = 1682\n",
    "n_samples = len(rating_list)\n",
    "\n",
    "n_users, n_items, n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining collaborative filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(CollaborativeFiltering, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        dot = (u * i).sum(1)\n",
    "        return torch.sigmoid(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_hat, y):\n",
    "    y = y.clone().int()\n",
    "    y_hat = (y_hat.clone() > 0.5).int()\n",
    "    accuracy = (y == y_hat).sum() / len(y)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Baseline for surrogate meta-attack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Learning rate:  51\n",
      "-> T:  400\n",
      "-> Delta: 100 (0.05%)\n",
      "-> Embedding size:  64\n",
      "-> Device:  cuda:3\n",
      "-> Retain graph:  True\n",
      "-> Create graph:  False\n",
      "-> Save results:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-> Perturbations: 100%|██████████| 100/100 [02:11<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Execution time: 00h 02m 12s\n"
     ]
    }
   ],
   "source": [
    "# start execution\n",
    "start_time = time.time()\n",
    "\n",
    "# GPU settings (set use_gpu = -1 if you want to use CPU)\n",
    "use_gpu = 3\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# some hyperparams\n",
    "lr = 51\n",
    "T = 400\n",
    "Delta = 100 # 5% ~ 10K perturbations\n",
    "n_factors = 64\n",
    "save_results = False\n",
    "retain_graph = True \n",
    "create_graph = False\n",
    "dataset = 'movielens'\n",
    "\n",
    "# list of perturbations\n",
    "perturbations = dict()\n",
    "perturbations['edges'] = []\n",
    "perturbations['metagrad'] = []\n",
    "perturbations['accuracy_before'] = []\n",
    "perturbations['accuracy_after'] = []\n",
    "perturbations['loss_before'] = []\n",
    "perturbations['loss_after'] = []\n",
    "\n",
    "perturbations['accuracy_before_eval'] = []\n",
    "perturbations['accuracy_after_eval'] = []\n",
    "perturbations['loss_before_eval'] = []\n",
    "perturbations['loss_after_eval'] = []\n",
    "\n",
    "# print hyperparam config\n",
    "print('-> Learning rate: ', lr)\n",
    "print('-> T: ', T)\n",
    "print('-> Delta: {} ({}%)'.format(Delta, round(Delta * 100 / n_samples, 2)))\n",
    "print('-> Embedding size: ', n_factors)\n",
    "print('-> Device: ', device)\n",
    "# print('-> Manual gradients: ', manual_gradients)\n",
    "print('-> Retain graph: ', retain_graph)\n",
    "print('-> Create graph: ', create_graph)\n",
    "print('-> Save results: ', save_results)\n",
    "\n",
    "# load users, items and ratings as tensors\n",
    "users = torch.tensor(user_list, device = device)\n",
    "items = torch.tensor(item_list, device = device)\n",
    "ratings = torch.tensor(rating_list, device = device, requires_grad = True)\n",
    "perturbs = torch.ones_like(ratings).bool()\n",
    "\n",
    "# neg_edges is set of indices of negative edges \n",
    "edges = ratings.detach().to('cpu').numpy()\n",
    "neg_edges = np.where(edges == 0)[0]\n",
    "np.random.seed(0)\n",
    "edges_to_perturb = np.random.choice(neg_edges, size=Delta, replace = True) # sample Delta edges randomly and perturb one by one inside loop \n",
    "\n",
    "# define model and it's parameters\n",
    "model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "# for each perturbation do the following\n",
    "for delta in tqdm(range(Delta), desc='-> Perturbations'):\n",
    "\n",
    "    # makes loss reproducible for each iteration in Delta\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # reset model paramters \n",
    "    for layer in model.children():\n",
    "        layer.reset_parameters()\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    # inner loop training process\n",
    "    model.train()\n",
    "    for i in range(T):\n",
    "        y_hat = model(users, items)\n",
    "        loss = loss_fn(y_hat, ratings)\n",
    "\n",
    "        # use torch.optim optimizer to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=retain_graph, create_graph=create_graph)\n",
    "        optimizer.step()\n",
    "\n",
    "    # compute and store accuracy of model after T training steps\n",
    "    with torch.no_grad():\n",
    "        # compute training accuracy and loss including perturbed edges\n",
    "        y_hat = model(users, items)\n",
    "        perturbations['accuracy_before'].append(get_accuracy(y_hat, ratings))\n",
    "        perturbations['loss_before'].append(loss_fn(y_hat, ratings).item())\n",
    "\n",
    "        # compute training accuracy and loss excluding perturbed edges\n",
    "        y_hat_masked = torch.masked_select(y_hat, perturbs)\n",
    "        ratings_masked = torch.masked_select(ratings, perturbs)\n",
    "        perturbations['accuracy_after'].append(get_accuracy(y_hat_masked, ratings_masked))\n",
    "        perturbations['loss_after'].append(loss_fn(y_hat_masked, ratings_masked).item())\n",
    "    \n",
    "    # compute meta gradient (not required for baseline experiment)\n",
    "    # meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "\n",
    "    # define evaluation model \n",
    "    eval_model = CollaborativeFiltering(n_users, n_items, n_factors) # experiment with twice the embedding size for evaluation\n",
    "    eval_model.to(device)\n",
    "    optimizer_eval = torch.optim.SGD(eval_model.parameters(), lr = lr)\n",
    "\n",
    "    torch.manual_seed(50)\n",
    "    # reset eval model parameters\n",
    "    for layer in eval_model.children():\n",
    "        layer.reset_parameters()\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn_eval = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    # detach ratings for eval model\n",
    "    ratings_eval = ratings.detach().clone()\n",
    "\n",
    "    # inner train  evaluation model\n",
    "    eval_model.train()\n",
    "    for i in range(T):\n",
    "        y_hat = eval_model(users, items)\n",
    "        loss_eval = loss_fn_eval(y_hat, ratings_eval)\n",
    "\n",
    "        # use torch.optim optimizer to compute gradients\n",
    "        optimizer_eval.zero_grad()\n",
    "        loss_eval.backward(retain_graph=retain_graph, create_graph=create_graph)\n",
    "        optimizer_eval.step()\n",
    "\n",
    "    # compute and store accuracy of eval model after T training steps\n",
    "    with torch.no_grad():\n",
    "        # compute training accuracy and loss including perturbed edges\n",
    "        y_hat = eval_model(users, items)\n",
    "        perturbations['accuracy_before_eval'].append(get_accuracy(y_hat, ratings_eval))\n",
    "        perturbations['loss_before_eval'].append(loss_fn_eval(y_hat, ratings_eval).item())\n",
    "\n",
    "        # compute training accuracy and loss excluding perturbed edges\n",
    "        y_hat_masked = torch.masked_select(y_hat, perturbs)\n",
    "        ratings_masked = torch.masked_select(ratings_eval, perturbs)\n",
    "        perturbations['accuracy_after_eval'].append(get_accuracy(y_hat_masked, ratings_masked))\n",
    "        perturbations['loss_after_eval'].append(loss_fn_eval(y_hat_masked, ratings_masked).item())\n",
    "\n",
    "    # baseline select edge and perform perturbation\n",
    "    with torch.no_grad():\n",
    "        best_edge = edges_to_perturb[delta]\n",
    "        ratings[best_edge] = 1 \n",
    "        perturbs[best_edge] = False\n",
    "\n",
    "        # keep track of perturbations and accuracy \n",
    "        perturbations['edges'].append(best_edge)\n",
    "        perturbations['metagrad'].append(-1)\n",
    "\n",
    "sleep(1)\n",
    "# compute execution time\n",
    "exec_time = int(time.time() - start_time)\n",
    "exec_time = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(exec_time))\n",
    "print('-> Execution time: {}'.format(exec_time))\n",
    "\n",
    "# convert results to dataframes for visualisation\n",
    "perturbations = pd.DataFrame(perturbations)\n",
    "filename = 'baseline_surrogate_meta_Delta={}_T={}_LR={}_Factors={}'.format(Delta, T, lr, n_factors) + '_auto' + ('_r' if retain_graph else '_c')\n",
    "\n",
    "# save results in CSV format\n",
    "if save_results:\n",
    "    perturbations.to_csv('results/' + dataset + '/perturbations_' + filename + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edges</th>\n",
       "      <th>metagrad</th>\n",
       "      <th>accuracy_before</th>\n",
       "      <th>accuracy_after</th>\n",
       "      <th>loss_before</th>\n",
       "      <th>loss_after</th>\n",
       "      <th>accuracy_before_eval</th>\n",
       "      <th>accuracy_after_eval</th>\n",
       "      <th>loss_before_eval</th>\n",
       "      <th>loss_after_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136555</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.367983</td>\n",
       "      <td>0.367983</td>\n",
       "      <td>0.865775</td>\n",
       "      <td>0.865775</td>\n",
       "      <td>0.368707</td>\n",
       "      <td>0.368707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87194</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864415</td>\n",
       "      <td>0.864419</td>\n",
       "      <td>0.367987</td>\n",
       "      <td>0.367985</td>\n",
       "      <td>0.865780</td>\n",
       "      <td>0.865784</td>\n",
       "      <td>0.368721</td>\n",
       "      <td>0.368714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85288</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.367995</td>\n",
       "      <td>0.865780</td>\n",
       "      <td>0.865789</td>\n",
       "      <td>0.368743</td>\n",
       "      <td>0.368724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91764</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864405</td>\n",
       "      <td>0.864418</td>\n",
       "      <td>0.368017</td>\n",
       "      <td>0.368004</td>\n",
       "      <td>0.865745</td>\n",
       "      <td>0.865758</td>\n",
       "      <td>0.368746</td>\n",
       "      <td>0.368725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42530</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864385</td>\n",
       "      <td>0.864397</td>\n",
       "      <td>0.367980</td>\n",
       "      <td>0.367969</td>\n",
       "      <td>0.865735</td>\n",
       "      <td>0.865747</td>\n",
       "      <td>0.368744</td>\n",
       "      <td>0.368724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>142690</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864310</td>\n",
       "      <td>0.864436</td>\n",
       "      <td>0.368407</td>\n",
       "      <td>0.368135</td>\n",
       "      <td>0.865365</td>\n",
       "      <td>0.865501</td>\n",
       "      <td>0.369132</td>\n",
       "      <td>0.368913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101348</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864295</td>\n",
       "      <td>0.864425</td>\n",
       "      <td>0.368429</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.865395</td>\n",
       "      <td>0.865530</td>\n",
       "      <td>0.369132</td>\n",
       "      <td>0.368911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>178467</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864270</td>\n",
       "      <td>0.864404</td>\n",
       "      <td>0.368433</td>\n",
       "      <td>0.368142</td>\n",
       "      <td>0.865375</td>\n",
       "      <td>0.865515</td>\n",
       "      <td>0.369143</td>\n",
       "      <td>0.368912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>80371</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864270</td>\n",
       "      <td>0.864404</td>\n",
       "      <td>0.368433</td>\n",
       "      <td>0.368142</td>\n",
       "      <td>0.865370</td>\n",
       "      <td>0.865509</td>\n",
       "      <td>0.369143</td>\n",
       "      <td>0.368913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>187722</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864280</td>\n",
       "      <td>0.864418</td>\n",
       "      <td>0.368457</td>\n",
       "      <td>0.368155</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.865503</td>\n",
       "      <td>0.369168</td>\n",
       "      <td>0.368926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     edges  metagrad  accuracy_before  accuracy_after  loss_before  \\\n",
       "0   136555        -1         0.864430        0.864430     0.367983   \n",
       "1    87194        -1         0.864415        0.864419     0.367987   \n",
       "2    85288        -1         0.864400        0.864409     0.368000   \n",
       "3    91764        -1         0.864405        0.864418     0.368017   \n",
       "4    42530        -1         0.864385        0.864397     0.367980   \n",
       "..     ...       ...              ...             ...          ...   \n",
       "95  142690        -1         0.864310        0.864436     0.368407   \n",
       "96  101348        -1         0.864295        0.864425     0.368429   \n",
       "97  178467        -1         0.864270        0.864404     0.368433   \n",
       "98   80371        -1         0.864270        0.864404     0.368433   \n",
       "99  187722        -1         0.864280        0.864418     0.368457   \n",
       "\n",
       "    loss_after  accuracy_before_eval  accuracy_after_eval  loss_before_eval  \\\n",
       "0     0.367983              0.865775             0.865775          0.368707   \n",
       "1     0.367985              0.865780             0.865784          0.368721   \n",
       "2     0.367995              0.865780             0.865789          0.368743   \n",
       "3     0.368004              0.865745             0.865758          0.368746   \n",
       "4     0.367969              0.865735             0.865747          0.368744   \n",
       "..         ...                   ...                  ...               ...   \n",
       "95    0.368135              0.865365             0.865501          0.369132   \n",
       "96    0.368140              0.865395             0.865530          0.369132   \n",
       "97    0.368142              0.865375             0.865515          0.369143   \n",
       "98    0.368142              0.865370             0.865509          0.369143   \n",
       "99    0.368155              0.865360             0.865503          0.369168   \n",
       "\n",
       "    loss_after_eval  \n",
       "0          0.368707  \n",
       "1          0.368714  \n",
       "2          0.368724  \n",
       "3          0.368725  \n",
       "4          0.368724  \n",
       "..              ...  \n",
       "95         0.368913  \n",
       "96         0.368911  \n",
       "97         0.368912  \n",
       "98         0.368913  \n",
       "99         0.368926  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Meta-attack, to compare with baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Learning rate:  51\n",
      "-> T:  400\n",
      "-> Delta: 100 (0.05%)\n",
      "-> Embedding size:  64\n",
      "-> Device:  cuda:3\n",
      "-> Retain graph:  True\n",
      "-> Create graph:  False\n",
      "-> Save results:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-> Perturbations: 100%|██████████| 100/100 [02:32<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Execution time: 00h 02m 33s\n"
     ]
    }
   ],
   "source": [
    "# start execution\n",
    "start_time = time.time()\n",
    "\n",
    "# GPU settings (set use_gpu = -1 if you want to use CPU)\n",
    "use_gpu = 3\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# some hyperparams\n",
    "lr = 51\n",
    "T = 400\n",
    "Delta = 100 # 5% ~ 10K perturbations\n",
    "n_factors = 64\n",
    "save_results = False\n",
    "retain_graph = True \n",
    "create_graph = False\n",
    "dataset = 'movielens'\n",
    "\n",
    "# list of perturbations\n",
    "perturbations = dict()\n",
    "perturbations['edges'] = []\n",
    "perturbations['metagrad'] = []\n",
    "perturbations['accuracy_before'] = []\n",
    "perturbations['accuracy_after'] = []\n",
    "perturbations['loss_before'] = []\n",
    "perturbations['loss_after'] = []\n",
    "\n",
    "perturbations['accuracy_before_eval'] = []\n",
    "perturbations['accuracy_after_eval'] = []\n",
    "perturbations['loss_before_eval'] = []\n",
    "perturbations['loss_after_eval'] = []\n",
    "\n",
    "# print hyperparam config\n",
    "print('-> Learning rate: ', lr)\n",
    "print('-> T: ', T)\n",
    "print('-> Delta: {} ({}%)'.format(Delta, round(Delta * 100 / n_samples, 2)))\n",
    "print('-> Embedding size: ', n_factors)\n",
    "print('-> Device: ', device)\n",
    "# print('-> Manual gradients: ', manual_gradients)\n",
    "print('-> Retain graph: ', retain_graph)\n",
    "print('-> Create graph: ', create_graph)\n",
    "print('-> Save results: ', save_results)\n",
    "\n",
    "# load users, items and ratings as tensors\n",
    "users = torch.tensor(user_list, device = device)\n",
    "items = torch.tensor(item_list, device = device)\n",
    "ratings = torch.tensor(rating_list, device = device, requires_grad = True)\n",
    "perturbs = torch.ones_like(ratings).bool()\n",
    "\n",
    "# define model and it's parameters\n",
    "model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "# for each perturbation do the following\n",
    "for delta in tqdm(range(Delta), desc='-> Perturbations'):\n",
    "\n",
    "    # makes loss reproducible for each iteration in Delta\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # reset model paramters \n",
    "    for layer in model.children():\n",
    "        layer.reset_parameters()\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    # inner loop training process\n",
    "    model.train()\n",
    "    for i in range(T):\n",
    "        y_hat = model(users, items)\n",
    "        loss = loss_fn(y_hat, ratings)\n",
    "\n",
    "        # use torch.optim optimizer to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=retain_graph, create_graph=create_graph)\n",
    "        optimizer.step()\n",
    "\n",
    "    # compute and store accuracy of model after T training steps\n",
    "    with torch.no_grad():\n",
    "        # compute training accuracy and loss including perturbed edges\n",
    "        y_hat = model(users, items)\n",
    "        perturbations['accuracy_before'].append(get_accuracy(y_hat, ratings))\n",
    "        perturbations['loss_before'].append(loss_fn(y_hat, ratings).item())\n",
    "\n",
    "        # compute training accuracy and loss excluding perturbed edges\n",
    "        y_hat_masked = torch.masked_select(y_hat, perturbs)\n",
    "        ratings_masked = torch.masked_select(ratings, perturbs)\n",
    "        perturbations['accuracy_after'].append(get_accuracy(y_hat_masked, ratings_masked))\n",
    "        perturbations['loss_after'].append(loss_fn(y_hat_masked, ratings_masked).item())\n",
    "    \n",
    "    # compute meta gradient\n",
    "    meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "\n",
    "    # define evaluation model \n",
    "    eval_model = CollaborativeFiltering(n_users, n_items, n_factors) # experiment with twice the embedding size for evaluation\n",
    "    eval_model.to(device)\n",
    "    optimizer_eval = torch.optim.SGD(eval_model.parameters(), lr = lr)\n",
    "\n",
    "    torch.manual_seed(50)\n",
    "    # reset eval model parameters\n",
    "    for layer in eval_model.children():\n",
    "        layer.reset_parameters()\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn_eval = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    # detach ratings for eval model\n",
    "    ratings_eval = ratings.detach().clone()\n",
    "\n",
    "    # inner train  evaluation model\n",
    "    eval_model.train()\n",
    "    for i in range(T):\n",
    "        y_hat = eval_model(users, items)\n",
    "        loss_eval = loss_fn_eval(y_hat, ratings_eval)\n",
    "\n",
    "        # use torch.optim optimizer to compute gradients\n",
    "        optimizer_eval.zero_grad()\n",
    "        loss_eval.backward(retain_graph=retain_graph, create_graph=create_graph)\n",
    "        optimizer_eval.step()\n",
    "\n",
    "    # compute and store accuracy of eval model after T training steps\n",
    "    with torch.no_grad():\n",
    "        # compute training accuracy and loss including perturbed edges\n",
    "        y_hat = eval_model(users, items)\n",
    "        perturbations['accuracy_before_eval'].append(get_accuracy(y_hat, ratings_eval))\n",
    "        perturbations['loss_before_eval'].append(loss_fn_eval(y_hat, ratings_eval).item())\n",
    "\n",
    "        # compute training accuracy and loss excluding perturbed edges\n",
    "        y_hat_masked = torch.masked_select(y_hat, perturbs)\n",
    "        ratings_masked = torch.masked_select(ratings_eval, perturbs)\n",
    "        perturbations['accuracy_after_eval'].append(get_accuracy(y_hat_masked, ratings_masked))\n",
    "        perturbations['loss_after_eval'].append(loss_fn_eval(y_hat_masked, ratings_masked).item())\n",
    "\n",
    "    # select best edge and perform perturbation\n",
    "    with torch.no_grad():\n",
    "        mask = ratings.detach().int()\n",
    "        meta_grad[mask == 1] = 0\n",
    "        best_edge = meta_grad.argmax().item()\n",
    "        ratings[best_edge] = 1\n",
    "        perturbs[best_edge] = False\n",
    "\n",
    "        # keep track of perturbations and accuracy\n",
    "        perturbations['edges'].append(best_edge)\n",
    "        perturbations['metagrad'].append(meta_grad[best_edge].item())\n",
    "\n",
    "sleep(1)\n",
    "# compute execution time\n",
    "exec_time = int(time.time() - start_time)\n",
    "exec_time = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(exec_time))\n",
    "print('-> Execution time: {}'.format(exec_time))\n",
    "\n",
    "# convert results to dataframes for visualisation\n",
    "perturbations = pd.DataFrame(perturbations)\n",
    "filename = 'surrogate_meta_Delta={}_T={}_LR={}_Factors={}'.format(Delta, T, lr, n_factors) + '_auto' + ('_r' if retain_graph else '_c')\n",
    "\n",
    "# save results in CSV format\n",
    "if save_results:\n",
    "    perturbations.to_csv('results/' + dataset + '/perturbations_' + filename + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edges</th>\n",
       "      <th>metagrad</th>\n",
       "      <th>accuracy_before</th>\n",
       "      <th>accuracy_after</th>\n",
       "      <th>loss_before</th>\n",
       "      <th>loss_after</th>\n",
       "      <th>accuracy_before_eval</th>\n",
       "      <th>accuracy_after_eval</th>\n",
       "      <th>loss_before_eval</th>\n",
       "      <th>loss_after_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40063</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.367983</td>\n",
       "      <td>0.367983</td>\n",
       "      <td>0.865775</td>\n",
       "      <td>0.865775</td>\n",
       "      <td>0.368707</td>\n",
       "      <td>0.368707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135373</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.864460</td>\n",
       "      <td>0.864464</td>\n",
       "      <td>0.368070</td>\n",
       "      <td>0.367995</td>\n",
       "      <td>0.865725</td>\n",
       "      <td>0.865729</td>\n",
       "      <td>0.368741</td>\n",
       "      <td>0.368722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26320</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.864455</td>\n",
       "      <td>0.864464</td>\n",
       "      <td>0.368141</td>\n",
       "      <td>0.367998</td>\n",
       "      <td>0.865740</td>\n",
       "      <td>0.865744</td>\n",
       "      <td>0.368745</td>\n",
       "      <td>0.368728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115985</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.864475</td>\n",
       "      <td>0.864488</td>\n",
       "      <td>0.368207</td>\n",
       "      <td>0.367992</td>\n",
       "      <td>0.865740</td>\n",
       "      <td>0.865743</td>\n",
       "      <td>0.368747</td>\n",
       "      <td>0.368730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57873</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.864500</td>\n",
       "      <td>0.864517</td>\n",
       "      <td>0.367822</td>\n",
       "      <td>0.367542</td>\n",
       "      <td>0.865745</td>\n",
       "      <td>0.865747</td>\n",
       "      <td>0.368749</td>\n",
       "      <td>0.368734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>136253</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.863730</td>\n",
       "      <td>0.864140</td>\n",
       "      <td>0.371690</td>\n",
       "      <td>0.367286</td>\n",
       "      <td>0.865615</td>\n",
       "      <td>0.865636</td>\n",
       "      <td>0.368850</td>\n",
       "      <td>0.368776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>178080</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.863720</td>\n",
       "      <td>0.864135</td>\n",
       "      <td>0.371746</td>\n",
       "      <td>0.367292</td>\n",
       "      <td>0.865600</td>\n",
       "      <td>0.865620</td>\n",
       "      <td>0.368859</td>\n",
       "      <td>0.368786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>127743</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.863720</td>\n",
       "      <td>0.864139</td>\n",
       "      <td>0.371801</td>\n",
       "      <td>0.367309</td>\n",
       "      <td>0.865600</td>\n",
       "      <td>0.865625</td>\n",
       "      <td>0.368859</td>\n",
       "      <td>0.368783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3844</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.863725</td>\n",
       "      <td>0.864148</td>\n",
       "      <td>0.371850</td>\n",
       "      <td>0.367320</td>\n",
       "      <td>0.865610</td>\n",
       "      <td>0.865634</td>\n",
       "      <td>0.368857</td>\n",
       "      <td>0.368783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>83599</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.863690</td>\n",
       "      <td>0.864118</td>\n",
       "      <td>0.371889</td>\n",
       "      <td>0.367331</td>\n",
       "      <td>0.865615</td>\n",
       "      <td>0.865639</td>\n",
       "      <td>0.368855</td>\n",
       "      <td>0.368783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     edges  metagrad  accuracy_before  accuracy_after  loss_before  \\\n",
       "0    40063  0.000131         0.864430        0.864430     0.367983   \n",
       "1   135373  0.000130         0.864460        0.864464     0.368070   \n",
       "2    26320  0.000123         0.864455        0.864464     0.368141   \n",
       "3   115985  0.000122         0.864475        0.864488     0.368207   \n",
       "4    57873  0.000121         0.864500        0.864517     0.367822   \n",
       "..     ...       ...              ...             ...          ...   \n",
       "95  136253  0.000089         0.863730        0.864140     0.371690   \n",
       "96  178080  0.000089         0.863720        0.864135     0.371746   \n",
       "97  127743  0.000089         0.863720        0.864139     0.371801   \n",
       "98    3844  0.000089         0.863725        0.864148     0.371850   \n",
       "99   83599  0.000089         0.863690        0.864118     0.371889   \n",
       "\n",
       "    loss_after  accuracy_before_eval  accuracy_after_eval  loss_before_eval  \\\n",
       "0     0.367983              0.865775             0.865775          0.368707   \n",
       "1     0.367995              0.865725             0.865729          0.368741   \n",
       "2     0.367998              0.865740             0.865744          0.368745   \n",
       "3     0.367992              0.865740             0.865743          0.368747   \n",
       "4     0.367542              0.865745             0.865747          0.368749   \n",
       "..         ...                   ...                  ...               ...   \n",
       "95    0.367286              0.865615             0.865636          0.368850   \n",
       "96    0.367292              0.865600             0.865620          0.368859   \n",
       "97    0.367309              0.865600             0.865625          0.368859   \n",
       "98    0.367320              0.865610             0.865634          0.368857   \n",
       "99    0.367331              0.865615             0.865639          0.368855   \n",
       "\n",
       "    loss_after_eval  \n",
       "0          0.368707  \n",
       "1          0.368722  \n",
       "2          0.368728  \n",
       "3          0.368730  \n",
       "4          0.368734  \n",
       "..              ...  \n",
       "95         0.368776  \n",
       "96         0.368786  \n",
       "97         0.368783  \n",
       "98         0.368783  \n",
       "99         0.368783  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Yeah, the baseline seems to work as expected, according to above results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **More baseline experiments below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Learning rate:  51\n",
      "-> T:  400\n",
      "-> Delta: 10000 (5.0%)\n",
      "-> Embedding size:  64\n",
      "-> Device:  cuda:3\n",
      "-> Retain graph:  True\n",
      "-> Create graph:  False\n",
      "-> Save results:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-> Perturbations: 100%|██████████| 10000/10000 [4:17:07<00:00,  1.54s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Execution time: 04h 17m 08s\n"
     ]
    }
   ],
   "source": [
    "# start execution\n",
    "start_time = time.time()\n",
    "\n",
    "# GPU settings (set use_gpu = -1 if you want to use CPU)\n",
    "use_gpu = 3\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# some hyperparams\n",
    "lr = 51\n",
    "T = 400\n",
    "Delta = 10000 # 5% ~ 10K perturbations\n",
    "n_factors = 64\n",
    "save_results = True\n",
    "retain_graph = True \n",
    "create_graph = False\n",
    "dataset = 'movielens'\n",
    "\n",
    "# list of perturbations\n",
    "perturbations = dict()\n",
    "perturbations['edges'] = []\n",
    "perturbations['metagrad'] = []\n",
    "perturbations['accuracy_before'] = []\n",
    "perturbations['accuracy_after'] = []\n",
    "perturbations['loss_before'] = []\n",
    "perturbations['loss_after'] = []\n",
    "\n",
    "perturbations['accuracy_before_eval'] = []\n",
    "perturbations['accuracy_after_eval'] = []\n",
    "perturbations['loss_before_eval'] = []\n",
    "perturbations['loss_after_eval'] = []\n",
    "\n",
    "# print hyperparam config\n",
    "print('-> Learning rate: ', lr)\n",
    "print('-> T: ', T)\n",
    "print('-> Delta: {} ({}%)'.format(Delta, round(Delta * 100 / n_samples, 2)))\n",
    "print('-> Embedding size: ', n_factors)\n",
    "print('-> Device: ', device)\n",
    "# print('-> Manual gradients: ', manual_gradients)\n",
    "print('-> Retain graph: ', retain_graph)\n",
    "print('-> Create graph: ', create_graph)\n",
    "print('-> Save results: ', save_results)\n",
    "\n",
    "# load users, items and ratings as tensors\n",
    "users = torch.tensor(user_list, device = device)\n",
    "items = torch.tensor(item_list, device = device)\n",
    "ratings = torch.tensor(rating_list, device = device, requires_grad = True)\n",
    "perturbs = torch.ones_like(ratings).bool()\n",
    "\n",
    "# neg_edges is set of indices of negative edges \n",
    "edges = ratings.detach().to('cpu').numpy()\n",
    "neg_edges = np.where(edges == 0)[0]\n",
    "np.random.seed(0)\n",
    "edges_to_perturb = np.random.choice(neg_edges, size=Delta, replace = True) # sample Delta edges randomly and perturb one by one inside loop \n",
    "\n",
    "# define model and it's parameters\n",
    "model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "# for each perturbation do the following\n",
    "for delta in tqdm(range(Delta), desc='-> Perturbations'):\n",
    "\n",
    "    # makes loss reproducible for each iteration in Delta\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # reset model paramters \n",
    "    for layer in model.children():\n",
    "        layer.reset_parameters()\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    # inner loop training process\n",
    "    model.train()\n",
    "    for i in range(T):\n",
    "        y_hat = model(users, items)\n",
    "        loss = loss_fn(y_hat, ratings)\n",
    "\n",
    "        # use torch.optim optimizer to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=retain_graph, create_graph=create_graph)\n",
    "        optimizer.step()\n",
    "\n",
    "    # compute and store accuracy of model after T training steps\n",
    "    with torch.no_grad():\n",
    "        # compute training accuracy and loss including perturbed edges\n",
    "        y_hat = model(users, items)\n",
    "        perturbations['accuracy_before'].append(get_accuracy(y_hat, ratings))\n",
    "        perturbations['loss_before'].append(loss_fn(y_hat, ratings).item())\n",
    "\n",
    "        # compute training accuracy and loss excluding perturbed edges\n",
    "        y_hat_masked = torch.masked_select(y_hat, perturbs)\n",
    "        ratings_masked = torch.masked_select(ratings, perturbs)\n",
    "        perturbations['accuracy_after'].append(get_accuracy(y_hat_masked, ratings_masked))\n",
    "        perturbations['loss_after'].append(loss_fn(y_hat_masked, ratings_masked).item())\n",
    "    \n",
    "    # compute meta gradient (not required for baseline experiment)\n",
    "    # meta_grad = torch.autograd.grad(loss, ratings)[0]\n",
    "\n",
    "    # define evaluation model \n",
    "    eval_model = CollaborativeFiltering(n_users, n_items, n_factors) # experiment with twice the embedding size for evaluation\n",
    "    eval_model.to(device)\n",
    "    optimizer_eval = torch.optim.SGD(eval_model.parameters(), lr = lr)\n",
    "\n",
    "    torch.manual_seed(50)\n",
    "    # reset eval model parameters\n",
    "    for layer in eval_model.children():\n",
    "        layer.reset_parameters()\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn_eval = nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "    # detach ratings for eval model\n",
    "    ratings_eval = ratings.detach().clone()\n",
    "\n",
    "    # inner train  evaluation model\n",
    "    eval_model.train()\n",
    "    for i in range(T):\n",
    "        y_hat = eval_model(users, items)\n",
    "        loss_eval = loss_fn_eval(y_hat, ratings_eval)\n",
    "\n",
    "        # use torch.optim optimizer to compute gradients\n",
    "        optimizer_eval.zero_grad()\n",
    "        loss_eval.backward(retain_graph=retain_graph, create_graph=create_graph)\n",
    "        optimizer_eval.step()\n",
    "\n",
    "    # compute and store accuracy of eval model after T training steps\n",
    "    with torch.no_grad():\n",
    "        # compute training accuracy and loss including perturbed edges\n",
    "        y_hat = eval_model(users, items)\n",
    "        perturbations['accuracy_before_eval'].append(get_accuracy(y_hat, ratings_eval))\n",
    "        perturbations['loss_before_eval'].append(loss_fn_eval(y_hat, ratings_eval).item())\n",
    "\n",
    "        # compute training accuracy and loss excluding perturbed edges\n",
    "        y_hat_masked = torch.masked_select(y_hat, perturbs)\n",
    "        ratings_masked = torch.masked_select(ratings_eval, perturbs)\n",
    "        perturbations['accuracy_after_eval'].append(get_accuracy(y_hat_masked, ratings_masked))\n",
    "        perturbations['loss_after_eval'].append(loss_fn_eval(y_hat_masked, ratings_masked).item())\n",
    "\n",
    "    # baseline select edge and perform perturbation\n",
    "    with torch.no_grad():\n",
    "        best_edge = edges_to_perturb[delta]\n",
    "        ratings[best_edge] = 1 \n",
    "        perturbs[best_edge] = False\n",
    "\n",
    "        # keep track of perturbations and accuracy \n",
    "        perturbations['edges'].append(best_edge)\n",
    "        perturbations['metagrad'].append(-1)\n",
    "\n",
    "sleep(1)\n",
    "# compute execution time\n",
    "exec_time = int(time.time() - start_time)\n",
    "exec_time = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(exec_time))\n",
    "print('-> Execution time: {}'.format(exec_time))\n",
    "\n",
    "# convert results to dataframes for visualisation\n",
    "perturbations = pd.DataFrame(perturbations)\n",
    "filename = 'baseline_surrogate_meta_Delta={}_T={}_LR={}_Factors={}'.format(Delta, T, lr, n_factors) + '_auto' + ('_r' if retain_graph else '_c')\n",
    "\n",
    "# save results in CSV format\n",
    "if save_results:\n",
    "    perturbations.to_csv('results/' + dataset + '/perturbations_' + filename + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edges</th>\n",
       "      <th>metagrad</th>\n",
       "      <th>accuracy_before</th>\n",
       "      <th>accuracy_after</th>\n",
       "      <th>loss_before</th>\n",
       "      <th>loss_after</th>\n",
       "      <th>accuracy_before_eval</th>\n",
       "      <th>accuracy_after_eval</th>\n",
       "      <th>loss_before_eval</th>\n",
       "      <th>loss_after_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136555</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.367983</td>\n",
       "      <td>0.367983</td>\n",
       "      <td>0.865775</td>\n",
       "      <td>0.865775</td>\n",
       "      <td>0.368707</td>\n",
       "      <td>0.368707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87194</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864415</td>\n",
       "      <td>0.864419</td>\n",
       "      <td>0.367987</td>\n",
       "      <td>0.367985</td>\n",
       "      <td>0.865780</td>\n",
       "      <td>0.865784</td>\n",
       "      <td>0.368721</td>\n",
       "      <td>0.368714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85288</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.367995</td>\n",
       "      <td>0.865780</td>\n",
       "      <td>0.865789</td>\n",
       "      <td>0.368743</td>\n",
       "      <td>0.368724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91764</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864405</td>\n",
       "      <td>0.864418</td>\n",
       "      <td>0.368017</td>\n",
       "      <td>0.368004</td>\n",
       "      <td>0.865745</td>\n",
       "      <td>0.865758</td>\n",
       "      <td>0.368746</td>\n",
       "      <td>0.368725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42530</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.864385</td>\n",
       "      <td>0.864397</td>\n",
       "      <td>0.367980</td>\n",
       "      <td>0.367969</td>\n",
       "      <td>0.865735</td>\n",
       "      <td>0.865747</td>\n",
       "      <td>0.368744</td>\n",
       "      <td>0.368724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>158987</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.847120</td>\n",
       "      <td>0.856866</td>\n",
       "      <td>0.400180</td>\n",
       "      <td>0.383669</td>\n",
       "      <td>0.847420</td>\n",
       "      <td>0.857554</td>\n",
       "      <td>0.404594</td>\n",
       "      <td>0.388932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>135563</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.847100</td>\n",
       "      <td>0.856844</td>\n",
       "      <td>0.400199</td>\n",
       "      <td>0.383682</td>\n",
       "      <td>0.847435</td>\n",
       "      <td>0.857569</td>\n",
       "      <td>0.404626</td>\n",
       "      <td>0.388945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>191021</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.847065</td>\n",
       "      <td>0.856812</td>\n",
       "      <td>0.400208</td>\n",
       "      <td>0.383694</td>\n",
       "      <td>0.847435</td>\n",
       "      <td>0.857573</td>\n",
       "      <td>0.404638</td>\n",
       "      <td>0.388948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>75824</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.847050</td>\n",
       "      <td>0.856806</td>\n",
       "      <td>0.400208</td>\n",
       "      <td>0.383696</td>\n",
       "      <td>0.847460</td>\n",
       "      <td>0.857604</td>\n",
       "      <td>0.404649</td>\n",
       "      <td>0.388958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>20581</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.847065</td>\n",
       "      <td>0.856816</td>\n",
       "      <td>0.400221</td>\n",
       "      <td>0.383711</td>\n",
       "      <td>0.847445</td>\n",
       "      <td>0.857598</td>\n",
       "      <td>0.404674</td>\n",
       "      <td>0.388973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       edges  metagrad  accuracy_before  accuracy_after  loss_before  \\\n",
       "0     136555        -1         0.864430        0.864430     0.367983   \n",
       "1      87194        -1         0.864415        0.864419     0.367987   \n",
       "2      85288        -1         0.864400        0.864409     0.368000   \n",
       "3      91764        -1         0.864405        0.864418     0.368017   \n",
       "4      42530        -1         0.864385        0.864397     0.367980   \n",
       "...      ...       ...              ...             ...          ...   \n",
       "9995  158987        -1         0.847120        0.856866     0.400180   \n",
       "9996  135563        -1         0.847100        0.856844     0.400199   \n",
       "9997  191021        -1         0.847065        0.856812     0.400208   \n",
       "9998   75824        -1         0.847050        0.856806     0.400208   \n",
       "9999   20581        -1         0.847065        0.856816     0.400221   \n",
       "\n",
       "      loss_after  accuracy_before_eval  accuracy_after_eval  loss_before_eval  \\\n",
       "0       0.367983              0.865775             0.865775          0.368707   \n",
       "1       0.367985              0.865780             0.865784          0.368721   \n",
       "2       0.367995              0.865780             0.865789          0.368743   \n",
       "3       0.368004              0.865745             0.865758          0.368746   \n",
       "4       0.367969              0.865735             0.865747          0.368744   \n",
       "...          ...                   ...                  ...               ...   \n",
       "9995    0.383669              0.847420             0.857554          0.404594   \n",
       "9996    0.383682              0.847435             0.857569          0.404626   \n",
       "9997    0.383694              0.847435             0.857573          0.404638   \n",
       "9998    0.383696              0.847460             0.857604          0.404649   \n",
       "9999    0.383711              0.847445             0.857598          0.404674   \n",
       "\n",
       "      loss_after_eval  \n",
       "0            0.368707  \n",
       "1            0.368714  \n",
       "2            0.368724  \n",
       "3            0.368725  \n",
       "4            0.368724  \n",
       "...               ...  \n",
       "9995         0.388932  \n",
       "9996         0.388945  \n",
       "9997         0.388948  \n",
       "9998         0.388958  \n",
       "9999         0.388973  \n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbations\n",
    "# almost no difference between baseline and surrogate meta-attack when it comes to \n",
    "# evaluation model with different weight initialisation "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f38835821387ecea7238337192aa99e87ed1a9c9c1fa6562e207de7e0c31193"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('PyG': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
