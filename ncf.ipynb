{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pyforest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from turtle import forward\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparams and data-loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682, 200000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_edges = np.load('movielens/train_edges.npy')\n",
    "user_list = train_edges[:, 0]\n",
    "item_list = train_edges[:, 1]\n",
    "rating_list = train_edges[:, 2].astype('float32')\n",
    "\n",
    "n_users = user_list.max() + 1 \n",
    "n_items = item_list.max() + 1\n",
    "n_samples = len(rating_list)\n",
    "\n",
    "n_users, n_items, n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Neural collaborative filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "        self.fc = nn.Linear(n_factors * 2, 1)\n",
    "        # self.user_emb.weight.data.uniform_(0, 0.5)\n",
    "        # self.item_emb.weight.data.uniform_(0, 0.5)\n",
    "        # self.fc.weight.data.uniform_(0, 0.5)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        features = torch.concat([u, i], dim = 1)\n",
    "        x = self.fc(features)\n",
    "        out = torch.sigmoid(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "        self.fc1 = nn.Linear(n_factors * 2, n_factors)\n",
    "        # self.fc2 = nn.Linear(n_factors, n_factors // 2)\n",
    "        self.fc2 = nn.Linear(n_factors, 1)\n",
    "        # self.fc3 = nn.Linear(n_factors // 2, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        tanh = nn.Tanh()\n",
    "        sigmoid = nn.Sigmoid()\n",
    "\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        features = torch.concat([u, i], dim = 1)\n",
    "        x = self.fc1(features)\n",
    "        x = tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = tanh(x)\n",
    "        # x = self.fc3(x)\n",
    "        x = sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(CollaborativeFiltering, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        dot = (u * i).sum(1)\n",
    "        return torch.sigmoid(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_hat, y):\n",
    "    y = y.clone().int()\n",
    "    y_hat = (y_hat.clone() > 0.5).int()\n",
    "    accuracy = (y == y_hat).sum() / len(y)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test NCF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:  1  T =  50  loss:  0.6828802824020386  acc:  0.5540849566459656\n",
      "lr:  1  T =  100  loss:  0.6806814074516296  acc:  0.5611050128936768\n",
      "lr:  1  T =  150  loss:  0.6764176487922668  acc:  0.5745750069618225\n",
      "lr:  1  T =  200  loss:  0.6643857359886169  acc:  0.6038050055503845\n",
      "lr:  1  T =  250  loss:  0.6322505474090576  acc:  0.6566149592399597\n",
      "lr:  1  T =  300  loss:  0.5861669182777405  acc:  0.7042199969291687\n",
      "lr:  1  T =  350  loss:  0.5473552942276001  acc:  0.7322149872779846\n",
      "lr:  1  T =  400  loss:  0.5206984281539917  acc:  0.7523349523544312\n",
      "lr:  1  T =  450  loss:  0.5298035144805908  acc:  0.7421799898147583\n",
      "lr:  1  T =  500  loss:  0.49649155139923096  acc:  0.7652599811553955\n",
      "lr:  1  T =  550  loss:  0.49068909883499146  acc:  0.7680400013923645\n",
      "lr:  1  T =  600  loss:  0.48390406370162964  acc:  0.7710899710655212\n",
      "lr:  1  T =  650  loss:  0.47969263792037964  acc:  0.7729150056838989\n",
      "lr:  1  T =  700  loss:  0.4759491980075836  acc:  0.7744999527931213\n",
      "lr:  1  T =  750  loss:  0.47344014048576355  acc:  0.7754349708557129\n",
      "lr:  1  T =  800  loss:  0.4710142910480499  acc:  0.7763499617576599\n",
      "lr:  1  T =  850  loss:  0.469257652759552  acc:  0.7771649956703186\n",
      "lr:  1  T =  900  loss:  0.4675847291946411  acc:  0.7778399586677551\n",
      "lr:  1  T =  950  loss:  0.4661485552787781  acc:  0.778499960899353\n",
      "lr:  1  T =  1000  loss:  0.46487122774124146  acc:  0.7790249586105347\n",
      "\n",
      "lr:  5  T =  50  loss:  0.6594396829605103  acc:  0.594664990901947\n",
      "lr:  5  T =  100  loss:  0.602039098739624  acc:  0.6657599806785583\n",
      "lr:  5  T =  150  loss:  0.5377337336540222  acc:  0.7353900074958801\n",
      "lr:  5  T =  200  loss:  0.5061017274856567  acc:  0.7545199990272522\n",
      "lr:  5  T =  250  loss:  0.489329993724823  acc:  0.7637799978256226\n",
      "lr:  5  T =  300  loss:  0.4770214259624481  acc:  0.7722499966621399\n",
      "lr:  5  T =  350  loss:  0.47812703251838684  acc:  0.7730099558830261\n",
      "lr:  5  T =  400  loss:  0.4725319743156433  acc:  0.7742499709129333\n",
      "lr:  5  T =  450  loss:  0.4724974036216736  acc:  0.7724449634552002\n",
      "lr:  5  T =  500  loss:  0.46630382537841797  acc:  0.7768549919128418\n",
      "lr:  5  T =  550  loss:  0.46594515442848206  acc:  0.7764899730682373\n",
      "lr:  5  T =  600  loss:  0.46377697587013245  acc:  0.7787649631500244\n",
      "lr:  5  T =  650  loss:  0.4616002142429352  acc:  0.7812249660491943\n",
      "lr:  5  T =  700  loss:  0.4638863801956177  acc:  0.7793449759483337\n",
      "lr:  5  T =  750  loss:  0.45850569009780884  acc:  0.7805449962615967\n",
      "lr:  5  T =  800  loss:  0.4532923996448517  acc:  0.7833999991416931\n",
      "lr:  5  T =  850  loss:  0.4511012136936188  acc:  0.7862749695777893\n",
      "lr:  5  T =  900  loss:  0.44973763823509216  acc:  0.787684977054596\n",
      "lr:  5  T =  950  loss:  0.4483756124973297  acc:  0.7889099717140198\n",
      "lr:  5  T =  1000  loss:  0.44904911518096924  acc:  0.7884299755096436\n",
      "\n",
      "lr:  10  T =  50  loss:  1.6607643365859985  acc:  0.49541500210762024\n",
      "lr:  10  T =  100  loss:  1.0927455425262451  acc:  0.5482000112533569\n",
      "lr:  10  T =  150  loss:  0.7400290369987488  acc:  0.6284549832344055\n",
      "lr:  10  T =  200  loss:  0.6184612512588501  acc:  0.693464994430542\n",
      "lr:  10  T =  250  loss:  0.5654822587966919  acc:  0.724934995174408\n",
      "lr:  10  T =  300  loss:  0.5190233588218689  acc:  0.7472749948501587\n",
      "lr:  10  T =  350  loss:  0.4941255748271942  acc:  0.7631999850273132\n",
      "lr:  10  T =  400  loss:  0.49888718128204346  acc:  0.7607150077819824\n",
      "lr:  10  T =  450  loss:  0.4852621853351593  acc:  0.768619954586029\n",
      "lr:  10  T =  500  loss:  0.47136765718460083  acc:  0.7761449813842773\n",
      "lr:  10  T =  550  loss:  0.47600609064102173  acc:  0.772475004196167\n",
      "lr:  10  T =  600  loss:  0.4712132215499878  acc:  0.7756749987602234\n",
      "lr:  10  T =  650  loss:  0.465878427028656  acc:  0.7792350053787231\n",
      "lr:  10  T =  700  loss:  0.4623377323150635  acc:  0.7822749614715576\n",
      "lr:  10  T =  750  loss:  0.46046003699302673  acc:  0.7824099659919739\n",
      "lr:  10  T =  800  loss:  0.458994597196579  acc:  0.7826249599456787\n",
      "lr:  10  T =  850  loss:  0.453656941652298  acc:  0.7879349589347839\n",
      "lr:  10  T =  900  loss:  0.44679632782936096  acc:  0.7892249822616577\n",
      "lr:  10  T =  950  loss:  0.44760599732398987  acc:  0.7892999649047852\n",
      "lr:  10  T =  1000  loss:  0.44290265440940857  acc:  0.7931249737739563\n",
      "\n",
      "lr:  20  T =  50  loss:  49.90034866333008  acc:  0.5009349584579468\n",
      "lr:  20  T =  100  loss:  48.94124221801758  acc:  0.5024999976158142\n",
      "lr:  20  T =  150  loss:  23.897769927978516  acc:  0.6930750012397766\n",
      "lr:  20  T =  200  loss:  18.486385345458984  acc:  0.7427899837493896\n",
      "lr:  20  T =  250  loss:  22.76975440979004  acc:  0.6475299596786499\n",
      "lr:  20  T =  300  loss:  15.739569664001465  acc:  0.7189449667930603\n",
      "lr:  20  T =  350  loss:  11.43697738647461  acc:  0.7897049784660339\n",
      "lr:  20  T =  400  loss:  10.346739768981934  acc:  0.7403950095176697\n",
      "lr:  20  T =  450  loss:  2.991542100906372  acc:  0.7982400059700012\n",
      "lr:  20  T =  500  loss:  0.9440417885780334  acc:  0.698390007019043\n",
      "lr:  20  T =  550  loss:  1.3396331071853638  acc:  0.7863149642944336\n",
      "lr:  20  T =  600  loss:  0.7326198220252991  acc:  0.7314599752426147\n",
      "lr:  20  T =  650  loss:  1.4438629150390625  acc:  0.7097649574279785\n",
      "lr:  20  T =  700  loss:  0.959009051322937  acc:  0.7173149585723877\n",
      "lr:  20  T =  750  loss:  0.7993448376655579  acc:  0.7765299677848816\n",
      "lr:  20  T =  800  loss:  0.8663464784622192  acc:  0.7263999581336975\n",
      "lr:  20  T =  850  loss:  0.7487209439277649  acc:  0.7473199963569641\n",
      "lr:  20  T =  900  loss:  0.8180806040763855  acc:  0.7394449710845947\n",
      "lr:  20  T =  950  loss:  0.7644063830375671  acc:  0.7618699669837952\n",
      "lr:  20  T =  1000  loss:  0.714838445186615  acc:  0.7648999691009521\n",
      "\n",
      "lr:  30  T =  50  loss:  16.944862365722656  acc:  0.7106249928474426\n",
      "lr:  30  T =  100  loss:  4.8376240730285645  acc:  0.593529999256134\n",
      "lr:  30  T =  150  loss:  1.5881561040878296  acc:  0.741005003452301\n",
      "lr:  30  T =  200  loss:  1.3063852787017822  acc:  0.7539299726486206\n",
      "lr:  30  T =  250  loss:  1.0125629901885986  acc:  0.7060499787330627\n",
      "lr:  30  T =  300  loss:  0.91546630859375  acc:  0.6994199752807617\n",
      "lr:  30  T =  350  loss:  1.332165241241455  acc:  0.7607749700546265\n",
      "lr:  30  T =  400  loss:  0.9112880229949951  acc:  0.7789899706840515\n",
      "lr:  30  T =  450  loss:  0.7985005974769592  acc:  0.7305099964141846\n",
      "lr:  30  T =  500  loss:  0.8168953061103821  acc:  0.7571550011634827\n",
      "lr:  30  T =  550  loss:  0.749144971370697  acc:  0.7513699531555176\n",
      "lr:  30  T =  600  loss:  0.7040426135063171  acc:  0.7733049988746643\n",
      "lr:  30  T =  650  loss:  0.657836377620697  acc:  0.7787100076675415\n",
      "lr:  30  T =  700  loss:  0.7242906093597412  acc:  0.789734959602356\n",
      "lr:  30  T =  750  loss:  0.6269789934158325  acc:  0.7960000038146973\n",
      "lr:  30  T =  800  loss:  0.5510469675064087  acc:  0.7974050045013428\n",
      "lr:  30  T =  850  loss:  0.4821985960006714  acc:  0.811519980430603\n",
      "lr:  30  T =  900  loss:  0.4518257677555084  acc:  0.8228549957275391\n",
      "lr:  30  T =  950  loss:  0.4427473843097687  acc:  0.8461999893188477\n",
      "lr:  30  T =  1000  loss:  0.4266873598098755  acc:  0.857824981212616\n",
      "\n",
      "lr:  40  T =  50  loss:  20.352619171142578  acc:  0.7405099868774414\n",
      "lr:  40  T =  100  loss:  15.475473403930664  acc:  0.729764997959137\n",
      "lr:  40  T =  150  loss:  12.195241928100586  acc:  0.8078500032424927\n",
      "lr:  40  T =  200  loss:  6.58379602432251  acc:  0.8498749732971191\n",
      "lr:  40  T =  250  loss:  9.35101318359375  acc:  0.833079993724823\n",
      "lr:  40  T =  300  loss:  4.9058074951171875  acc:  0.8712700009346008\n",
      "lr:  40  T =  350  loss:  7.379741668701172  acc:  0.8511049747467041\n",
      "lr:  40  T =  400  loss:  8.430509567260742  acc:  0.8566199541091919\n",
      "lr:  40  T =  450  loss:  7.117222309112549  acc:  0.8755999803543091\n",
      "lr:  40  T =  500  loss:  6.9532670974731445  acc:  0.8447349667549133\n",
      "lr:  40  T =  550  loss:  4.212071895599365  acc:  0.9199949502944946\n",
      "lr:  40  T =  600  loss:  3.218104839324951  acc:  0.944694995880127\n",
      "lr:  40  T =  650  loss:  3.3354954719543457  acc:  0.940464973449707\n",
      "lr:  40  T =  700  loss:  3.2938246726989746  acc:  0.9429949522018433\n",
      "lr:  40  T =  750  loss:  2.7891273498535156  acc:  0.9609599709510803\n",
      "lr:  40  T =  800  loss:  2.7564895153045654  acc:  0.9615649580955505\n",
      "lr:  40  T =  850  loss:  2.7230656147003174  acc:  0.9631649851799011\n",
      "lr:  40  T =  900  loss:  2.698411703109741  acc:  0.9640949964523315\n",
      "lr:  40  T =  950  loss:  2.671384334564209  acc:  0.9643999934196472\n",
      "lr:  40  T =  1000  loss:  2.648153305053711  acc:  0.9650449752807617\n",
      "\n",
      "lr:  50  T =  50  loss:  21.948200225830078  acc:  0.7224949598312378\n",
      "lr:  50  T =  100  loss:  19.482746124267578  acc:  0.7426300048828125\n",
      "lr:  50  T =  150  loss:  8.762524604797363  acc:  0.4622649848461151\n",
      "lr:  50  T =  200  loss:  8.114157676696777  acc:  0.7613649964332581\n",
      "lr:  50  T =  250  loss:  1.5793380737304688  acc:  0.7626000046730042\n",
      "lr:  50  T =  300  loss:  7.502703666687012  acc:  0.704194962978363\n",
      "lr:  50  T =  350  loss:  3.007694959640503  acc:  0.6669699549674988\n",
      "lr:  50  T =  400  loss:  1.828526496887207  acc:  0.7514449954032898\n",
      "lr:  50  T =  450  loss:  2.2784996032714844  acc:  0.764490008354187\n",
      "lr:  50  T =  500  loss:  1.5925568342208862  acc:  0.8065449595451355\n",
      "lr:  50  T =  550  loss:  1.0484501123428345  acc:  0.8273249864578247\n",
      "lr:  50  T =  600  loss:  0.7060638666152954  acc:  0.8547749519348145\n",
      "lr:  50  T =  650  loss:  0.9555507302284241  acc:  0.8572199940681458\n",
      "lr:  50  T =  700  loss:  0.894527792930603  acc:  0.8555349707603455\n",
      "lr:  50  T =  750  loss:  1.029175877571106  acc:  0.8797499537467957\n",
      "lr:  50  T =  800  loss:  0.5648796558380127  acc:  0.8942399621009827\n",
      "lr:  50  T =  850  loss:  0.6460553407669067  acc:  0.9001100063323975\n",
      "lr:  50  T =  900  loss:  0.40936726331710815  acc:  0.9247699975967407\n",
      "lr:  50  T =  950  loss:  0.47818565368652344  acc:  0.921489953994751\n",
      "lr:  50  T =  1000  loss:  0.31800755858421326  acc:  0.9409849643707275\n",
      "\n",
      "lr:  60  T =  50  loss:  18.998760223388672  acc:  0.729640007019043\n",
      "lr:  60  T =  100  loss:  19.90110969543457  acc:  0.7626699805259705\n",
      "lr:  60  T =  150  loss:  11.323873519897461  acc:  0.8144049644470215\n",
      "lr:  60  T =  200  loss:  14.165251731872559  acc:  0.7846999764442444\n",
      "lr:  60  T =  250  loss:  6.241816520690918  acc:  0.8203099966049194\n",
      "lr:  60  T =  300  loss:  13.87374496459961  acc:  0.7644099593162537\n",
      "lr:  60  T =  350  loss:  3.9853928089141846  acc:  0.7546049952507019\n",
      "lr:  60  T =  400  loss:  4.229449272155762  acc:  0.8406599760055542\n",
      "lr:  60  T =  450  loss:  1.395048975944519  acc:  0.8378399610519409\n",
      "lr:  60  T =  500  loss:  1.6557347774505615  acc:  0.8527799844741821\n",
      "lr:  60  T =  550  loss:  2.1843152046203613  acc:  0.888384997844696\n",
      "lr:  60  T =  600  loss:  4.57830810546875  acc:  0.8522999882698059\n",
      "lr:  60  T =  650  loss:  1.5240651369094849  acc:  0.9053049683570862\n",
      "lr:  60  T =  700  loss:  1.4338270425796509  acc:  0.9139049649238586\n",
      "lr:  60  T =  750  loss:  0.5975689888000488  acc:  0.9371549487113953\n",
      "lr:  60  T =  800  loss:  0.6241536140441895  acc:  0.9157799482345581\n",
      "lr:  60  T =  850  loss:  0.5974071621894836  acc:  0.9311400055885315\n",
      "lr:  60  T =  900  loss:  0.5781558156013489  acc:  0.9266899824142456\n",
      "lr:  60  T =  950  loss:  0.41119998693466187  acc:  0.9694799780845642\n",
      "lr:  60  T =  1000  loss:  0.4302346110343933  acc:  0.9614749550819397\n",
      "\n",
      "lr:  70  T =  50  loss:  26.777559280395508  acc:  0.7190699577331543\n",
      "lr:  70  T =  100  loss:  19.096385955810547  acc:  0.742805004119873\n",
      "lr:  70  T =  150  loss:  7.99574089050293  acc:  0.6612150073051453\n",
      "lr:  70  T =  200  loss:  3.082505464553833  acc:  0.7253899574279785\n",
      "lr:  70  T =  250  loss:  10.372579574584961  acc:  0.8036499619483948\n",
      "lr:  70  T =  300  loss:  4.768772602081299  acc:  0.7457799911499023\n",
      "lr:  70  T =  350  loss:  3.309774875640869  acc:  0.7803449630737305\n",
      "lr:  70  T =  400  loss:  2.443429946899414  acc:  0.8294199705123901\n",
      "lr:  70  T =  450  loss:  3.2418370246887207  acc:  0.8297449946403503\n",
      "lr:  70  T =  500  loss:  1.0698118209838867  acc:  0.8854050040245056\n",
      "lr:  70  T =  550  loss:  1.0125133991241455  acc:  0.8854299783706665\n",
      "lr:  70  T =  600  loss:  0.6801303029060364  acc:  0.9062249660491943\n",
      "lr:  70  T =  650  loss:  0.8650489449501038  acc:  0.9339399933815002\n",
      "lr:  70  T =  700  loss:  0.5190929770469666  acc:  0.9185999631881714\n",
      "lr:  70  T =  750  loss:  0.5294778943061829  acc:  0.9551249742507935\n",
      "lr:  70  T =  800  loss:  1.005610466003418  acc:  0.9196449518203735\n",
      "lr:  70  T =  850  loss:  0.35628095269203186  acc:  0.9712599515914917\n",
      "lr:  70  T =  900  loss:  0.3253495693206787  acc:  0.9609799981117249\n",
      "lr:  70  T =  950  loss:  0.33119598031044006  acc:  0.9776199460029602\n",
      "lr:  70  T =  1000  loss:  0.3902004063129425  acc:  0.9634699821472168\n",
      "\n",
      "lr:  80  T =  50  loss:  26.269166946411133  acc:  0.7286750078201294\n",
      "lr:  80  T =  100  loss:  17.973106384277344  acc:  0.782414972782135\n",
      "lr:  80  T =  150  loss:  8.627280235290527  acc:  0.7697449922561646\n",
      "lr:  80  T =  200  loss:  10.292831420898438  acc:  0.8252149820327759\n",
      "lr:  80  T =  250  loss:  8.245035171508789  acc:  0.8448899984359741\n",
      "lr:  80  T =  300  loss:  12.348772048950195  acc:  0.7825799584388733\n",
      "lr:  80  T =  350  loss:  8.204500198364258  acc:  0.842644989490509\n",
      "lr:  80  T =  400  loss:  9.378485679626465  acc:  0.8271499872207642\n",
      "lr:  80  T =  450  loss:  6.513168811798096  acc:  0.8351999521255493\n",
      "lr:  80  T =  500  loss:  5.374807834625244  acc:  0.8795949816703796\n",
      "lr:  80  T =  550  loss:  3.4925053119659424  acc:  0.9137749671936035\n",
      "lr:  80  T =  600  loss:  4.01807975769043  acc:  0.8489399552345276\n",
      "lr:  80  T =  650  loss:  1.3176404237747192  acc:  0.960794985294342\n",
      "lr:  80  T =  700  loss:  3.4440829753875732  acc:  0.8659600019454956\n",
      "lr:  80  T =  750  loss:  1.692579984664917  acc:  0.934654951095581\n",
      "lr:  80  T =  800  loss:  1.1676546335220337  acc:  0.958549976348877\n",
      "lr:  80  T =  850  loss:  1.393554449081421  acc:  0.958299994468689\n",
      "lr:  80  T =  900  loss:  5.6109299659729  acc:  0.8703299760818481\n",
      "lr:  80  T =  950  loss:  1.234126091003418  acc:  0.9546799659729004\n",
      "lr:  80  T =  1000  loss:  1.124827265739441  acc:  0.96110999584198\n",
      "\n",
      "lr:  90  T =  50  loss:  28.23194122314453  acc:  0.7138149738311768\n",
      "lr:  90  T =  100  loss:  22.77950668334961  acc:  0.7517199516296387\n",
      "lr:  90  T =  150  loss:  18.35457992553711  acc:  0.7834699749946594\n",
      "lr:  90  T =  200  loss:  14.541094779968262  acc:  0.801889955997467\n",
      "lr:  90  T =  250  loss:  11.729057312011719  acc:  0.8469099998474121\n",
      "lr:  90  T =  300  loss:  10.574573516845703  acc:  0.862405002117157\n",
      "lr:  90  T =  350  loss:  14.460553169250488  acc:  0.8204500079154968\n",
      "lr:  90  T =  400  loss:  6.855306148529053  acc:  0.875154972076416\n",
      "lr:  90  T =  450  loss:  8.771894454956055  acc:  0.8816399574279785\n",
      "lr:  90  T =  500  loss:  8.386441230773926  acc:  0.8872799873352051\n",
      "lr:  90  T =  550  loss:  5.369565010070801  acc:  0.8955099582672119\n",
      "lr:  90  T =  600  loss:  10.118464469909668  acc:  0.864995002746582\n",
      "lr:  90  T =  650  loss:  5.364223480224609  acc:  0.9150649905204773\n",
      "lr:  90  T =  700  loss:  4.600955963134766  acc:  0.9485899806022644\n",
      "lr:  90  T =  750  loss:  4.5727152824401855  acc:  0.9495449662208557\n",
      "lr:  90  T =  800  loss:  4.556915283203125  acc:  0.9499799609184265\n",
      "lr:  90  T =  850  loss:  4.546469688415527  acc:  0.950249969959259\n",
      "lr:  90  T =  900  loss:  4.535699844360352  acc:  0.9503999948501587\n",
      "lr:  90  T =  950  loss:  4.524726867675781  acc:  0.950594961643219\n",
      "lr:  90  T =  1000  loss:  4.514681339263916  acc:  0.9507899880409241\n",
      "\n",
      "lr:  100  T =  50  loss:  28.72559928894043  acc:  0.7115849852561951\n",
      "lr:  100  T =  100  loss:  28.22560691833496  acc:  0.7163499593734741\n",
      "lr:  100  T =  150  loss:  26.703378677368164  acc:  0.7295249700546265\n",
      "lr:  100  T =  200  loss:  24.26621437072754  acc:  0.7501499652862549\n",
      "lr:  100  T =  250  loss:  21.6801700592041  acc:  0.7659599781036377\n",
      "lr:  100  T =  300  loss:  17.696592330932617  acc:  0.8021649718284607\n",
      "lr:  100  T =  350  loss:  14.234114646911621  acc:  0.8326849937438965\n",
      "lr:  100  T =  400  loss:  15.59145450592041  acc:  0.8173499703407288\n",
      "lr:  100  T =  450  loss:  11.197707176208496  acc:  0.8625449538230896\n",
      "lr:  100  T =  500  loss:  16.635364532470703  acc:  0.8065499663352966\n",
      "lr:  100  T =  550  loss:  14.051020622253418  acc:  0.8278749585151672\n",
      "lr:  100  T =  600  loss:  8.20119857788086  acc:  0.8825500011444092\n",
      "lr:  100  T =  650  loss:  8.614358901977539  acc:  0.8921399712562561\n",
      "lr:  100  T =  700  loss:  8.113285064697266  acc:  0.8971049785614014\n",
      "lr:  100  T =  750  loss:  7.98842716217041  acc:  0.9002599716186523\n",
      "lr:  100  T =  800  loss:  6.797597408294678  acc:  0.9046099781990051\n",
      "lr:  100  T =  850  loss:  6.078244686126709  acc:  0.9116500020027161\n",
      "lr:  100  T =  900  loss:  5.838303565979004  acc:  0.9181050062179565\n",
      "lr:  100  T =  950  loss:  6.537864685058594  acc:  0.9107549786567688\n",
      "lr:  100  T =  1000  loss:  5.157901287078857  acc:  0.9350699782371521\n",
      "\n",
      "lr:  200  T =  50  loss:  27.04150390625  acc:  0.7291349768638611\n",
      "lr:  200  T =  100  loss:  26.888723373413086  acc:  0.7308899760246277\n",
      "lr:  200  T =  150  loss:  26.89838409423828  acc:  0.7308449745178223\n",
      "lr:  200  T =  200  loss:  26.882976531982422  acc:  0.7309899926185608\n",
      "lr:  200  T =  250  loss:  26.882186889648438  acc:  0.7310000061988831\n",
      "lr:  200  T =  300  loss:  26.88232421875  acc:  0.7310049533843994\n",
      "lr:  200  T =  350  loss:  26.88260269165039  acc:  0.7310049533843994\n",
      "lr:  200  T =  400  loss:  26.882532119750977  acc:  0.7310149669647217\n",
      "lr:  200  T =  450  loss:  26.882492065429688  acc:  0.7310199737548828\n",
      "lr:  200  T =  500  loss:  26.88246726989746  acc:  0.7310199737548828\n",
      "lr:  200  T =  550  loss:  26.88249969482422  acc:  0.7310199737548828\n",
      "lr:  200  T =  600  loss:  26.88253402709961  acc:  0.7310199737548828\n",
      "lr:  200  T =  650  loss:  26.882564544677734  acc:  0.7310199737548828\n",
      "lr:  200  T =  700  loss:  26.88264274597168  acc:  0.7310199737548828\n",
      "lr:  200  T =  750  loss:  26.88266372680664  acc:  0.7310199737548828\n",
      "lr:  200  T =  800  loss:  26.882688522338867  acc:  0.7310199737548828\n",
      "lr:  200  T =  850  loss:  26.88276481628418  acc:  0.7310199737548828\n",
      "lr:  200  T =  900  loss:  26.882780075073242  acc:  0.7310199737548828\n",
      "lr:  200  T =  950  loss:  26.882797241210938  acc:  0.7310199737548828\n",
      "lr:  200  T =  1000  loss:  26.8828125  acc:  0.7310199737548828\n",
      "\n",
      "lr:  300  T =  50  loss:  23.696277618408203  acc:  0.7507500052452087\n",
      "lr:  300  T =  100  loss:  26.672483444213867  acc:  0.7331199645996094\n",
      "lr:  300  T =  150  loss:  26.67237663269043  acc:  0.7324249744415283\n",
      "lr:  300  T =  200  loss:  26.62964630126953  acc:  0.7327399849891663\n",
      "lr:  300  T =  250  loss:  26.651004791259766  acc:  0.7325299978256226\n",
      "lr:  300  T =  300  loss:  26.626182556152344  acc:  0.7323649525642395\n",
      "lr:  300  T =  350  loss:  26.638063430786133  acc:  0.732795000076294\n",
      "lr:  300  T =  400  loss:  26.635753631591797  acc:  0.7327799797058105\n",
      "lr:  300  T =  450  loss:  26.666173934936523  acc:  0.732509970664978\n",
      "lr:  300  T =  500  loss:  26.663833618164062  acc:  0.7325149774551392\n",
      "lr:  300  T =  550  loss:  26.663734436035156  acc:  0.7325149774551392\n",
      "lr:  300  T =  600  loss:  26.65105628967285  acc:  0.7324950098991394\n",
      "lr:  300  T =  650  loss:  26.651931762695312  acc:  0.7320899963378906\n",
      "lr:  300  T =  700  loss:  26.652109146118164  acc:  0.7325199842453003\n",
      "lr:  300  T =  750  loss:  26.653501510620117  acc:  0.7320899963378906\n",
      "lr:  300  T =  800  loss:  26.651729583740234  acc:  0.7324950098991394\n",
      "lr:  300  T =  850  loss:  26.652070999145508  acc:  0.7324849963188171\n",
      "lr:  300  T =  900  loss:  26.61894416809082  acc:  0.7328499555587769\n",
      "lr:  300  T =  950  loss:  26.623674392700195  acc:  0.7324249744415283\n",
      "lr:  300  T =  1000  loss:  26.624807357788086  acc:  0.7328499555587769\n",
      "\n",
      "lr:  400  T =  50  loss:  24.351900100708008  acc:  0.7470549941062927\n",
      "lr:  400  T =  100  loss:  22.918304443359375  acc:  0.7575849890708923\n",
      "lr:  400  T =  150  loss:  21.707006454467773  acc:  0.7569449543952942\n",
      "lr:  400  T =  200  loss:  21.58380889892578  acc:  0.7611649632453918\n",
      "lr:  400  T =  250  loss:  19.951709747314453  acc:  0.7717300057411194\n",
      "lr:  400  T =  300  loss:  19.510164260864258  acc:  0.7746700048446655\n",
      "lr:  400  T =  350  loss:  20.068374633789062  acc:  0.7771349549293518\n",
      "lr:  400  T =  400  loss:  19.920169830322266  acc:  0.7751149535179138\n",
      "lr:  400  T =  450  loss:  19.323352813720703  acc:  0.7739899754524231\n",
      "lr:  400  T =  500  loss:  18.356904983520508  acc:  0.7859799861907959\n",
      "lr:  400  T =  550  loss:  19.653852462768555  acc:  0.7764449715614319\n",
      "lr:  400  T =  600  loss:  18.642745971679688  acc:  0.7841299772262573\n",
      "lr:  400  T =  650  loss:  17.52631378173828  acc:  0.796144962310791\n",
      "lr:  400  T =  700  loss:  17.62009620666504  acc:  0.7928999662399292\n",
      "lr:  400  T =  750  loss:  16.95345115661621  acc:  0.7950999736785889\n",
      "lr:  400  T =  800  loss:  17.0367374420166  acc:  0.8009849786758423\n",
      "lr:  400  T =  850  loss:  16.46656608581543  acc:  0.8008149862289429\n",
      "lr:  400  T =  900  loss:  16.33304214477539  acc:  0.8083199858665466\n",
      "lr:  400  T =  950  loss:  16.657278060913086  acc:  0.8013049960136414\n",
      "lr:  400  T =  1000  loss:  15.640236854553223  acc:  0.8124499917030334\n",
      "\n",
      "lr:  500  T =  50  loss:  21.559324264526367  acc:  0.7562899589538574\n",
      "lr:  500  T =  100  loss:  22.753591537475586  acc:  0.7648800015449524\n",
      "lr:  500  T =  150  loss:  22.447341918945312  acc:  0.7653999924659729\n",
      "lr:  500  T =  200  loss:  18.253759384155273  acc:  0.774304986000061\n",
      "lr:  500  T =  250  loss:  18.737350463867188  acc:  0.7862100005149841\n",
      "lr:  500  T =  300  loss:  22.93547248840332  acc:  0.7596799731254578\n",
      "lr:  500  T =  350  loss:  19.223163604736328  acc:  0.7822499871253967\n",
      "lr:  500  T =  400  loss:  16.7795467376709  acc:  0.7946850061416626\n",
      "lr:  500  T =  450  loss:  15.415746688842773  acc:  0.7930449843406677\n",
      "lr:  500  T =  500  loss:  17.868337631225586  acc:  0.7908799648284912\n",
      "lr:  500  T =  550  loss:  14.790520668029785  acc:  0.8056649565696716\n",
      "lr:  500  T =  600  loss:  13.107259750366211  acc:  0.8069249987602234\n",
      "lr:  500  T =  650  loss:  14.239867210388184  acc:  0.800754964351654\n",
      "lr:  500  T =  700  loss:  18.334226608276367  acc:  0.7690500020980835\n",
      "lr:  500  T =  750  loss:  13.993106842041016  acc:  0.8095849752426147\n",
      "lr:  500  T =  800  loss:  13.953692436218262  acc:  0.8096599578857422\n",
      "lr:  500  T =  850  loss:  11.815978050231934  acc:  0.8057249784469604\n",
      "lr:  500  T =  900  loss:  12.85833740234375  acc:  0.8148699998855591\n",
      "lr:  500  T =  950  loss:  12.460054397583008  acc:  0.8078349828720093\n",
      "lr:  500  T =  1000  loss:  15.265297889709473  acc:  0.8071149587631226\n",
      "\n",
      "lr:  1000  T =  50  loss:  23.645092010498047  acc:  0.7532899975776672\n",
      "lr:  1000  T =  100  loss:  23.527099609375  acc:  0.7566699981689453\n",
      "lr:  1000  T =  150  loss:  22.832979202270508  acc:  0.7603650093078613\n",
      "lr:  1000  T =  200  loss:  24.272296905517578  acc:  0.7543649673461914\n",
      "lr:  1000  T =  250  loss:  23.55290985107422  acc:  0.7583949565887451\n",
      "lr:  1000  T =  300  loss:  22.600431442260742  acc:  0.7619450092315674\n",
      "lr:  1000  T =  350  loss:  22.41736602783203  acc:  0.7642399668693542\n",
      "lr:  1000  T =  400  loss:  22.051719665527344  acc:  0.7613199949264526\n",
      "lr:  1000  T =  450  loss:  21.386974334716797  acc:  0.7665899991989136\n",
      "lr:  1000  T =  500  loss:  20.768064498901367  acc:  0.7683849930763245\n",
      "lr:  1000  T =  550  loss:  21.752220153808594  acc:  0.7685449719429016\n",
      "lr:  1000  T =  600  loss:  21.583580017089844  acc:  0.7678549885749817\n",
      "lr:  1000  T =  650  loss:  21.14153480529785  acc:  0.7697449922561646\n",
      "lr:  1000  T =  700  loss:  21.223215103149414  acc:  0.7712799906730652\n",
      "lr:  1000  T =  750  loss:  21.49356460571289  acc:  0.7672500014305115\n",
      "lr:  1000  T =  800  loss:  22.60832405090332  acc:  0.7664799690246582\n",
      "lr:  1000  T =  850  loss:  21.84317398071289  acc:  0.7680949568748474\n",
      "lr:  1000  T =  900  loss:  21.22635269165039  acc:  0.7664499878883362\n",
      "lr:  1000  T =  950  loss:  21.91227149963379  acc:  0.7447999715805054\n",
      "lr:  1000  T =  1000  loss:  20.54435157775879  acc:  0.7670999765396118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use gpu \n",
    "use_gpu = 6\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load users, items and ratings as tensors\n",
    "users = torch.tensor(user_list, device = device)\n",
    "items = torch.tensor(item_list, device = device)\n",
    "ratings = torch.tensor(rating_list, device = device)\n",
    "ratings = ratings.reshape((len(ratings), 1))\n",
    "\n",
    "# define model and it's parameters\n",
    "n_factors = 64\n",
    "lr = 65\n",
    "T = 501\n",
    "\n",
    "for lr in [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 1000]:\n",
    "\n",
    "    for T in [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]:\n",
    "\n",
    "        model = NCF(n_users, n_items, n_factors)\n",
    "        # model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "        model.to(device)\n",
    "\n",
    "        manual_gradients = False\n",
    "        if manual_gradients == True:\n",
    "            p1, p2 = model.parameters()\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        for layer in model.children():\n",
    "            layer.reset_parameters()\n",
    "\n",
    "        loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "        model.train()\n",
    "\n",
    "        if manual_gradients == True:\n",
    "            for i in range(T):\n",
    "                y_hat = model(users, items)\n",
    "                loss = loss_fn(y_hat, ratings)\n",
    "                # results.append([delta, i, loss.item()])\n",
    "                if i % 10 == 0:\n",
    "                    print('T = ', T, ' loss: ', loss.item(), ' acc: ', get_accuracy(y_hat, ratings))\n",
    "\n",
    "                # compute inner parameter gradients\n",
    "                p1_grad = torch.autograd.grad(loss, p1, retain_graph=retain_graph, create_graph=create_graph)\n",
    "                p2_grad = torch.autograd.grad(loss, p2, retain_graph=retain_graph, create_graph=create_graph)\n",
    "\n",
    "                # update inner parameters\n",
    "                with torch.no_grad():\n",
    "                    p1_new = p1 - lr * p1_grad[0]\n",
    "                    p2_new = p2 - lr * p2_grad[0]\n",
    "                    p1.copy_(p1_new)\n",
    "                    p2.copy_(p2_new)\n",
    "        else:\n",
    "            for i in range(T):\n",
    "                y_hat = model(users, items)\n",
    "                loss = loss_fn(y_hat, ratings)\n",
    "                # results.append([delta, i, loss.item()])\n",
    "                if (i % (T - 1) == 0) and  (i > 0):\n",
    "                    print('lr: ', lr, ' T = ', i + 1, ' loss: ', loss.item(), ' acc: ', get_accuracy(y_hat, ratings))\n",
    "                    \n",
    "\n",
    "                # use torch.optim optimizer to compute gradients\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=True, create_graph=False)\n",
    "                optimizer.step()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1703818/1308426376.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "u = torch.tensor([1,2,3,4])\n",
    "v = torch.tensor([5,6,7,8])\n",
    "torch.concat([u, v], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb = nn.Embedding(n_users, n_factors)\n",
    "item_emb = nn.Embedding(n_items, n_factors)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f38835821387ecea7238337192aa99e87ed1a9c9c1fa6562e207de7e0c31193"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('PyG': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
