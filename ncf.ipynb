{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pyforest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from turtle import forward\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparams and data-loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682, 200000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_edges = np.load('movielens/train_edges.npy')\n",
    "user_list = train_edges[:, 0]\n",
    "item_list = train_edges[:, 1]\n",
    "rating_list = train_edges[:, 2].astype('float32')\n",
    "\n",
    "n_users = user_list.max() + 1 \n",
    "n_items = item_list.max() + 1\n",
    "n_samples = len(rating_list)\n",
    "\n",
    "n_users, n_items, n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Neural collaborative filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "        self.fc = nn.Linear(n_factors * 2, 1)\n",
    "        # self.user_emb.weight.data.uniform_(0, 0.5)\n",
    "        # self.item_emb.weight.data.uniform_(0, 0.5)\n",
    "        # self.fc.weight.data.uniform_(0, 0.5)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        features = torch.concat([u, i], dim = 1)\n",
    "        x = self.fc(features)\n",
    "        out = torch.sigmoid(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "        self.fc1 = nn.Linear(n_factors * 2, n_factors)\n",
    "        self.fc2 = nn.Linear(n_factors, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        features = torch.concat([u, i], dim = 1)\n",
    "        x = self.fc1(features)\n",
    "        x = self.fc2(x)\n",
    "        out = torch.sigmoid(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering(Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(CollaborativeFiltering, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        dot = (u * i).sum(1)\n",
    "        return torch.sigmoid(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_hat, y):\n",
    "    y = y.clone().int()\n",
    "    y_hat = (y_hat.clone() > 0.5).int()\n",
    "    accuracy = (y == y_hat).sum() / len(y)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test NCF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T =  301  loss:  0.7098179459571838  acc:  0.4984299838542938\n",
      "T =  301  loss:  32.774559020996094  acc:  0.6716349720954895\n",
      "T =  301  loss:  30.654199600219727  acc:  0.6933099627494812\n",
      "T =  301  loss:  30.435564041137695  acc:  0.6954999566078186\n",
      "T =  301  loss:  30.250099182128906  acc:  0.697439968585968\n",
      "T =  301  loss:  29.816503524780273  acc:  0.701770007610321\n",
      "T =  301  loss:  29.847911834716797  acc:  0.7014899849891663\n",
      "T =  301  loss:  29.60127830505371  acc:  0.7039799690246582\n",
      "T =  301  loss:  29.617719650268555  acc:  0.7037999629974365\n",
      "T =  301  loss:  29.589561462402344  acc:  0.7040449976921082\n",
      "T =  301  loss:  29.512977600097656  acc:  0.7048049569129944\n",
      "T =  301  loss:  29.384414672851562  acc:  0.7060949802398682\n",
      "T =  301  loss:  29.336135864257812  acc:  0.7066049575805664\n",
      "T =  301  loss:  29.172136306762695  acc:  0.7082299590110779\n",
      "T =  301  loss:  29.11808967590332  acc:  0.7087649703025818\n",
      "T =  301  loss:  29.058895111083984  acc:  0.7093899846076965\n",
      "T =  301  loss:  28.985149383544922  acc:  0.7101249694824219\n",
      "T =  301  loss:  28.852771759033203  acc:  0.7114599943161011\n",
      "T =  301  loss:  28.838592529296875  acc:  0.7116049528121948\n",
      "T =  301  loss:  28.838592529296875  acc:  0.7116049528121948\n",
      "T =  301  loss:  28.83858871459961  acc:  0.7116049528121948\n",
      "T =  301  loss:  28.838592529296875  acc:  0.7116049528121948\n",
      "T =  301  loss:  28.83858871459961  acc:  0.7116049528121948\n",
      "T =  301  loss:  28.838586807250977  acc:  0.7116049528121948\n",
      "T =  301  loss:  28.838584899902344  acc:  0.7116049528121948\n",
      "T =  301  loss:  28.838571548461914  acc:  0.7116049528121948\n",
      "T =  301  loss:  28.793563842773438  acc:  0.7120599746704102\n",
      "T =  301  loss:  28.751651763916016  acc:  0.7124800086021423\n",
      "T =  301  loss:  28.751998901367188  acc:  0.7124800086021423\n",
      "T =  301  loss:  28.751998901367188  acc:  0.7124800086021423\n",
      "T =  301  loss:  28.751998901367188  acc:  0.7124800086021423\n"
     ]
    }
   ],
   "source": [
    "# use gpu \n",
    "use_gpu = 6\n",
    "if use_gpu == -1:\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(str(use_gpu)) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load users, items and ratings as tensors\n",
    "users = torch.tensor(user_list, device = device)\n",
    "items = torch.tensor(item_list, device = device)\n",
    "ratings = torch.tensor(rating_list, device = device)\n",
    "ratings = ratings.reshape((len(ratings), 1))\n",
    "\n",
    "# define model and it's parameters\n",
    "n_factors = 64\n",
    "lr = 100\n",
    "T = 301\n",
    "model = NCF(n_users, n_items, n_factors)\n",
    "# model = CollaborativeFiltering(n_users, n_items, n_factors)\n",
    "model.to(device)\n",
    "\n",
    "manual_gradients = False\n",
    "if manual_gradients == True:\n",
    "    p1, p2 = model.parameters()\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 100)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for layer in model.children():\n",
    "    layer.reset_parameters()\n",
    "\n",
    "loss_fn = nn.BCELoss(reduction = 'mean')\n",
    "model.train()\n",
    "\n",
    "if manual_gradients == True:\n",
    "    for i in range(T):\n",
    "        y_hat = model(users, items)\n",
    "        loss = loss_fn(y_hat, ratings)\n",
    "        # results.append([delta, i, loss.item()])\n",
    "        if i % 10 == 0:\n",
    "            print('T = ', T, ' loss: ', loss.item(), ' acc: ', get_accuracy(y_hat, ratings))\n",
    "\n",
    "        # compute inner parameter gradients\n",
    "        p1_grad = torch.autograd.grad(loss, p1, retain_graph=retain_graph, create_graph=create_graph)\n",
    "        p2_grad = torch.autograd.grad(loss, p2, retain_graph=retain_graph, create_graph=create_graph)\n",
    "\n",
    "        # update inner parameters\n",
    "        with torch.no_grad():\n",
    "            p1_new = p1 - lr * p1_grad[0]\n",
    "            p2_new = p2 - lr * p2_grad[0]\n",
    "            p1.copy_(p1_new)\n",
    "            p2.copy_(p2_new)\n",
    "else:\n",
    "    for i in range(T):\n",
    "        y_hat = model(users, items)\n",
    "        loss = loss_fn(y_hat, ratings)\n",
    "        # results.append([delta, i, loss.item()])\n",
    "        if i % 10 == 0:\n",
    "            print('T = ', T, ' loss: ', loss.item(), ' acc: ', get_accuracy(y_hat, ratings))\n",
    "            \n",
    "\n",
    "        # use torch.optim optimizer to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True, create_graph=False)\n",
    "        optimizer.step()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f38835821387ecea7238337192aa99e87ed1a9c9c1fa6562e207de7e0c31193"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('PyG': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
